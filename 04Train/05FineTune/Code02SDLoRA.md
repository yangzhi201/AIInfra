<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->

# CODE02: LoRA å¾®è°ƒ SD(DONE)

> Author by: åº·ç…œ

æœ¬æ–‡å°†ä»åŸç†åˆ°ä»£ç ï¼Œä¸€æ­¥æ­¥å¸¦ä½ å®ç°ä½¿ç”¨ LoRA æŠ€æœ¯å¾®è°ƒ Stable Diffusion æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”ŸæˆäºŒæ¬¡å…ƒé£æ ¼å›¾åƒã€‚

## 1. Stable Diffusion åŸç†

Stable Diffusion å‘å¸ƒäº 2022 å¹´ï¼Œæ˜¯ä¸€ä¸ªç”±æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ˆtext-to-imageï¼‰æˆ–è€…ï¼ˆimage-to-imageï¼‰çš„ç”Ÿæˆå¼ AI æ¨¡å‹ï¼ˆGenerative Modelï¼‰ã€‚ä½œä¸ºä¸€ç§æ‰©æ•£æ¨¡å‹ï¼ŒStable Diffusion åœ¨å›¾åƒç”Ÿæˆä¸Šçš„æ–¹æ³•ä¸è®¸å¤šå…¶ä»–çš„ç”Ÿæˆæ¨¡å‹ä¸åŒï¼Œå®ƒçš„åŸç†æ˜¯ï¼šå°†å›¾åƒå‹ç¼©åˆ°ä¸€ä¸ªä½ç»´çš„â€œæ½œåœ¨ç©ºé—´â€ï¼ˆlatent spaceï¼‰ä¹‹åï¼Œå†è¿›è¡Œå¤„ç†ã€‚ä»¥ text-to-image ä¸ºä¾‹ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å…ˆå¯¹è¾“å…¥çš„å›¾åƒä¸æ–­æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œå¦‚ä¸‹å›¾ 1 æ‰€ç¤ºã€‚å¦‚æœèƒ½æŠŠè¿™ä¸ªè¿‡ç¨‹åè¿‡æ¥ï¼ˆåå‘æ‰©æ•£ï¼‰ï¼Œç”±ä¸€å¼ å®Œå…¨æ˜¯å™ªå£°çš„å›¾åƒï¼Œä¸€ç‚¹ç‚¹å»é™¤å™ªå£°åï¼Œé‡å»ºå‡ºåŸå§‹çš„å›¾åƒï¼ˆåœ¨æ¨¡å‹ä»¥åŠ prompt çš„å¼•å¯¼ä¹‹ä¸‹ï¼‰ï¼Œä¹Ÿå°±å®Œæˆäº† text-to-image çš„ä»»åŠ¡ã€‚


<div align="center">
    <img src="./images/Code02SDLoRA01.png" >
    <br>
    <em> å›¾ 1ï¼šForward Diffusion â€”â€” è¯¥è¿‡ç¨‹æ˜¯åœ¨æ¯ä¸€æ­¥ä¸­ä¾æ¬¡æ·»åŠ å™ªå£°ã€‚å™ªå£°é¢„æµ‹å™¨ä¼šä¼°è®¡åˆ°æ¯ä¸€æ­¥ä¸ºæ­¢æ‰€æ·»åŠ çš„æ€»å™ªå£°ã€‚</em>
</div>

</br>


<div align="center">
    <img src="./images/Code02SDLoRA02.png" >
    <br>
    <em> å›¾ 2ï¼šReverse Diffusion â€”â€” æˆ‘ä»¬é¦–å…ˆç”Ÿæˆä¸€ä¸ªå®Œå…¨éšæœºçš„å›¾åƒï¼Œå¹¶è¦æ±‚å™ªå£°é¢„æµ‹å™¨å‘Šè¯‰æˆ‘ä»¬å™ªå£°ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»åŸå§‹å›¾åƒä¸­å‡å»è¿™ä¸ªä¼°è®¡çš„å™ªå£°ã€‚é‡å¤è¿™ä¸ªè¿‡ç¨‹å‡ æ¬¡ã€‚å°±ä¼šå¾—åˆ°æœ€ç»ˆä¼šéœ€è¦çš„å›¾åƒï¼ˆçŒ«çŒ«ï¼‰ã€‚</em>
</div>

</br>

<div align="center">
    <img src="./images/Code02SDLoRA03.png" >
    <br>
    <em> å›¾ 3ï¼šLatent Diffusion Model: </em>
</div>

Stable Diffusion çš„å·¥ä½œåŸç†ç±»ä¼¼äºä¸€ä¸ªæœ‰æŸå‹ç¼©ç®—æ³•ï¼Œæ—¢èƒ½å¤Ÿå‹ç¼©ä¹Ÿèƒ½è§£å‹ç¼©ï¼Œè™½ç„¶ä¸ä¿è¯è§£å‹ç»“æœå’Œå‹ç¼©å‰å®Œå…¨ä¸€è‡´ï¼Œä½†æ˜¯æ•ˆæœå·®è·ä¸ä¼šç‰¹åˆ«è¿œã€‚è¿™ä¸ª encode/decode çš„è¿‡ç¨‹ä¹Ÿæ˜¯ç”±ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹å®Œæˆï¼Œè¯¥æ¨¡å‹ç§°ä¸º VAE (Variational Autoencoder)ã€‚å…¶ä¸­ VAE çš„ encoder å°†å›¾åƒå‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ä¸­çš„ä½ç»´è¡¨ç¤ºï¼Œ decoder ä» latent space ä¸­é€æ­¥é‡å»ºå›¾åƒã€‚

æœ‰å…³äº Stable Diffusion ä¸ºä»€ä¹ˆè¦é‡‡ç”¨è¿™æ ·çš„æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€ä¸ªç®€å•çš„å¯¹æ¯”ï¼šä¸€å¼ æ™®é€šçš„å½©è‰²å›¾ç‰‡ï¼Œå¦‚æœåˆ†è¾¨ç‡æ˜¯ 512Ã—512ï¼Œå…¶åƒç´ æ•°é‡é«˜è¾¾ 78 ä¸‡ä»¥ä¸Šï¼Œç›´æ¥å¤„ç†å¯¹è®¡ç®—èµ„æºçš„è¦æ±‚éå¸¸é«˜ã€‚è€Œ Stable Diffusion ä½¿ç”¨çš„å‹ç¼©å›¾åƒï¼Œæ•°æ®é‡åªæœ‰åŸæ¥çš„çº¦ 1/48ã€‚å¤§å¤§å‡è½»äº†è®¡ç®—è´Ÿæ‹…ã€‚æ­£å› å¦‚æ­¤ï¼Œå¦‚ä»Šæˆ‘ä»¬ç”šè‡³å¯ä»¥åœ¨é…å¤‡ 8GB æ˜¾å­˜çš„æ™®é€šå°å¼æœºæ˜¾å¡ä¸Šè¿è¡Œ Stable Diffusion æ¨¡å‹ã€‚


<a name="table1"></a>
<div align="center">

**è¡¨ 1ï¼šStable Diffusion å„ç»„ä»¶æ¨¡å‹å‚æ•°ç»Ÿè®¡**

| ç»„ä»¶   | å‚æ•°ä¸ªæ•°        | æ–‡ä»¶å¤§å°   | å æ¯”   |
| ------ | ------------- | -------- | ----- |
| CLIP   | 123,060,480   | 492 MB   | 12%   |
| VAE    | 83,653,863    | 335 MB   | 8%    |
| UNet   | 859,520,964   | 3.44 GB  | 80%   |
| Total  | 1,066,235,307 | 4.27 GB  | 100%  |

</div>


æ­¤å¤–ï¼Œ**å™ªéŸ³é¢„æµ‹å™¨ï¼ˆnoise predictorï¼‰** ä¹Ÿæ˜¯ Stable Diffusion éå¸¸é‡è¦çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ï¼Œè¿™ä¸ªç»„ä»¶æ˜¯ä¸€ä¸ª[U-Net æ¨¡å‹](https://zhouyifan.net/2024/01/23/20230713-SD3/)ï¼Œä¹Ÿæ˜¯æ•´ä¸ª SD æœ€å…³é”®çš„æ¨¡å‹ï¼Œ åŒ…æ‹¬äº†ä¸€ç³»åˆ— ResNet çš„å·ç§¯çŸ©é˜µå’Œ Cross-Attention çš„çŸ©é˜µï¼Œæ•´ä¸ª SD åŒ…å«å¤§çº¦ 860 M çš„ å‚æ•°ï¼Œç²¾åº¦ç¼–ç æ˜¯ float32ï¼Œæ€»ä½“éœ€è¦ 3.4G çš„å­˜å‚¨ç©ºé—´ï¼ˆå…¶ä»–è§[å„ç»„ä»¶æ¨¡å‹å‚æ•°ç»Ÿè®¡è¡¨](#table1)ï¼‰ã€‚

<div align="center">
    <img src="./images/Code02SDLoRA04.png" >
    <br>
    <em> å›¾ 4ï¼šStable Diffusion ä¸­çš„ Unet æ¨¡å‹æ¶æ„ </em>
</div>

è¿˜æœ‰ä¸€ä¸ªå¯¹è¾“å…¥è¿›è¡Œ embedding çš„æ¨¡å‹ CLIPï¼ŒStable Diffusion 1.x ç”¨çš„æ˜¯ OpenAI å¼€æºçš„ [ViT-L/14](https://github.com/CompVis/stable-diffusion) CLIP æ¨¡å‹ï¼Œ2.x ç”¨çš„æ˜¯ [OpenClip](https://stability.ai/blog/stable-diffusion-v2-release) æ¨¡å‹ã€‚





ç»¼ä¸Šï¼ŒStable Diffusion ä¸­ä¸€å…±æœ‰ä¸‰ä¸ªæ¨¡å‹

- CLIPï¼šç”¨äºå¯¹ prompt text è¿›è¡Œ embedding ç„¶åè¾“å…¥ç»™ U-Net
- VAE: å°†å›¾åƒä»åƒç´ ç©ºé—´ç¼–ç åˆ° latent space ä»¥åŠæœ€å decode å›æ¥
- U-Netï¼šè¿­ä»£ denoise æ‰€ç”¨çš„æ¨¡å‹ LoRA å¾®è°ƒæˆ‘ä»¬ä¸»è¦å¾®è°ƒè¿™ä¸ªæ¨¡å‹



**å…¶ä»–**ï¼š åœ¨åŸå§‹çš„ Stable Diffusion v1 ç‰ˆæœ¬ä¸­ï¼Œä½¿ç”¨çš„æ•°æ®é›†åŒ…æ‹¬ LAION-Aesthetics v2.6ï¼Œè¯¥æ•°æ®é›†ç­›é€‰äº† Common Crawl ä¸­ç¾å­¦è¯„åˆ†è¾¾åˆ° 6 åˆ†åŠä»¥ä¸Šçš„å›¾åƒï¼Œç¡®ä¿äº†è®­ç»ƒç´ æçš„è´¨é‡ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒStable Diffusion å·²ç»æ›´æ–°åˆ°äº† [v3.5 ç‰ˆæœ¬](https://huggingface.co/spaces/stabilityai/stable-diffusion-3.5-medium)

## 2. å‡†å¤‡å·¥ä½œä¸ç¯å¢ƒé…ç½®

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦é…ç½®å®éªŒç¯å¢ƒï¼Œç›¸å…³éœ€è¦çš„åº“å¦‚ä¸‹ï¼š


```python
# å®‰è£…å¿…è¦çš„åº“ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
# !pip install torch torchvision diffusers transformers datasets pillow accelerate

# å¯¼å…¥æ‰€éœ€åº“
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import matplotlib.pyplot as plt
from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler, AutoencoderKL
from transformers import CLIPTextModel, CLIPTokenizer
import numpy as np

# è®¾ç½®è®¾å¤‡ï¼ˆGPU ä¼˜å…ˆï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
torch.manual_seed(202509)
```

    ä½¿ç”¨è®¾å¤‡: cuda





    <torch._C.Generator at 0x7f40941723d0>




```python
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

## 2. LoRA åŸç†è¯¦è§£

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯**åœ¨ä¸æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹å¤§éƒ¨åˆ†å‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä½ç§©çŸ©é˜µæ¥æ•æ‰ä»»åŠ¡ç‰¹å®šçš„å˜åŒ–**ã€‚

ä¼ ç»Ÿçš„å…¨å‚æ•°å¾®è°ƒä¼šæ›´æ–°æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œå¯¹äºå¤§æ¨¡å‹æ¥è¯´è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚è€Œ LoRA çš„åˆ›æ–°ä¹‹å¤„åœ¨äºï¼š

å½“æˆ‘ä»¬å¾®è°ƒæ¨¡å‹æ—¶ï¼Œæƒé‡çš„æ›´æ–°å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$W = W_0 + \Delta W$$

å…¶ä¸­ $W_0$ æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„åŸå§‹æƒé‡ï¼Œ$\Delta W$ æ˜¯å¾®è°ƒè¿‡ç¨‹ä¸­å­¦ä¹ åˆ°çš„æƒé‡å˜åŒ–ã€‚

LoRA å‡è®¾ $\Delta W$ å¯ä»¥ç”¨ä¸¤ä¸ªä½ç§©çŸ©é˜µçš„ä¹˜ç§¯æ¥è¿‘ä¼¼ï¼š

$$\Delta W = BA$$

è¿™é‡Œ $B \in \mathbb{R}^{d \times r}$ å’Œ $A \in \mathbb{R}^{r \times k}$ æ˜¯ä½ç§©çŸ©é˜µï¼Œ$r$ æ˜¯ç§©ï¼ˆrankï¼‰ï¼Œä¸” $r \ll min(d, k)$ã€‚

å› æ­¤ï¼Œå‰å‘ä¼ æ’­å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$h = W_0 x + BA x$$

ä¸ºäº†å¹³è¡¡ LoRA æ›´æ–°çš„å½±å“ï¼Œé€šå¸¸ä¼šæ·»åŠ ä¸€ä¸ªç¼©æ”¾å› å­ï¼š

$$h = W_0 x + \frac{\alpha}{r} BA x$$

å…¶ä¸­ $\alpha$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œé€šå¸¸è®¾ç½®ä¸ºä¸ $r$ ç›¸å½“çš„å€¼ã€‚

åœ¨ Stable Diffusion ç­‰æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒLoRA ä¸»è¦åº”ç”¨äº**äº¤å‰æ³¨æ„åŠ›å±‚**ï¼Œå› ä¸ºè¿™äº›å±‚è´Ÿè´£æ–‡æœ¬ä¸å›¾åƒç‰¹å¾çš„äº¤äº’ï¼Œå¯¹é£æ ¼è¿ç§»æœ€ä¸ºå…³é”®ã€‚

é€šè¿‡åªè®­ç»ƒ $A$ å’Œ $B$ è¿™ä¸¤ä¸ªä½ç§©çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥ï¼šå¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼ˆé€šå¸¸å‡å°‘ 99%ä»¥ä¸Šï¼‰ã€é™ä½æ˜¾å­˜å ç”¨ã€åŠ å¿«è®­ç»ƒé€Ÿåº¦å’Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©ã€‚

## 3. æ‰‹åŠ¨å®ç° LoRA å±‚

è®©æˆ‘ä»¬æ ¹æ®ä¸Šè¿°åŸç†ï¼Œæ‰‹åŠ¨å®ç°ä¸€ä¸ª LoRA å±‚ï¼š


```python
class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=16, alpha=32):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        # è®¡ç®—ç¼©æ”¾å› å­
        self.scaling = alpha / rank
        
        # å®šä¹‰ä½ç§©çŸ©é˜µ A å’Œ B
        # A å°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°ä½ç»´ç©ºé—´
        self.A = nn.Linear(in_features, rank, bias=False)
        # B å°†ä½ç»´ç©ºé—´æ˜ å°„å›è¾“å‡ºç‰¹å¾ç©ºé—´
        self.B = nn.Linear(rank, out_features, bias=False)
        
        # åˆå§‹åŒ–æƒé‡
        # A çŸ©é˜µç”¨å°çš„éšæœºå€¼åˆå§‹åŒ–
        nn.init.normal_(self.A.weight, std=0.01)
        # B çŸ©é˜µåˆå§‹åŒ–ä¸ºé›¶ï¼Œç¡®ä¿åˆå§‹æ—¶ LoRA å±‚ä¸å½±å“åŸå§‹è¾“å‡º
        nn.init.zeros_(self.B.weight)

    def forward(self, x):
        """å‰å‘ä¼ æ’­ï¼šx -> A -> B -> ç¼©æ”¾"""
        # å…ˆé€šè¿‡ A çŸ©é˜µé™ç»´ï¼Œå†é€šè¿‡ B çŸ©é˜µå‡ç»´ï¼Œæœ€ååº”ç”¨ç¼©æ”¾
        return self.B(self.A(x)) * self.scaling
```

## 4. å°† LoRA æ³¨å…¥

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†å®ç°çš„ LoRA å±‚æ³¨å…¥åˆ° Stable Diffusion çš„ UNet æ¨¡å‹ä¸­ï¼Œç‰¹åˆ«æ˜¯æ³¨æ„åŠ›å±‚çš„ Qã€Kã€V æŠ•å½±çŸ©é˜µï¼š


```python
# åˆ›å»º
class LinearWithLoRA(nn.Module):
    def __init__(self, original_linear, lora_layer):
        super().__init__()
        self.original_linear = original_linear
        self.lora_layer = lora_layer
    
    def forward(self, x):
        return self.original_linear(x) + self.lora_layer(x)

def inject_lora_into_unet(unet, rank=16, alpha=32):
    lora_layers_count = 0
    
    # éå† UNet çš„æ‰€æœ‰æ¨¡å—ï¼Œæ‰¾åˆ° Attention æ¨¡å—
    for name, module in unet.named_modules():
        # æ‰¾åˆ°åŒ…å« to_q, to_k, to_v æ³¨æ„åŠ›æ¨¡å—ï¼ˆattn1 æˆ– attn2ï¼‰
        if ("attn1" in name or "attn2" in name) and hasattr(module, 'to_q'):
            # å¤„ç† to_q æŠ•å½±çŸ©é˜µ
            if hasattr(module, 'to_q'):
                original_module = module.to_q
                lora_layer = LoRALayer(
                    in_features=original_module.in_features,
                    out_features=original_module.out_features,
                    rank=rank,
                    alpha=alpha
                ).to(original_module.weight.device)
                
                # åˆ›å»ºåŒ…è£…æ¨¡å—å¹¶æ›¿æ¢åŸå§‹æ¨¡å—
                wrapped_module = LinearWithLoRA(original_module, lora_layer)
                module.to_q = wrapped_module
                lora_layers_count += 1
            
            # å¤„ç† to_k æŠ•å½±çŸ©é˜µ
            if hasattr(module, 'to_k'):
                original_module = module.to_k
                lora_layer = LoRALayer(
                    in_features=original_module.in_features,
                    out_features=original_module.out_features,
                    rank=rank,
                    alpha=alpha
                ).to(original_module.weight.device)
                
                wrapped_module = LinearWithLoRA(original_module, lora_layer)
                module.to_k = wrapped_module
                lora_layers_count += 1
            
            # å¤„ç† to_v æŠ•å½±çŸ©é˜µ
            if hasattr(module, 'to_v'):
                original_module = module.to_v
                lora_layer = LoRALayer(
                    in_features=original_module.in_features,
                    out_features=original_module.out_features,
                    rank=rank,
                    alpha=alpha
                ).to(original_module.weight.device)
                
                wrapped_module = LinearWithLoRA(original_module, lora_layer)
                module.to_v = wrapped_module
                lora_layers_count += 1
    
    print(f"æˆåŠŸæ³¨å…¥äº† {lora_layers_count} ä¸ª LoRA å±‚")
    return unet

```

## 5. åŠ è½½æ¨¡å‹é…ç½® LoRA

ç°åœ¨æˆ‘ä»¬åŠ è½½ Stable Diffusion åŸºç¡€æ¨¡å‹ï¼Œå¹¶åº”ç”¨æˆ‘ä»¬å®ç°çš„ LoRA å±‚ï¼š



```python
# å¦‚æœæ¨¡å‹ä¸‹è½½é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨ hfd ä¸‹è½½æ¨¡å‹ï¼Œhfd çš„ä½¿ç”¨è¯¦è§ https://hf-mirror.com/
#!hfd benjamin-paine/stable-diffusion-v1-5 --hf_username your_huggingface_name --hf_token your_huggingface_token --local-dir your_local_dir
```


```python
# æ¨¡å‹ ID
model_id = "./sd-15"

# åŠ è½½ UNet æ¨¡å‹ï¼ˆæ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒï¼‰
unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet").to(device)

# åŠ è½½æ–‡æœ¬ç¼–ç å™¨å’Œåˆ†è¯å™¨
text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder").to(device)
tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer")

# åŠ è½½ VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰
vae = AutoencoderKL.from_pretrained(model_id, subfolder="vae").to(device)

# æ³¨å…¥ LoRA å±‚ï¼Œä½¿ç”¨ rank=16 å’Œ alpha=32 çš„é…ç½®
# è¿™ä¸ªé…ç½®æ˜¯ç»è¿‡å®éªŒéªŒè¯çš„å¹³è¡¡ç‚¹
unet = inject_lora_into_unet(unet, rank=16, alpha=32)

# å®šä¹‰å‡½æ•°ï¼šå†»ç»“é LoRA å‚æ•°
def freeze_non_lora_params(model):
    for name, param in model.named_parameters():
        # åªä¿ç•™ LoRA ç›¸å…³å‚æ•°å¯è®­ç»ƒ
        if "lora_" not in name:
            param.requires_grad = False

# å†»ç»“ UNet ä¸­çš„é LoRA å‚æ•°
freeze_non_lora_params(unet)

# å†»ç»“æ–‡æœ¬ç¼–ç å™¨å’Œ VAE çš„æ‰€æœ‰å‚æ•°
for param in text_encoder.parameters():
    param.requires_grad = False

for param in vae.parameters():
    param.requires_grad = False

# è®¡ç®—å¯è®­ç»ƒå‚æ•°æ•°é‡
trainable_params = sum(p.numel() for p in unet.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in unet.parameters())

print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
print(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params:.2%}")
```

    æˆåŠŸæ³¨å…¥äº† 96 ä¸ª LoRA å±‚
    å¯è®­ç»ƒå‚æ•°æ•°é‡: 2,390,016
    æ€»å‚æ•°æ•°é‡: 861,910,980
    å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: 0.28%


ä»è¾“å‡ºå¯ä»¥çœ‹åˆ°ï¼ŒLoRA åªè®­ç»ƒäº†çº¦ 2.4M å‚æ•°ï¼Œä»…å æ€»å‚æ•°çš„ 0.3%å·¦å³ï¼Œè¿™å°±æ˜¯ LoRA å‚æ•°é«˜æ•ˆçš„åŸå› ï¼

## 6. æ•°æ®å‡†å¤‡

å¦‚ä½•å‡†å¤‡ Stable Diffusion çš„å¾®è°ƒæ•°æ®é›†éå¸¸çš„é‡è¦ï¼Œä¸è®ºæ˜¯åœ¨å›¾åƒè¿˜æ˜¯æ–‡æœ¬ï¼Œæ•°æ®é›†å¾€å¾€èƒ½å¤Ÿå†³å®šä¸€ä¸ªå¾®è°ƒæ¨¡å‹æœ€åçš„è´¨é‡ã€‚ç”±äºçœŸäººå›¾ç‰‡æ¶‰åŠåˆ°äº†ä¸€äº›éšç§ï¼Œåœ¨è¿™é‡Œçš„è¯æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ [Danbooru2021](https://gwern.net/danbooru2021) æ•°æ®é›†ï¼Œè¿™ä¸ªæ•°æ®é›†å…¨éƒ¨æ˜¯ç”±äºŒæ¬¡å…ƒçš„å›¾åƒç»„æˆï¼Œæˆ‘ä»¬ä¸ä¼šé€‰æ‹©è¿™ä¸ªæ•°æ®é›†é‡Œé¢çš„æ‰€æœ‰å›¾åƒæ¥åšï¼Œè€Œæ˜¯åªé€‰æ‹©ä¸€ä¸ªå­é›†æ¥å®Œæˆã€‚åœ¨ç½‘ä¸Šå¹¶æ²¡æœ‰ç°æˆçš„å­é›†ï¼Œè¿™é‡Œéœ€è¦æˆ‘ä»¬è‡ªå·±æœç´¢ã€‚é¦–å…ˆä»‹ç»ä¸€ä¸‹è¯¥æ•°æ®é›†çš„ä¸€äº›åŸºæœ¬å±æ€§

### 1. æ•°æ®é›†ç®€ä»‹
Danbooru2021 æ‹¥æœ‰ 490 ä¸‡å¤šå¼ å›¾åƒï¼Œå¹¶æ ‡æ³¨äº† 1.62 äº¿å¤šä¸ªæ ‡ç­¾ï¼Œå›¾åƒå’Œæ ‡ç­¾ä¸»è¦ç”±å¹¿å¤§çš„åŠ¨æ¼«çˆ±å¥½è€…ç¤¾åŒºä¸Šä¼ å’Œæ ‡æ³¨ã€‚

### 2. æ•°æ®æ ‡ç­¾

danbooru çš„æ ‡ç­¾ç³»ç»Ÿæ˜¯å…¶çµé­‚ï¼Œè¿™äº›æ ‡ç­¾å¹¶éé¢„å…ˆè®¾å®šçš„ tagï¼Œè€Œæ˜¯ç”±ç”¨æˆ·è‡ªç”±æ·»åŠ ã€‚ç›®å‰çš„æ ‡ç­¾å·²ç»è¾¾åˆ°äº†çº¦ 39.2 ä¸‡ç§æ ‡ç­¾ã€‚å¹³å‡æ¯å¼ å›¾åƒå·®ä¸å¤šæœ‰ 29 ä¸ªæ ‡ç­¾ã€‚ æ ‡ç­¾å†…å®¹å¦‚ä¸‹ï¼š

- äººç‰©ç‰¹å¾ï¼šå¦‚å‘å‹ï¼ˆlong_hair, twintailsï¼‰ã€ç³è‰²ï¼ˆblue_eyesï¼‰ã€è¡¨æƒ…ï¼ˆsmileï¼‰ã€æ€§åˆ«ï¼ˆ1girl, 1boyï¼‰

- æœé¥°ä¸è£…æ‰®ï¼šå¦‚æœè£…ç±»å‹ï¼ˆschool_uniform, maid_dressï¼‰ã€é…é¥°ï¼ˆhair_ribbon, cat_earsï¼‰

- æ„å›¾ä¸åœºæ™¯ï¼šå¦‚è§†è§’ï¼ˆfrom_aboveï¼‰ã€èƒŒæ™¯ï¼ˆoutdoors, skyï¼‰ã€ç”»é¢ä¸­çš„äººç‰©æ•°é‡ï¼ˆsoloï¼‰

- è‰ºæœ¯é£æ ¼ï¼šå¦‚è‰ºæœ¯å®¶/ç”»å¸ˆï¼ˆartist:nameï¼‰ã€ä½œå“ç³»åˆ—ï¼ˆfate/grand_orderï¼‰

- å…ƒæ ‡ç­¾ï¼šå¦‚è¯„åˆ†ï¼ˆrating:safe, rating:questionableï¼‰å’Œç‰ˆæƒä¿¡æ¯


</br> ä¸€èˆ¬æ¥è¯´ï¼Œå¾®è°ƒæ•°æ®é›†å¾€å¾€ä»é£æ ¼æˆ–è€…äººç‰©å…¥æ‰‹ï¼Œæ¯”å¦‚è®­ç»ƒä¸€ä¸ª fate ç³»åˆ—çš„ saber/è¿œå‚å‡›æˆ–è€…è®­ç»ƒæŸä¸ªåŠ¨æ¼«çš„ç”»é£ã€‚è¿™é‡Œçš„è¯é¦–å…ˆæˆ‘ä»¬ä¸»è¦å°è¯•åªç”Ÿäº§åŠ¨æ¼«çš„ç”»é£ï¼Œä¸è€ƒè™‘ä»»ä½•è§’è‰²æˆ–è€…é£æ ¼ï¼Œä¹Ÿå°±æ˜¯èƒ½ç”ŸæˆäºŒæ¬¡å…ƒçš„å›¾åƒå³å¯ã€‚æ•°æ®é›†è‡³å°‘éœ€è¦æ”¶é›† 20 å¼ ä»¥ä¸Šçš„å›¾ç‰‡ã€‚

æ¥ç€æˆ‘ä»¬å°†å›¾ç‰‡è¿›è¡Œç¼©æ”¾å¹¶è£å‰ªåˆ° 512x512 æˆ– 512x768 æˆ– 768x512 è¿™ 3 ç§å°ºå¯¸ä¹‹ä¸€ï¼Œè¿™é‡Œæˆ‘ä»¬é€‰çš„æ˜¯ 512x512 çš„å°ºå¯¸ï¼Œæœ‰éœ€è¦æ³¨æ„çš„æ˜¯ Stable Diffusion åŒä¸€æ¬¡è®­ç»ƒä¸­åªèƒ½å¤„ç†ä¸€ç§å°ºå¯¸çš„å›¾ç‰‡ã€‚


```python
class DanbooruDataset(Dataset):
    """Danbooru2021 æ•°æ®é›†å¤„ç†ç±»"""
    def __init__(self, image_dir, transform=None, size=512):
        # è·å–æ‰€æœ‰å›¾åƒè·¯å¾„
        self.image_paths = [
            os.path.join(image_dir, f) 
            for f in os.listdir(image_dir) 
            if f.endswith(('.png', '.jpg', '.jpeg'))
        ]
        
        # å®šä¹‰å›¾åƒå˜æ¢
        self.transform = transform or transforms.Compose([
            transforms.Resize((size, size)),  # è°ƒæ•´å¤§å°ä¸º 512x512
            transforms.RandomHorizontalFlip(p=0.5),  # éšæœºæ°´å¹³ç¿»è½¬å¢å¼º
            transforms.ToTensor(),  # è½¬æ¢ä¸º Tensor
            transforms.Normalize([0.5], [0.5])  # å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´
        ])
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        """æ ¹æ®ç´¢å¼•è·å–å›¾åƒ"""
        img_path = self.image_paths[idx]
        # æ‰“å¼€å›¾åƒå¹¶è½¬æ¢ä¸º RGB æ ¼å¼
        image = Image.open(img_path).convert('RGB')
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        return image

# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
# æ³¨æ„ï¼šè¯·å°†è·¯å¾„æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†å®é™…è·¯å¾„
dataset = DanbooruDataset('./danbooru2021')
dataloader = DataLoader(
    dataset, 
    batch_size=4,  # æ‰¹æ¬¡å¤§å°
    shuffle=True,  # æ‰“ä¹±æ•°æ®
    num_workers=2  # å¤šçº¿ç¨‹åŠ è½½
)

print(f"æ•°æ®é›†å¤§å°: {len(dataset)} å¼ å›¾åƒ")

# æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹å›¾åƒ
def show_samples(dataset, num_samples=4):
    plt.figure(figsize=(15, 10))
    for i in range(num_samples):
        idx = torch.randint(0, len(dataset), (1,)).item()
        img = dataset[idx]
        # åå½’ä¸€åŒ–ä»¥ä¾¿æ­£ç¡®æ˜¾ç¤º
        img = img.permute(1, 2, 0) * 0.5 + 0.5
        plt.subplot(1, num_samples, i+1)
        plt.imshow(img)
        plt.axis('off')
    plt.show()

# æ˜¾ç¤ºæ ·æœ¬å›¾åƒ
show_samples(dataset)
```

    æ•°æ®é›†å¤§å°: 57 å¼ å›¾åƒ



    
![png](output_13_1.png)
    


## 7. è®­ç»ƒ LoRA æ¨¡å‹

ç°åœ¨æˆ‘ä»¬æ¥è®¾ç½®è®­ç»ƒå‚æ•°å¹¶å¼€å§‹è®­ç»ƒï¼š


```python
# è®­ç»ƒå‚æ•°è®¾ç½®
num_epochs = 20
learning_rate = 1e-4
gradient_accumulation_steps = 4
weight_decay = 1e-2

# ä¼˜åŒ–å™¨
optimizer = optim.AdamW(
    [p for p in unet.parameters() if p.requires_grad],
    lr=learning_rate,
    weight_decay=weight_decay
)

lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer, 
    T_max=num_epochs,
    eta_min=1e-6
)

criterion = nn.MSELoss()
noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")
train_losses = []

# ğŸ”¥ æ·»åŠ è°ƒè¯•æ¨¡å¼
debug_mode = True


print("å¼€å§‹è®­ç»ƒ...")
for epoch in range(num_epochs):
    unet.train()
    total_loss = 0
    
    for step, batch in enumerate(dataloader):
        try:
            # å°†å›¾åƒç§»åŠ¨åˆ°è®¾å¤‡
            clean_images = batch.to(device)
            batch_size = clean_images.shape[0]  
            
            # ä½¿ç”¨ VAE ç¼–ç å™¨å°† RGB å›¾åƒè½¬æ¢ä¸ºæ½œåœ¨ç©ºé—´è¡¨ç¤º
            with torch.no_grad():
                latents = vae.encode(clean_images).latent_dist.sample()
                latents = latents * vae.config.scaling_factor
            
            # éšæœºé‡‡æ ·æ—¶é—´æ­¥
            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps,
                (batch_size,),  
                device=device
            ).long()
            
            # ç”Ÿæˆéšæœºå™ªå£°
            noise = torch.randn_like(latents)
            
            # å‰å‘æ‰©æ•£è¿‡ç¨‹
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            
            # å‰å‘ä¼ æ’­
            with torch.amp.autocast('cuda'):
                text_inputs = tokenizer(
                    [""] * batch_size,  # ğŸ”¥ åˆ›å»ºä¸æ‰¹æ¬¡å¤§å°åŒ¹é…çš„ç©ºæ–‡æœ¬åˆ—è¡¨
                    return_tensors="pt",
                    padding="max_length",
                    max_length=77,
                    truncation=True
                )
                encoder_hidden_states = text_encoder(text_inputs.input_ids.to(device))[0]
                
                # ğŸ”¥ è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€æ­¥æ˜¾ç¤ºï¼‰
                if step == 0 and epoch == 0:
                    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
                    print(f"ä¿®å¤åæ–‡æœ¬ç¼–ç å½¢çŠ¶: {encoder_hidden_states.shape}")
                    print(f"æ½œåœ¨ç©ºé—´å½¢çŠ¶: {latents.shape}")
                    print(f"å™ªå£°æ½œåœ¨ç©ºé—´å½¢çŠ¶: {noisy_latents.shape}")
                
                # é¢„æµ‹å™ªå£°
                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
                
                # è®¡ç®—æŸå¤±
                loss = criterion(noise_pred, noise)
                loss = loss / gradient_accumulation_steps
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦ç´¯ç§¯
            if (step + 1) % gradient_accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
            
            total_loss += loss.item()
            
            # æ‰“å°è®­ç»ƒè¿›åº¦
            if step % 50 == 0 and step > 0:
                avg_loss = total_loss / (step + 1)
                print(f"Epoch {epoch}, Step {step}, å¹³å‡æŸå¤±: {avg_loss:.4f}")
                
        except Exception as e:
            print(f"è®­ç»ƒæ­¥éª¤å‡ºé”™ - Epoch {epoch}, Step {step}: {str(e)}")
            continue
    
    # è®¡ç®—å¹¶è®°å½• epoch å¹³å‡æŸå¤±
    if len(dataloader) > 0:  # é¿å…é™¤é›¶é”™è¯¯
        avg_epoch_loss = total_loss / len(dataloader)
        train_losses.append(avg_epoch_loss)
        print(f"Epoch {epoch} å®Œæˆ, å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")
    
    # æ›´æ–°å­¦ä¹ ç‡
    lr_scheduler.step()


# ä¿å­˜è®­ç»ƒå¥½çš„ LoRA å‚æ•°
torch.save({
    'lora_state_dict': unet.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'train_losses': train_losses
}, 'lora_weights.pth')

print("è®­ç»ƒå®Œæˆï¼")
```

    å¼€å§‹è®­ç»ƒ...
    æ‰¹æ¬¡å¤§å°: 4
    ä¿®å¤åæ–‡æœ¬ç¼–ç å½¢çŠ¶: torch.Size([4, 77, 768])
    æ½œåœ¨ç©ºé—´å½¢çŠ¶: torch.Size([4, 4, 64, 64])
    å™ªå£°æ½œåœ¨ç©ºé—´å½¢çŠ¶: torch.Size([4, 4, 64, 64])
    Epoch 0 å®Œæˆ, å¹³å‡æŸå¤±: 0.0399
    Epoch 1 å®Œæˆ, å¹³å‡æŸå¤±: 0.0311
    Epoch 2 å®Œæˆ, å¹³å‡æŸå¤±: 0.0338
    Epoch 3 å®Œæˆ, å¹³å‡æŸå¤±: 0.0344
    Epoch 4 å®Œæˆ, å¹³å‡æŸå¤±: 0.0408
    Epoch 5 å®Œæˆ, å¹³å‡æŸå¤±: 0.0251
    Epoch 6 å®Œæˆ, å¹³å‡æŸå¤±: 0.0315
    Epoch 7 å®Œæˆ, å¹³å‡æŸå¤±: 0.0242
    Epoch 8 å®Œæˆ, å¹³å‡æŸå¤±: 0.0277
    Epoch 9 å®Œæˆ, å¹³å‡æŸå¤±: 0.0336
    Epoch 10 å®Œæˆ, å¹³å‡æŸå¤±: 0.0304
    Epoch 11 å®Œæˆ, å¹³å‡æŸå¤±: 0.0237
    Epoch 12 å®Œæˆ, å¹³å‡æŸå¤±: 0.0232
    Epoch 13 å®Œæˆ, å¹³å‡æŸå¤±: 0.0315
    Epoch 14 å®Œæˆ, å¹³å‡æŸå¤±: 0.0365
    Epoch 15 å®Œæˆ, å¹³å‡æŸå¤±: 0.0277
    Epoch 16 å®Œæˆ, å¹³å‡æŸå¤±: 0.0275
    Epoch 17 å®Œæˆ, å¹³å‡æŸå¤±: 0.0291
    Epoch 18 å®Œæˆ, å¹³å‡æŸå¤±: 0.0324
    Epoch 19 å®Œæˆ, å¹³å‡æŸå¤±: 0.0279
    è®­ç»ƒå®Œæˆï¼


ä»æŸå¤±æ›²çº¿å¯ä»¥çœ‹åˆ°ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼ŒæŸå¤±å€¼é€æ¸ä¸‹é™å¹¶è¶‹äºç¨³å®šï¼Œè¡¨æ˜æ¨¡å‹æ­£åœ¨æœ‰æ•ˆå­¦ä¹ äºŒæ¬¡å…ƒé£æ ¼ç‰¹å¾ã€‚

## 8. è¯„ä¼°æ¨¡å‹æ•ˆæœ

è®­ç»ƒå®Œæˆåï¼Œè®©æˆ‘ä»¬è¯„ä¼°æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœï¼š


```python
from transformers import CLIPImageProcessor

feature_extractor = CLIPImageProcessor.from_pretrained(model_id, subfolder="feature_extractor")

# åŠ è½½è®­ç»ƒå¥½çš„ LoRA æƒé‡
checkpoint = torch.load('lora_weights.pth')
unet.load_state_dict(checkpoint['lora_state_dict'])

# åˆ›å»ºç”Ÿæˆç®¡é“
pipe = StableDiffusionPipeline(
    unet=unet,
    text_encoder=text_encoder,
    vae=vae,
    tokenizer=tokenizer,
    scheduler=DDPMScheduler.from_pretrained(model_id, subfolder="scheduler"),
    feature_extractor=feature_extractor,  # ğŸ”¥ æ·»åŠ è¿™ä¸ªå‚æ•°
    safety_checker=None  # ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œå…³é—­å®‰å…¨æ£€æŸ¥å™¨
).to(device)

# å®šä¹‰ç”Ÿæˆå‚æ•°
prompt = "japanese animation, a girl, upper body, yellow dress, smiling, looking at viewer"
negative_prompt = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry"
num_inference_steps = 75
guidance_scale = 5

# ç”Ÿæˆå›¾åƒ
def generate_images(pipe, prompt, negative_prompt, num_images=3):
    images = []
    for _ in range(num_images):
        with torch.no_grad():
            image = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale
            ).images[0]
        images.append(image)
    return images

# ç”Ÿæˆ LoRA å¾®è°ƒåçš„å›¾åƒ
lora_images = generate_images(pipe, prompt, negative_prompt)

# åŠ è½½åŸå§‹æ¨¡å‹ç”¨äºå¯¹æ¯”
original_unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet").to(device)
original_pipe = StableDiffusionPipeline(
    unet=original_unet,
    text_encoder=text_encoder,
    vae=vae,
    tokenizer=tokenizer,
    scheduler=DDPMScheduler.from_pretrained(model_id, subfolder="scheduler"),
    feature_extractor=feature_extractor, 
    safety_checker=None
).to(device)

# ç”ŸæˆåŸå§‹æ¨¡å‹çš„å›¾åƒ
original_images = generate_images(original_pipe, prompt, negative_prompt)

# æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
def show_comparison(original, lora, prompt):
    plt.figure(figsize=(15, 10))
    plt.suptitle(f'prompt: {prompt}', fontsize=12)
    
    for i in range(len(original)):
        # åŸå§‹æ¨¡å‹ç»“æœ
        plt.subplot(2, len(original), i+1)
        plt.imshow(original[i])
        plt.title('original model')
        plt.axis('off')
        
        # LoRA å¾®è°ƒç»“æœ
        plt.subplot(2, len(original), i+1+len(original))
        plt.imshow(lora[i])
        plt.title('LoRA model')
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()

# æ˜¾ç¤ºå¯¹æ¯”å›¾åƒ
show_comparison(original_images, lora_images, prompt)
```

    You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .



      0%|          | 0/75 [00:00<?, ?it/s]



      0%|          | 0/75 [00:00<?, ?it/s]



      0%|          | 0/75 [00:00<?, ?it/s]


    You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .



      0%|          | 0/75 [00:00<?, ?it/s]



      0%|          | 0/75 [00:00<?, ?it/s]



      0%|          | 0/75 [00:00<?, ?it/s]



    
![png](output_17_8.png)
    


é€šè¿‡å¯¹æ¯”å¯ä»¥æ˜æ˜¾çœ‹å‡ºï¼š
- åŸå§‹æ¨¡å‹ç”Ÿæˆçš„å›¾åƒé£æ ¼åå‘å†™å®
- LoRA å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆçš„å›¾åƒå…·æœ‰æ˜æ˜¾çš„äºŒæ¬¡å…ƒé£æ ¼ç‰¹å¾
- çœ¼ç›ã€å¤´å‘ç­‰ç»†èŠ‚æ›´ç¬¦åˆåŠ¨æ¼«å®¡ç¾

å¦å¤–ï¼Œç”±äºæ•°æ®é›†è¾ƒå°‘å’Œæ¨¡å‹æœ¬èº«å‚æ•°ä¸å¤Ÿçš„æƒ…å†µä¸‹ï¼Œå°½ç®¡å›¾ç‰‡çš„ç”Ÿæˆå®Œæˆäº†åŸºæœ¬çš„éµå¾ªï¼Œä½†æ˜¯å¯ä»¥å‘ç°åŸå§‹æ¨¡å‹çš„å›¾ç‰‡ç”Ÿæˆæ•ˆæœå¾ˆå·®ï¼Œè„¸éƒ¨æ¸…æ™°åº¦ä¸å¤Ÿã€‚

## 9. LoRA æ•ˆæœå¯¹æ¯”

è®©æˆ‘ä»¬æ¯”è¾ƒä¸åŒç§©ï¼ˆrankï¼‰å¯¹ç”Ÿæˆæ•ˆæœçš„å½±å“ï¼š


```python
# æµ‹è¯•ä¸åŒ rank å€¼çš„æ•ˆæœ
ranks = [4, 8, 16, 32]
results = {}

for r in ranks:
    # åˆ›å»ºæ–°çš„ UNet å¹¶æ³¨å…¥ä¸åŒ rank çš„ LoRA
    test_unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet").to(device)
    test_unet = inject_lora_into_unet(test_unet, rank=r, alpha=r*2)  # ä¿æŒ alpha/r=2
    
    # å†»ç»“é LoRA å‚æ•°
    freeze_non_lora_params(test_unet)
    
    # åˆ›å»ºç®¡é“
    test_pipe = StableDiffusionPipeline(
        unet=test_unet,
        text_encoder=text_encoder,
        vae=vae,
        tokenizer=tokenizer,
        feature_extractor=feature_extractor,
        scheduler=DDPMScheduler.from_pretrained(model_id, subfolder="scheduler"),
        safety_checker=None
    ).to(device)
    
    # ç”Ÿæˆå›¾åƒ
    results[r] = generate_images(test_pipe, prompt, negative_prompt, num_images=2)
    print(f"å®Œæˆ rank={r} çš„å›¾åƒç”Ÿæˆ")

# æ˜¾ç¤ºä¸åŒ rank çš„å¯¹æ¯”ç»“æœ
plt.figure(figsize=(15, 15))
for i, r in enumerate(ranks):
    for j in range(len(results[r])):
        plt.subplot(len(ranks), len(results[r]), i*len(results[r]) + j + 1)
        plt.imshow(results[r][j])
        plt.title(f'rank={r}')
        plt.axis('off')

plt.tight_layout()
plt.show()
```

    You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .


    æˆåŠŸæ³¨å…¥äº† 96 ä¸ª LoRA å±‚



      0%|          | 0/75 [00:00<?, ?it/s]



      0%|          | 0/75 [00:00<?, ?it/s]


    å®Œæˆ rank=4 çš„å›¾åƒç”Ÿæˆ


    You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .


    æˆåŠŸæ³¨å…¥äº† 96 ä¸ª LoRA å±‚



      0%|          | 0/75 [00:00<?, ?it/s]



      0%|          | 0/75 [00:00<?, ?it/s]


    å®Œæˆ rank=8 çš„å›¾åƒç”Ÿæˆ


    You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .


    æˆåŠŸæ³¨å…¥äº† 96 ä¸ª LoRA å±‚



      0%|          | 0/75 [00:00<?, ?it/s]



      0%|          | 0/75 [00:00<?, ?it/s]


    å®Œæˆ rank=16 çš„å›¾åƒç”Ÿæˆ


    You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .


    æˆåŠŸæ³¨å…¥äº† 96 ä¸ª LoRA å±‚



      0%|          | 0/75 [00:00<?, ?it/s]



      0%|          | 0/75 [00:00<?, ?it/s]


    å®Œæˆ rank=32 çš„å›¾åƒç”Ÿæˆ



    
![png](output_19_20.png)
    


ä»ç»“æœå¯ä»¥è§‚å¯Ÿåˆ°ï¼š
- è¾ƒå°çš„ rankï¼ˆå¦‚ 4ï¼‰ï¼šå‚æ•°å°‘ï¼Œè®­ç»ƒå¿«ï¼Œä½†é£æ ¼è¡¨è¾¾èƒ½åŠ›æœ‰é™
- ä¸­ç­‰çš„ rankï¼ˆå¦‚ 16ï¼‰ï¼šåœ¨å‚æ•°æ•°é‡å’Œè¡¨è¾¾èƒ½åŠ›ä¹‹é—´å–å¾—å¹³è¡¡
- è¾ƒå¤§çš„ rankï¼ˆå¦‚ 32ï¼‰ï¼šè¡¨è¾¾èƒ½åŠ›æ›´å¼ºï¼Œä½†å‚æ•°å¢åŠ ï¼Œè®­ç»ƒæˆæœ¬æé«˜ï¼Œå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼ˆå¦‚å›¾æ‰€ç¤ºï¼‰

## 10. æ€»ç»“

é€šè¿‡å®éªŒå¯ä»¥çœ‹åˆ°ï¼ŒLoRA æŠ€æœ¯èƒ½å¤Ÿä»¥æä½çš„å‚æ•°æˆæœ¬ï¼ˆä»… 0.1%çš„å‚æ•°ï¼‰å®ç°ä¸å…¨å‚æ•°å¾®è°ƒæ¥è¿‘çš„æ•ˆæœï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†æ˜¾å­˜éœ€æ±‚å’Œè®­ç»ƒæ—¶é—´ã€‚

å¯¹äºäºŒæ¬¡å…ƒé£æ ¼å¾®è°ƒä»»åŠ¡ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ rank=16ã€alpha=32 çš„é…ç½®ï¼Œè¿™ä¸ªé…ç½®åœ¨æ•ˆæœå’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ã€‚

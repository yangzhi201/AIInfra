<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->

# CODE 03: Pipeline å¹¶è¡Œå®è·µ

> Author by: è®¸ç¿å²·

æœ¬å®éªŒæ—¨åœ¨æ·±å…¥ç†è§£ Pipeline å¹¶è¡ŒåŸç†ã€‚å…ˆå®ç° Gpipe æµæ°´çº¿å¹¶åˆ†æç©ºæ³¡ç‡ç°è±¡ï¼Œåè¿›é˜¶å®ç° 1F1B å’Œ Interleaved 1F1B è°ƒåº¦ç­–ç•¥ï¼Œä¼˜åŒ–ç©ºæ³¡ç‡ç°è±¡ï¼Œå¹¶å®è·µæ··åˆå¹¶è¡Œç­–ç•¥ã€‚

## 1. Pipeline å¹¶è¡ŒåŸºç¡€

**Pipeline å¹¶è¡Œï¼ˆPipeline Parallelism, PPï¼‰** å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸€ä¸ªåºå¤§çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ²¿ç€å±‚ï¼ˆLayerï¼‰çš„ç»´åº¦è¿›è¡Œçºµå‘åˆ‡å‰²ï¼Œåˆ†å‰²æˆå¤šä¸ªè¿ç»­çš„å­æ¨¡å—ï¼ˆç§°ä¸ºâ€œé˜¶æ®µâ€ï¼ŒStageï¼‰ï¼Œå¹¶å°†è¿™äº›é˜¶æ®µéƒ¨ç½²åˆ°ä¸åŒçš„è®¡ç®—è®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ä¸Šã€‚

æ•°å­¦ä¸Šï¼Œæ¨¡å‹å¯è¡¨ç¤ºä¸ºå‡½æ•°å¤åˆï¼š$F(x) = f_n(f_{n-1}(...f_1(x)...))$ï¼Œå…¶ä¸­æ¯ä¸ª $f_i$ï¼ˆæ¨¡å‹å±‚/å±‚ç»„ï¼‰å¯¹åº” Pipeline çš„ä¸€ä¸ªâ€œé˜¶æ®µâ€ï¼Œåˆ†é…åˆ°ä¸åŒè®¾å¤‡ä¸Šæ‰§è¡Œã€‚æ•°æ®ä»¥â€œæ‰¹æ¬¡â€ï¼ˆbatchï¼‰çš„å½¢å¼ï¼Œåƒå·¥å‚æµæ°´çº¿ä¸€æ ·ï¼Œä¾æ¬¡æµç»å„ä¸ªé˜¶æ®µã€‚

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¯ä¸ªè®¾å¤‡åªéœ€åŠ è½½å’Œå¤„ç†æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œä»è€Œçªç ´**å•å¡æ˜¾å­˜çš„é™åˆ¶**ã€‚

ç„¶è€Œï¼Œè¿™ç§æ‹†åˆ†ä¹Ÿå¼•å…¥äº†æ–°çš„æŒ‘æˆ˜ï¼š
*   **é€šä¿¡å¼€é”€ï¼š** å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç›¸é‚»é˜¶æ®µä¹‹é—´éœ€è¦é¢‘ç¹åœ°ä¼ é€’ä¸­é—´ç»“æœï¼ˆæ¿€æ´»å€¼å’Œæ¢¯åº¦ï¼‰ï¼Œè¿™ä¼šå¸¦æ¥é¢å¤–çš„é€šä¿¡å»¶è¿Ÿã€‚
*   **ç©ºæ³¡ç°è±¡ï¼ˆBubbleï¼‰ï¼š** ç”±äºæµæ°´çº¿çš„â€œå¡«å……â€ï¼ˆFillï¼‰å’Œâ€œæ’ç©ºâ€ï¼ˆDrainï¼‰è¿‡ç¨‹ï¼Œéƒ¨åˆ†è®¾å¤‡åœ¨æŸäº›æ—¶åˆ»ä¼šå¤„äºç­‰å¾…æ•°æ®çš„ç©ºé—²çŠ¶æ€ï¼Œé€ æˆè®¡ç®—èµ„æºçš„æµªè´¹ã€‚

**åç»­ä¼˜åŒ–æ–¹å‘**ï¼š
Gpipeã€1F1Bã€Interleaved 1F1B ç­‰è°ƒåº¦ç­–ç•¥ï¼Œæœ¬è´¨éƒ½æ˜¯é€šè¿‡è°ƒæ•´ã€Œå‰å‘ã€å’Œã€Œåå‘ã€çš„æ‰§è¡ŒèŠ‚å¥ï¼Œæ¥**å‹ç¼©ç©ºæ³¡æ—¶é—´ã€é™ä½é€šä¿¡å½±å“ã€æ›´é«˜æ•ˆåˆ©ç”¨æ˜¾å­˜** â€”â€” è¿™äº›æˆ‘ä»¬å°†åœ¨ä»£ç å®è·µä¸­é€ä¸€å®ç°å’Œå¯¹æ¯”ã€‚


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import time

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
```


```python
def get_available_devices(max_devices=4):
    """è‡ªåŠ¨è·å–å¯ç”¨è®¾å¤‡"""
    devices = []
    num_cuda = torch.cuda.device_count()
    if num_cuda > 0:
        devices = [torch.device(f"cuda:{i}") for i in range(min(num_cuda, max_devices))]
    else:
        devices = [torch.device("cpu")]
    print(f"å¯ç”¨è®¾å¤‡åˆ—è¡¨: {[str(dev) for dev in devices]}")
    return devices


def calculate_bubble_rate(strategy_name, num_stages, num_microbatches, interleaving_degree=2):
    """æ ¹æ®ç­–ç•¥ç±»å‹è®¡ç®—æ­£ç¡®çš„ç©ºæ³¡ç‡"""
    if num_stages == 1:
        return 0.0

    if strategy_name == "Naive":
        # Naive ç­–ç•¥æ²¡æœ‰æµæ°´çº¿å¹¶è¡Œï¼Œç©ºæ³¡ç‡ä¸º 0
        return 0.0
    elif strategy_name == "GPipe":
        # GPipe çš„ç©ºæ³¡ç‡å…¬å¼
        return (num_stages - 1) / (num_microbatches + num_stages - 1)
    elif strategy_name == "1F1B":
        # 1F1B çš„ç©ºæ³¡ç‡å…¬å¼
        return (num_stages - 1) / num_microbatches
    elif strategy_name == "Interleaved 1F1B":
        # Interleaved 1F1B çš„ç©ºæ³¡ç‡å…¬å¼
        return (num_stages - 1) / (num_microbatches * interleaving_degree)
    else:
        return 0.0

def create_model_parts(input_size=100, output_size=10):
    """åˆ›å»ºæ›´å¤æ‚çš„æ¨¡å‹åˆ†æ®µ"""
    layers = [
        nn.Sequential(
            nn.Linear(100, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Dropout(0.3)
        ),
        nn.Sequential(
            nn.Linear(1024, 2048),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Dropout(0.4)
        ),
        nn.Sequential(
            nn.Linear(2048, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 1024),
            nn.ReLU(),
            nn.Dropout(0.3)
        ),
        nn.Sequential(
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, output_size)
        )
    ]
    return layers
```

## 2. Native Pipeline Parallelismï¼ˆä¼ ç»Ÿæµæ°´çº¿å¹¶è¡Œï¼‰

é¦–å…ˆï¼Œæˆ‘ä»¬å®ç°ä¸€ä¸ªåŸºç¡€çš„æµæ°´çº¿å¹¶è¡Œæ¡†æ¶ï¼Œåªè€ƒè™‘äº†æ¨¡å‹åˆ†å‰²å’Œæµæ°´çº¿è°ƒåº¦ï¼Œå°†æ•°æ®ä»¥ batch ä¸ºå•ä½è¿›è¡Œå¤„ç†ã€‚

![](./images/Code03Pipeline01.png)


```python
class NaivePipelineParallel(nn.Module):
    def __init__(self, module_list, device_ids):
        super().__init__()
        assert len(module_list) == len(device_ids), "æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ"

        self.stages = nn.ModuleList(module_list)
        self.device_ids = device_ids
        self.num_stages = len(device_ids)

        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡
        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):
            self.stages[i] = stage.to(dev)

    def forward(self, x):
        intermediates = []
        current_output = x.to(self.device_ids[0])

        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):
            current_output = stage(current_output)
            if i < len(self.stages) - 1:
                # ç§»é™¤ detach()ï¼Œä¿ç•™æ¢¯åº¦
                current_output_act = current_output.requires_grad_(True)
                intermediates.append(current_output_act)
                current_output = current_output_act.to(self.device_ids[i+1])

        return current_output, intermediates
```

ä¸Šé¢çš„ä»£ç å®ç°äº†ä¸€ä¸ªåŸºç¡€çš„æµæ°´çº¿å¹¶è¡Œæ¡†æ¶ã€‚å®ƒå°†æ¨¡å‹åˆ†å‰²ä¸ºå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µæ”¾ç½®åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šã€‚åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ•°æ®ä¾æ¬¡é€šè¿‡è¿™äº›é˜¶æ®µï¼Œå¹¶åœ¨é˜¶æ®µé—´è¿›è¡Œè®¾å¤‡é—´çš„æ•°æ®ä¼ è¾“ã€‚

## 3. Gpipe æµæ°´çº¿å¹¶è¡Œ

Gpipe(Gradient Pipeline) æ˜¯ä¸€ç§åŸºäºæµæ°´çº¿å¹¶è¡Œçš„æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼Œå®ƒå°†ä¸€ä¸ªå¤§çš„è®­ç»ƒæ‰¹æ¬¡ï¼ˆBatchï¼‰æ‹†åˆ†æˆå¤šä¸ªå°çš„å¾®æ‰¹æ¬¡ï¼ˆMicro-batchï¼‰ï¼Œä¾æ¬¡æµè¿‡ Pipeline çš„å„ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µæ”¾ç½®åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šã€‚

![](./images/Code03Pipeline02.png)


```python


class GPipeParallel(nn.Module):
    def __init__(self, module_list, device_ids, num_microbatches=4):
        super().__init__()
        assert len(module_list) == len(device_ids), "æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ"

        self.stages = nn.ModuleList(module_list)
        self.device_ids = device_ids
        self.num_stages = len(device_ids)
        self.num_microbatches = num_microbatches

        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡
        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):
            self.stages[i] = stage.to(dev)

    def forward(self, x):
        """GPipe ç­–ç•¥: å…ˆæ‰€æœ‰å¾®æ‰¹æ¬¡å‰å‘ï¼Œå†æ‰€æœ‰å¾®æ‰¹æ¬¡åå‘"""
        # åˆ†å‰²è¾“å…¥ä¸ºå¾®æ‰¹æ¬¡
        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)
        activations = [[] for _ in range(self.num_stages)]

        # å‰å‘ä¼ æ’­: æ‰€æœ‰å¾®æ‰¹æ¬¡é€šè¿‡æ‰€æœ‰é˜¶æ®µ
        for i, micro_batch in enumerate(micro_batches):
            current = micro_batch.to(self.device_ids[0])
            for stage_idx, stage in enumerate(self.stages):
                current = stage(current)
                if stage_idx < self.num_stages - 1:
                    # ä¿å­˜ä¸­é—´æ¿€æ´»å€¼ï¼Œä¿ç•™æ¢¯åº¦è®¡ç®—
                    current_act = current.detach().clone().requires_grad_(True)
                    activations[stage_idx].append(current_act)
                    current = current_act.to(self.device_ids[stage_idx + 1])
                else:
                    # æœ€åé˜¶æ®µç›´æ¥ä¿å­˜è¾“å‡º
                    activations[stage_idx].append(current)

        # æ‹¼æ¥æœ€ç»ˆè¾“å‡º
        output = torch.cat(activations[-1], dim=0)
        return output, activations

    def backward(self, loss, activations):
        """GPipe åå‘ä¼ æ’­ - ä¿®å¤ç‰ˆæœ¬"""
        # è®¡ç®—æœ€ç»ˆæŸå¤±æ¢¯åº¦
        loss.backward()

        # ä»æœ€åé˜¶æ®µå¼€å§‹åå‘ä¼ æ’­
        for stage_idx in range(self.num_stages - 2, -1, -1):
            # è·å–å½“å‰é˜¶æ®µçš„æ¿€æ´»å€¼å’Œä¸‹ä¸€é˜¶æ®µçš„æ¢¯åº¦
            stage_activations = activations[stage_idx]
            next_gradients = []

            # æ”¶é›†ä¸‹ä¸€é˜¶æ®µçš„æ¢¯åº¦
            for act in activations[stage_idx + 1]:
                if act.grad is not None:
                    # ç¡®ä¿æ¢¯åº¦å½¢çŠ¶åŒ¹é…
                    grad = act.grad
                    if grad.shape != stage_activations[0].shape:
                        # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œå°è¯•è°ƒæ•´æ¢¯åº¦å½¢çŠ¶
                        try:
                            grad = grad.view(stage_activations[0].shape)
                        except:
                            # å¦‚æœæ— æ³•è°ƒæ•´å½¢çŠ¶ï¼Œè·³è¿‡è¿™ä¸ªæ¢¯åº¦
                            continue
                    next_gradients.append(grad.to(self.device_ids[stage_idx]))

        # åå‘ä¼ æ’­é€šè¿‡å½“å‰é˜¶æ®µ
        for i in range(len(stage_activations) - 1, -1, -1):
            if next_gradients and i < len(next_gradients):
                stage_activations[i].backward(next_gradients[i], retain_graph=True)
```

## 4. ç©ºæ³¡ç‡åˆ†æä¸è®¡ç®—

**ç©ºæ³¡ç‡**æ˜¯è¡¡é‡æµæ°´çº¿å¹¶è¡Œæ•ˆç‡çš„é‡è¦æŒ‡æ ‡ï¼Œè¡¨ç¤ºç”±äºæµæ°´çº¿å¡«å……å’Œæ’ç©ºé€ æˆçš„è®¡ç®—èµ„æºæµªè´¹æ¯”ä¾‹ã€‚ç©ºæ³¡ç‡çš„è®¡ç®—åŸºäºæµæ°´çº¿å¡«å……å’Œæ’ç©ºçš„æ—¶é—´å¼€é”€ã€‚å½“å¾®æ‰¹æ¬¡æ•°é‡è¿œå¤§äºæµæ°´çº¿é˜¶æ®µæ•°æ—¶ï¼Œç©ºæ³¡ç‡ä¼šé™ä½ï¼Œå› ä¸ºå¡«å……å’Œæ’ç©ºæ—¶é—´ç›¸å¯¹äºæ€»è®¡ç®—æ—¶é—´çš„æ¯”ä¾‹å˜å°ã€‚

æˆ‘ä»¬åœ¨è¿™é‡Œä»¥**Gpipe æµæ°´çº¿å¹¶è¡Œ**çš„ç©ºæ³¡ç‡è®¡ç®—ä¸ºä¾‹ï¼Œè®¡ç®—ç©ºæ³¡ç‡ã€‚

åœ¨æ•°å­¦ä¸Šï¼Œç©ºæ³¡ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
Bubble = (T_{fill} + T_{drain}) / (T_{total}) = (S - 1 + S - 1) / (2*(M + S - 1)) = (S - 1) / (M + S - 1)
$$

å…¶ä¸­ $S$ æ˜¯æµæ°´çº¿é˜¶æ®µæ•°ï¼Œ$M$ æ˜¯å¾®æ‰¹æ¬¡æ•°é‡ã€‚$T_{fill}$ è¡¨ç¤ºæµæ°´çº¿å¡«å……æ—¶é—´ï¼Œ$T_{drain}$ è¡¨ç¤ºæµæ°´çº¿æ’ç©ºæ—¶é—´,$T_{total}$ è¡¨ç¤ºæµæ°´çº¿æ€»æ—¶é—´ã€‚


```python
def calculate_bubble_rate(strategy_name, num_stages, num_microbatches, interleaving_degree=2):
    """æ ¹æ®ç­–ç•¥ç±»å‹è®¡ç®—æ­£ç¡®çš„ç©ºæ³¡ç‡"""
    if num_stages == 1:
        return 0.0

    if strategy_name == "Naive":
        # Naive ç­–ç•¥æ²¡æœ‰æµæ°´çº¿å¹¶è¡Œï¼Œç©ºæ³¡ç‡ä¸º 0
        return 0.0
    elif strategy_name == "GPipe":
        # GPipe çš„ç©ºæ³¡ç‡å…¬å¼
        return (num_stages - 1) / (num_microbatches + num_stages - 1)
    elif strategy_name == "1F1B":
        # 1F1B çš„ç©ºæ³¡ç‡å…¬å¼
        return (num_stages - 1) / num_microbatches
    elif strategy_name == "Interleaved 1F1B":
        # Interleaved 1F1B çš„ç©ºæ³¡ç‡å…¬å¼
        return (num_stages - 1) / (num_microbatches * interleaving_degree)
    else:
        return 0.0

configurations = [
    # ã€å¯¹æ¯”ç»„ 1ã€‘å›ºå®š S=4ï¼Œè§‚å¯Ÿ M å¢å¤§å¦‚ä½•é™ä½ç©ºæ³¡ç‡ï¼ˆå±•ç¤ºæ”¶ç›Šé€’å‡ï¼‰
    (4, 4),   # M = Sï¼Œç©ºæ³¡ç‡è¾ƒé«˜ï¼Œä¸´ç•Œç‚¹
    (4, 8),   # M = 2S
    (4, 16),  # M = 4Sï¼ˆæ¨èå·¥ç¨‹èµ·ç‚¹ï¼‰
    (4, 32),  # M = 8S
    (4, 64),  # M = 16S
    (4, 100),  # M = 25Sï¼Œæ¥è¿‘ç†æƒ³

    # ã€å¯¹æ¯”ç»„ 2ã€‘å›ºå®š M=2Sï¼Œè§‚å¯Ÿ S å¢å¤§æ—¶ç©ºæ³¡ç‡å¦‚ä½•ä¸Šå‡ï¼ˆå±•ç¤ºè§„æ¨¡ä»£ä»·ï¼‰
    (8, 16),  # M = 2S
    (16, 32), # M = 2S
    (32, 64), # M = 2Sï¼ˆå¦‚èµ„æºå…è®¸ï¼‰

    # ã€å¯¹æ¯”ç»„ 3ã€‘å›ºå®š M=4Sï¼Œè§‚å¯Ÿä¸åŒè§„æ¨¡ä¸‹çš„è¡¨ç°ï¼ˆæ¨èå·¥ç¨‹é…ç½®ï¼‰
    (8, 32),  # M = 4S
    (16, 64), # M = 4S
]

print("=== ä¸åŒé…ç½®ä¸‹çš„ç©ºæ³¡ç‡è®¡ç®—ç»“æœ ===")
for num_stages, num_microbatches in configurations:
    rate = calculate_bubble_rate("GPipe",num_stages, num_microbatches)
    print(f"é˜¶æ®µæ•°: {num_stages:3d}, å¾®æ‰¹æ¬¡: {num_microbatches:3d}, ç©ºæ³¡ç‡: {rate:.3f}")
```

    === ä¸åŒé…ç½®ä¸‹çš„ç©ºæ³¡ç‡è®¡ç®—ç»“æœ ===
    é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:   4, ç©ºæ³¡ç‡: 0.429
    é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:   8, ç©ºæ³¡ç‡: 0.273
    é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  16, ç©ºæ³¡ç‡: 0.158
    é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.086
    é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.045
    é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡: 100, ç©ºæ³¡ç‡: 0.029
    é˜¶æ®µæ•°:   8, å¾®æ‰¹æ¬¡:  16, ç©ºæ³¡ç‡: 0.304
    é˜¶æ®µæ•°:  16, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.319
    é˜¶æ®µæ•°:  32, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.326
    é˜¶æ®µæ•°:   8, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.179
    é˜¶æ®µæ•°:  16, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.190


ä»ä¸Šé¢ä»£ç çš„è¿è¡Œç»“æœæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼š
- **å¾®æ‰¹æ¬¡çš„å½±å“**ï¼šå½“ $M \gg S$ æ—¶ï¼Œç©ºæ³¡ç‡è¶‹è¿‘äº 0ï¼ˆå¦‚ $S=4, M=100$ï¼Œç©ºæ³¡ç‡â‰ˆ0.029ï¼‰ï¼Œå› æ­¤å¢åŠ å¾®æ‰¹æ¬¡æ˜¯é™ä½ç©ºæ³¡ç‡çš„æ ¸å¿ƒæ‰‹æ®µã€‚
- **é˜¶æ®µæ•°çš„å½±å“**ï¼š$S$ è¶Šå¤§ï¼Œç©ºæ³¡ç‡è¶Šé«˜ï¼ˆç›¸åŒ $M$ ä¸‹ï¼Œ$S=16$ æ¯” $S=4$ ç©ºæ³¡ç‡é«˜çº¦ 20%ï¼‰ï¼Œå› æ­¤ Pipeline é˜¶æ®µæ•°éœ€ä¸å¾®æ‰¹æ¬¡æ•°é‡åŒ¹é…ï¼ˆå»ºè®® $M \geq 4S$ï¼‰ã€‚

## 5. 1F1B è°ƒåº¦ç­–ç•¥å®ç°

1F1B(One-Forward-One-Backward) è°ƒåº¦æ˜¯ä¸€ç§ä¼˜åŒ–çš„æµæ°´çº¿å¹¶è¡Œç­–ç•¥ï¼Œå®ƒé€šè¿‡äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­æ¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œç©ºæ³¡æ—¶é—´ã€‚

![](./images/Code03Pipeline03.png)


```python
class OneFOneBPipeline(nn.Module):
    def __init__(self, module_list, device_ids, num_microbatches=4):
        super().__init__()
        assert len(module_list) == len(device_ids), "æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ"

        self.stages = nn.ModuleList(module_list)
        self.device_ids = device_ids
        self.num_stages = len(device_ids)
        self.num_microbatches = num_microbatches

        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡
        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):
            self.stages[i] = stage.to(dev)

    def forward(self, x):
        """1F1B ç­–ç•¥: äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­ - é‡æ–°å®ç°"""
        # åˆ†å‰²è¾“å…¥ä¸ºå¾®æ‰¹æ¬¡
        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)
        activations = [[] for _ in range(self.num_stages)]
        outputs = []

        # 1. å‰å‘å¡«å……é˜¶æ®µ (Warm-up)
        for i in range(self.num_stages):
            # å¤„ç†å‰ i+1 ä¸ªå¾®æ‰¹æ¬¡çš„å‰ i+1 ä¸ªé˜¶æ®µ
            for j in range(i + 1):
                if j >= len(micro_batches):
                    break

                current = micro_batches[j].to(self.device_ids[0])
                for stage_idx in range(i + 1):
                    if stage_idx >= self.num_stages:
                        break

                    current = self.stages[stage_idx](current)
                    if stage_idx < self.num_stages - 1:
                        current_act = current.detach().clone().requires_grad_(True)
                        if stage_idx < len(activations):
                            activations[stage_idx].append(current_act)
                        current = current_act.to(self.device_ids[stage_idx + 1])

                if i == self.num_stages - 1:
                    outputs.append(current)

        # 2. 1F1B é˜¶æ®µ (Steady state)
        for i in range(self.num_stages, self.num_microbatches):
            # å‰å‘ä¼ æ’­
            current = micro_batches[i].to(self.device_ids[0])
            for stage_idx in range(self.num_stages):
                current = self.stages[stage_idx](current)
                if stage_idx < self.num_stages - 1:
                    current_act = current.detach().clone().requires_grad_(True)
                    activations[stage_idx].append(current_act)
                    current = current_act.to(self.device_ids[stage_idx + 1])
            outputs.append(current)

        # 3. åå‘æ’ç©ºé˜¶æ®µ (Cool-down)
        for i in range(self.num_microbatches, self.num_microbatches + self.num_stages - 1):
            # è¿™é‡Œåªéœ€è¦å¤„ç†åå‘ä¼ æ’­ï¼Œå‰å‘å·²ç»å®Œæˆ
            pass

        # ç¡®ä¿è¾“å‡ºæ‰¹æ¬¡å¤§å°æ­£ç¡®
        if outputs:
            output = torch.cat(outputs, dim=0)
        else:
            output = torch.tensor([])

        return output, activations

    def backward(self, loss, activations):
        """1F1B åå‘ä¼ æ’­ - ä¿®å¤ç‰ˆæœ¬"""
        # è®¡ç®—æœ€ç»ˆæŸå¤±æ¢¯åº¦
        loss.backward()

        # ä»æœ€åé˜¶æ®µå¼€å§‹åå‘ä¼ æ’­
        for stage_idx in range(self.num_stages - 2, -1, -1):
            stage_activations = activations[stage_idx]
            next_gradients = []

            for act in activations[stage_idx + 1]:
                if act.grad is not None:
                    # ç¡®ä¿æ¢¯åº¦å½¢çŠ¶åŒ¹é…
                    grad = act.grad
                    if grad.shape != stage_activations[0].shape:
                        try:
                            grad = grad.view(stage_activations[0].shape)
                        except:
                            continue
                    next_gradients.append(grad.to(self.device_ids[stage_idx]))

            for i in range(len(stage_activations) - 1, -1, -1):
                if next_gradients and i < len(next_gradients):
                    stage_activations[i].backward(next_gradients[i], retain_graph=True)
```

1F1B è°ƒåº¦çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æµæ°´çº¿ä¸­äº¤æ›¿æ‰§è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œè€Œä¸æ˜¯å…ˆå®Œæˆæ‰€æœ‰å‰å‘ä¼ æ’­å†è¿›è¡Œåå‘ä¼ æ’­ã€‚è¿™ç§ç­–ç•¥æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š

1. **å‡å°‘å†…å­˜ä½¿ç”¨**ï¼šä¸éœ€è¦å­˜å‚¨æ‰€æœ‰å¾®æ‰¹æ¬¡çš„å‰å‘ä¼ æ’­ä¸­é—´ç»“æœ
2. **é™ä½ç©ºæ³¡ç‡**ï¼šé€šè¿‡æ›´æ—©å¼€å§‹åå‘ä¼ æ’­ï¼Œå‡å°‘è®¾å¤‡ç©ºé—²æ—¶é—´

## 6. Interleaved 1F1B è°ƒåº¦ç­–ç•¥å®ç°

Interleaved 1F1B è°ƒåº¦æ˜¯ä¸€ç§æ”¹è¿›çš„ 1F1B è°ƒåº¦ç­–ç•¥ï¼Œå®ƒé€šè¿‡äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­ï¼Œå¹¶å¼•å…¥é¢å¤–çš„å¡«å……å’Œæ’ç©ºæ­¥éª¤æ¥å‡å°‘ç©ºæ³¡ç‡ã€‚

![](./images/Code03Pipeline04.png)


```python
class InterleavedOneFOneBPipeline(nn.Module):
    def __init__(self, module_list, device_ids, num_microbatches=4, interleaving_degree=2):
        super().__init__()
        assert len(module_list) == len(device_ids), "æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ"

        self.stages = nn.ModuleList(module_list)
        self.device_ids = device_ids
        self.num_stages = len(device_ids)
        self.num_microbatches = num_microbatches
        self.interleaving_degree = interleaving_degree

        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡
        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):
            self.stages[i] = stage.to(dev)

    def forward(self, x):
        """Interleaved 1F1B ç­–ç•¥: æ”¹è¿›çš„ 1F1Bï¼Œæ›´ç»†ç²’åº¦çš„æµæ°´çº¿"""
        # åˆ†å‰²è¾“å…¥ä¸ºå¾®æ‰¹æ¬¡
        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)
        activations = [[] for _ in range(self.num_stages)]
        outputs = []

        # ç®€åŒ–çš„ Interleaved å®ç° - ä½¿ç”¨åˆ†ç»„å¤„ç†
        group_size = self.interleaving_degree

        # å¤„ç†æ¯ä¸ªå¾®æ‰¹æ¬¡ç»„
        for group_start in range(0, self.num_microbatches, group_size):
            group_end = min(group_start + group_size, self.num_microbatches)

            # å¯¹ç»„å†…æ¯ä¸ªå¾®æ‰¹æ¬¡è¿›è¡Œå¤„ç†
            for i in range(group_start, group_end):
                current = micro_batches[i].to(self.device_ids[0])
                for stage_idx in range(self.num_stages):
                    current = self.stages[stage_idx](current)
                    if stage_idx < self.num_stages - 1:
                        current_act = current.detach().clone().requires_grad_(True)
                        activations[stage_idx].append(current_act)
                        current = current_act.to(self.device_ids[stage_idx + 1])
                outputs.append(current)

        output = torch.cat(outputs, dim=0)
        return output, activations

    def backward(self, loss, activations):
        """Interleaved 1F1B åå‘ä¼ æ’­ - ä¿®å¤ç‰ˆæœ¬"""
        # è®¡ç®—æœ€ç»ˆæŸå¤±æ¢¯åº¦
        loss.backward()

        # ä»æœ€åé˜¶æ®µå¼€å§‹åå‘ä¼ æ’­
        for stage_idx in range(self.num_stages - 2, -1, -1):
            stage_activations = activations[stage_idx]
            next_gradients = []

            for act in activations[stage_idx + 1]:
                if act.grad is not None:
                    # ç¡®ä¿æ¢¯åº¦å½¢çŠ¶åŒ¹é…
                    grad = act.grad
                    if grad.shape != stage_activations[0].shape:
                        try:
                            grad = grad.view(stage_activations[0].shape)
                        except:
                            continue
                    next_gradients.append(grad.to(self.device_ids[stage_idx]))

            for i in range(len(stage_activations) - 1, -1, -1):
                if next_gradients and i < len(next_gradients):
                    stage_activations[i].backward(next_gradients[i], retain_graph=True)
```

## 7. æ··åˆå¹¶è¡Œç­–ç•¥

æ··åˆå¹¶è¡Œç»“åˆäº†æ•°æ®å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œå’Œå¼ é‡å¹¶è¡Œï¼Œä»¥å……åˆ†åˆ©ç”¨å¤šç§å¹¶è¡Œç­–ç•¥çš„ä¼˜åŠ¿ã€‚


```python
import torch
import torch.nn as nn

# è¾…åŠ©å‡½æ•°ï¼šè·å–å¯ç”¨ GPU è®¾å¤‡ï¼ˆæ¨¡æ‹Ÿï¼‰
def get_available_devices(max_devices=4):
    devices = []
    for i in range(torch.cuda.device_count()):
        if len(devices) >= max_devices:
            break
        devices.append(torch.device(f'cuda:{i}'))
    if len(devices) == 0:
        devices = [torch.device('cpu')] * min(max_devices, 1)
    return devices

# ç¤ºä¾‹æ¨¡å‹ï¼ˆå¤ç”¨åŸç»“æ„ï¼Œç¡®ä¿å…¼å®¹æ€§ï¼‰
class ExampleModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# æ··åˆå¹¶è¡Œæ¨¡å‹ï¼šPipeline + DataParallel
class HybridParallelModel(nn.Module):
    def __init__(self, base_model, device_ids, dp_size=2, pp_size=2):
        super().__init__()
        self.dp_size = dp_size  # æ•°æ®å¹¶è¡Œè·¯æ•°ï¼ˆæ¯ä¸ª Pipeline é˜¶æ®µçš„å¤åˆ¶ä»½æ•°ï¼‰
        self.pp_size = pp_size  # Pipeline é˜¶æ®µæ•°ï¼ˆæ¨¡å‹åˆ†å‰²åçš„æ®µæ•°ï¼‰
        self.device_ids = device_ids

        # éªŒè¯è®¾å¤‡æ•°é‡ï¼šæ€»è®¾å¤‡æ•° = æ•°æ®å¹¶è¡Œè·¯æ•° Ã— Pipeline é˜¶æ®µæ•°
        assert len(device_ids) == dp_size * pp_size, \
            f"è®¾å¤‡æ•°éœ€ç­‰äºæ•°æ®å¹¶è¡Œè·¯æ•°Ã—Pipeline é˜¶æ®µæ•°ï¼ˆå½“å‰ï¼š{len(device_ids)} != {dp_size}Ã—{pp_size}ï¼‰"

        # 1. Pipeline åˆ†å‰²ï¼šå°†åŸºç¡€æ¨¡å‹æ‹†åˆ†ä¸º pp_size ä¸ªé˜¶æ®µ
        self.pipeline_stages = self._split_model_for_pipeline(base_model, pp_size)

        # 2. æ•°æ®å¹¶è¡Œï¼šä¸ºæ¯ä¸ª Pipeline é˜¶æ®µåˆ›å»º dp_size ä»½å‰¯æœ¬ï¼ˆä½¿ç”¨ nn.DataParallelï¼‰
        self.parallel_stages = nn.ModuleList()
        current_devices = device_ids  # å¾…åˆ†é…çš„è®¾å¤‡åˆ—è¡¨
        for stage in self.pipeline_stages:
            # ä¸ºå½“å‰ Pipeline é˜¶æ®µåˆ†é… dp_size ä¸ªè®¾å¤‡ï¼ˆæ•°æ®å¹¶è¡Œï¼‰
            dp_devices = current_devices[:dp_size]
            current_devices = current_devices[dp_size:]  # å‰©ä½™è®¾å¤‡ç”¨äºä¸‹ä¸€é˜¶æ®µ

            # ğŸ”¥ ä¿®å¤å…³é”®ï¼šå°† stage ç§»åŠ¨åˆ°ç¬¬ä¸€ä¸ªè®¾å¤‡ï¼ˆDataParallel è¦æ±‚ï¼‰
            stage = stage.to(f'cuda:{dp_devices[0]}')

            # åŒ…è£…ä¸ºæ•°æ®å¹¶è¡Œæ¨¡å—
            dp_stage = nn.DataParallel(stage, device_ids=dp_devices)
            self.parallel_stages.append(dp_stage)

    def _split_model_for_pipeline(self, model, pp_size):
        """
        è¾…åŠ©å‡½æ•°ï¼šå°† ExampleModel æŒ‰ Pipeline é€»è¾‘åˆ†å‰²ä¸º pp_size ä¸ªé˜¶æ®µ
        åˆ†å‰²è§„åˆ™ï¼šæ ¹æ®çº¿æ€§å±‚æ‹†åˆ†ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µè®¡ç®—é‡å‡è¡¡
        """
        stages = []
        if pp_size == 2:
            # 2 é˜¶æ®µåˆ†å‰²ï¼š[fc1+relu, fc2+relu+fc3]
            stages.append(nn.Sequential(model.fc1, model.relu))
            stages.append(nn.Sequential(model.fc2, model.relu, model.fc3))
        elif pp_size == 3:
            # 3 é˜¶æ®µåˆ†å‰²ï¼š[fc1+relu, fc2+relu, fc3]
            stages.append(nn.Sequential(model.fc1, model.relu))
            stages.append(nn.Sequential(model.fc2, model.relu))
            stages.append(nn.Sequential(model.fc3))
        else:
            # é»˜è®¤ä¸åˆ†å‰²ï¼ˆpp_size=1ï¼Œä»…æ•°æ®å¹¶è¡Œï¼‰
            stages.append(nn.Sequential(model.fc1, model.relu, model.fc2, model.relu, model.fc3))
        return stages

    def forward(self, x):
        """
        æ··åˆå¹¶è¡Œå‰å‘ä¼ æ’­æµç¨‹ï¼š
        è¾“å…¥ â†’ Pipeline é˜¶æ®µ 1ï¼ˆæ•°æ®å¹¶è¡Œï¼‰â†’ Pipeline é˜¶æ®µ 2ï¼ˆæ•°æ®å¹¶è¡Œï¼‰â†’ è¾“å‡º
        """
        if len(self.parallel_stages) == 0:
            return x

        # ç¡®ä¿è¾“å…¥åœ¨ç¬¬ä¸€ä¸ª stage çš„ç¬¬ä¸€ä¸ªè®¾å¤‡ä¸Š
        first_device = self.parallel_stages[0].device_ids[0]
        current_x = x.to(f'cuda:{first_device}')

        for stage in self.parallel_stages:
            current_x = stage(current_x)  # æ¯ä¸ªé˜¶æ®µå†…éƒ¨æ•°æ®å¹¶è¡Œè®¡ç®—
        return current_x


# ========== ä¸»ç¨‹åºï¼šé…ç½®ä¸æµ‹è¯• ==========

if __name__ == "__main__":
    # 1. æ¨¡å‹å‚æ•°é…ç½®
    input_size, hidden_size, output_size = 100, 200, 10
    base_model = ExampleModel(input_size, hidden_size, output_size)

    # 2. è‡ªåŠ¨è·å–è®¾å¤‡ï¼ˆæ¨¡æ‹Ÿï¼‰
    available_devices = get_available_devices(max_devices=4)
    device_ids = [dev.index for dev in available_devices if dev.type == 'cuda']
    if len(device_ids) == 0:
        print("âš ï¸  æœªæ£€æµ‹åˆ° CUDA è®¾å¤‡ï¼Œå›é€€åˆ° CPU æ¨¡å¼ï¼ˆä¸æ”¯æŒ DataParallelï¼‰")
        device_ids = [0]  # æ¨¡æ‹Ÿ CPU indexï¼Œä½† DataParallel ä¸æ”¯æŒçº¯ CPUï¼Œéœ€ç‰¹æ®Šå¤„ç†
        # ä¸ºæ¼”ç¤ºï¼Œæˆ‘ä»¬å¼ºåˆ¶è‡³å°‘ 2 ä¸ªè®¾å¤‡ï¼Œè‹¥æ—  GPU åˆ™è·³è¿‡å¹¶è¡Œ
        print("âš ï¸  è·³è¿‡å¹¶è¡Œæµ‹è¯•ï¼ˆæ—  GPUï¼‰")
        exit(0)

    # 3. è°ƒæ•´å¹¶è¡Œé…ç½®ä»¥åŒ¹é…è®¾å¤‡æ•°
    dp_size = 2 if len(device_ids) >= 4 else 1
    pp_size = len(device_ids) // dp_size

    print(f"å¯ç”¨è®¾å¤‡: {device_ids}")
    print(f"é…ç½® â†’ æ•°æ®å¹¶è¡Œè·¯æ•°: {dp_size}, Pipeline é˜¶æ®µæ•°: {pp_size}")

    # 4. åˆ›å»ºæ··åˆå¹¶è¡Œæ¨¡å‹
    hybrid_model = HybridParallelModel(
        base_model,
        device_ids=device_ids,
        dp_size=dp_size,
        pp_size=pp_size
    )

    # 5. æµ‹è¯•è¾“å…¥ä¸è¾“å‡º
    x = torch.randn(32, input_size)  # è¾“å…¥ï¼šæ‰¹é‡ 32ï¼Œç»´åº¦ 100
    output = hybrid_model(x)

    # 6. æ‰“å°æµ‹è¯•ç»“æœ
    print(f"\n=== æ··åˆå¹¶è¡Œæµ‹è¯•ç»“æœ ===")
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}, è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"å¹¶è¡Œé…ç½®: æ•°æ®å¹¶è¡Œè·¯æ•°={dp_size}, Pipeline é˜¶æ®µæ•°={pp_size}")
    current_devices = device_ids
    for i in range(pp_size):
        dp_devices = current_devices[:dp_size]
        current_devices = current_devices[dp_size:]
        print(f"Pipeline é˜¶æ®µ {i+1} ç”¨è®¾å¤‡: {dp_devices}")
```

    å¯ç”¨è®¾å¤‡: [0, 1, 2, 3]
    é…ç½® â†’ æ•°æ®å¹¶è¡Œè·¯æ•°: 2, Pipeline é˜¶æ®µæ•°: 2
    
    === æ··åˆå¹¶è¡Œæµ‹è¯•ç»“æœ ===
    è¾“å…¥å½¢çŠ¶: torch.Size([32, 100]), è¾“å‡ºå½¢çŠ¶: torch.Size([32, 10])
    å¹¶è¡Œé…ç½®: æ•°æ®å¹¶è¡Œè·¯æ•°=2, Pipeline é˜¶æ®µæ•°=2
    Pipeline é˜¶æ®µ 1 ç”¨è®¾å¤‡: [0, 1]
    Pipeline é˜¶æ®µ 2 ç”¨è®¾å¤‡: [2, 3]


## 8. å®Œæ•´å®éªŒä¸æ€§èƒ½åˆ†æ

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„æµæ°´çº¿å¹¶è¡Œå®éªŒï¼ŒåŒ…æ‹¬è®­ç»ƒå¾ªç¯å’Œæ€§èƒ½åˆ†æã€‚


```python
def get_gpu_memory_usage(device_ids):
    """è·å–æ‰€æœ‰ GPU çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    memory_usage = {}
    for device in device_ids:
        if device.type == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)  # è½¬æ¢ä¸º GB
            memory_cached = torch.cuda.memory_reserved(device) / (1024 ** 3)  # è½¬æ¢ä¸º GB
            memory_usage[str(device)] = {
                'allocated': memory_allocated,
                'cached': memory_cached
            }
    return memory_usage

def track_memory_usage(device_ids, memory_history):
    """è·Ÿè¸ªæ˜¾å­˜ä½¿ç”¨æƒ…å†µå¹¶è®°å½•åˆ°å†å²"""
    current_memory = get_gpu_memory_usage(device_ids)
    memory_history.append(current_memory)
    return memory_history

def calculate_avg_memory_usage(memory_history):
    """è®¡ç®—å¹³å‡æ˜¾å­˜ä½¿ç”¨é‡"""
    if not memory_history:
        return 0.0

    total_allocated = 0.0
    total_cached = 0.0
    count = 0

    for memory_snapshot in memory_history:
        for device, usage in memory_snapshot.items():
            total_allocated += usage['allocated']
            total_cached += usage['cached']
            count += 1

    if count == 0:
        return 0.0, 0.0

    return total_allocated / count, total_cached / count

# ä¿®æ”¹å®éªŒè¿è¡Œå‡½æ•°
def run_pipeline_experiment(pipeline_class, strategy_name, num_epochs=50, batch_size=256, num_microbatches=32):
    """è¿è¡ŒæŒ‡å®šæµæ°´çº¿ç­–ç•¥çš„å®éªŒ - æ·»åŠ æ˜¾å­˜è·Ÿè¸ª"""
    # 1. è‡ªåŠ¨è·å–è®¾å¤‡ä¸é…ç½®
    device_ids = get_available_devices(max_devices=4)
    num_stages = len(device_ids)
    input_size, output_size = 100, 10

    # æ¸…ç©ºæ˜¾å­˜ç¼“å­˜
    for device in device_ids:
        if device.type == 'cuda':
            torch.cuda.empty_cache()

    # 2. æ„å»º Pipeline æ¨¡å‹
    model_parts = create_model_parts(input_size=input_size, output_size=output_size)
    model_parts = model_parts[:num_stages]

    # æ ¹æ®ç­–ç•¥åç§°é€‰æ‹©ä¸åŒçš„åˆå§‹åŒ–å‚æ•°
    if strategy_name == "Naive":
        pipeline_model = pipeline_class(model_parts, device_ids)
    elif strategy_name == "GPipe":
        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches)
    elif strategy_name == "1F1B":
        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches)
    elif strategy_name == "Interleaved 1F1B":
        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches, interleaving_degree=2)
    else:
        raise ValueError(f"æœªçŸ¥ç­–ç•¥: {strategy_name}")

    # 3. ä¼˜åŒ–å™¨ä¸è®­ç»ƒé…ç½®
    optimizer = torch.optim.Adam(pipeline_model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    losses = []
    times = []
    memory_history = []  # å­˜å‚¨æ˜¾å­˜ä½¿ç”¨å†å²

    # 4. è®­ç»ƒå¾ªç¯
    print(f"\n=== å¼€å§‹ {strategy_name} Pipeline è®­ç»ƒï¼ˆå…±{num_epochs}è½®ï¼‰===")
    for epoch in range(num_epochs):
        start_time = time.time()

        # è®°å½•è®­ç»ƒå‰çš„æ˜¾å­˜ä½¿ç”¨
        memory_history = track_memory_usage(device_ids, memory_history)

        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        x = torch.randn(batch_size, input_size)
        y = torch.randint(0, output_size, (batch_size,))

        # å‰å‘ä¼ æ’­
        outputs, activations = pipeline_model(x)

        # å¤„ç†è¾“å‡ºæ‰¹æ¬¡å¤§å°ä¸åŒ¹é…çš„é—®é¢˜
        if outputs.shape[0] != batch_size:
            y_adjusted = y[:outputs.shape[0]].to(device_ids[-1])
        else:
            y_adjusted = y.to(device_ids[-1])

        loss = F.cross_entropy(outputs, y_adjusted)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        if hasattr(pipeline_model, 'backward'):
            pipeline_model.backward(loss, activations)
        else:
            loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(pipeline_model.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()

        epoch_time = time.time() - start_time
        losses.append(loss.item())
        times.append(epoch_time)

        # è®°å½•è®­ç»ƒåçš„æ˜¾å­˜ä½¿ç”¨
        memory_history = track_memory_usage(device_ids, memory_history)

        if (epoch + 1) % 10 == 0:
            # è®¡ç®—å½“å‰å¹³å‡æ˜¾å­˜ä½¿ç”¨
            avg_allocated, avg_cached = calculate_avg_memory_usage(memory_history)
            print(f"Epoch {epoch+1:3d}/{num_epochs}, æŸå¤±: {loss.item():.4f}, æ—¶é—´: {epoch_time:.4f}s, "
                  f"æ˜¾å­˜: {avg_allocated:.2f}GB/{avg_cached:.2f}GB, LR: {scheduler.get_last_lr()[0]:.6f}")

    # 5. æ€§èƒ½åˆ†æ
    bubble_rate = calculate_bubble_rate(strategy_name, num_stages, num_microbatches)
    avg_time = sum(times) / len(times)
    avg_allocated, avg_cached = calculate_avg_memory_usage(memory_history)

    print(f"\n=== {strategy_name} å®éªŒç»“æœ ===")
    print(f"è®¾å¤‡é…ç½®: {[str(dev) for dev in device_ids]}")
    print(f"æµæ°´çº¿é˜¶æ®µ: {num_stages}, å¾®æ‰¹æ¬¡: {num_microbatches}")
    print(f"ç©ºæ³¡ç‡: {bubble_rate:.3f} ({bubble_rate*100:.1f}%)")
    print(f"å¹³å‡æ¯è½®æ—¶é—´: {avg_time:.4f}s")
    print(f"å¹³å‡æ˜¾å­˜ä½¿ç”¨: {avg_allocated:.2f}GB (åˆ†é…) / {avg_cached:.2f}GB (ç¼“å­˜)")
    print(f"æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")

    # æ”¶æ•›åˆ¤æ–­
    if losses[-1] < 1.0 and losses[-1] < losses[0]:
        print("è®­ç»ƒç»“è®º: æˆåŠŸæ”¶æ•›")
    elif losses[-1] < losses[0]:
        print("è®­ç»ƒç»“è®º: éƒ¨åˆ†æ”¶æ•›")
    else:
        print("è®­ç»ƒç»“è®º: å¯èƒ½æœªæ”¶æ•›")

    return losses, bubble_rate, avg_time, avg_allocated, avg_cached

# æ›´æ–°ç»“æœå±•ç¤ºå‡½æ•°
def print_results_table(results):
    """æ‰“å°ç»“æœè¡¨æ ¼ - æ·»åŠ æ˜¾å­˜ä½¿ç”¨åˆ—"""
    if not results:
        print("æ²¡æœ‰æˆåŠŸè¿è¡Œçš„ç­–ç•¥")
        return

    print("\n=== æ‰€æœ‰ç­–ç•¥ç»¼åˆæ¯”è¾ƒ ===")
    # è¡¨å¤´
    print(f"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+")
    print(f"| {'ç­–ç•¥åç§°':<18} | {'å¹³å‡æ—¶é—´':<10} | {'æœ€ç»ˆæŸå¤±':<10} | {'ç©ºæ³¡ç‡':<10} | {'æ˜¾å­˜(GB)':<10} | {'ç¼“å­˜(GB)':<10} |")
    print(f"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+")

    # è·å– Naive ç­–ç•¥çš„ç»“æœä½œä¸ºåŸºå‡†
    naive_time = results["Naive"]["avg_time"] if "Naive" in results else 1.0
    num_devices = len(get_available_devices(max_devices=4))

    # æ•°æ®è¡Œ
    for strategy, data in results.items():
        # speedup = calculate_speedup(naive_time, data["avg_time"])
        # efficiency = calculate_efficiency(speedup, num_devices)
        print(f"| {strategy:<18} | {data['avg_time']:>10.4f}s | {data['losses'][-1]:>10.4f} | "
              f"{data['bubble_rate']:>10.3f} | "
              f"{data['avg_allocated']:>10.2f} | {data['avg_cached']:>10.2f} |")

    print(f"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+")

```


```python
# ç­–ç•¥ç±»æ˜ å°„
strategy_classes = {
    "Naive": NaivePipelineParallel,
    "GPipe": GPipeParallel,
    "1F1B": OneFOneBPipeline,
    "Interleaved 1F1B": InterleavedOneFOneBPipeline
}

# è¿è¡Œæ‰€æœ‰å››ç§æµæ°´çº¿ç­–ç•¥
results = {}

for strategy_name, strategy_class in strategy_classes.items():
    print(f"\n{'='*60}")
    print(f"æ­£åœ¨è¿è¡Œ {strategy_name} ç­–ç•¥...")
    print(f"{'='*60}")

    try:
        losses, bubble_rate, avg_time, avg_allocated, avg_cached = run_pipeline_experiment(
            strategy_class,
            strategy_name,
            num_epochs=50,
            batch_size=256,
            num_microbatches=32
        )
        results[strategy_name] = {
            "losses": losses,
            "bubble_rate": bubble_rate,
            "avg_time": avg_time,
            "avg_allocated": avg_allocated,
            "avg_cached": avg_cached
        }
    except Exception as e:
        print(f"ç­–ç•¥ {strategy_name} æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    print(f"{'='*60}\n")

# æ‰“å°ç»¼åˆæ¯”è¾ƒç»“æœ
print_results_table(results)
```

    
    ============================================================
    æ­£åœ¨è¿è¡Œ Naive ç­–ç•¥...
    ============================================================
    
    === å¼€å§‹ Naive Pipeline è®­ç»ƒï¼ˆå…± 50 è½®ï¼‰===
    Epoch  10/50, æŸå¤±: 2.3016, æ—¶é—´: 0.0090s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.001000
    Epoch  20/50, æŸå¤±: 2.3015, æ—¶é—´: 0.0084s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.000500
    Epoch  30/50, æŸå¤±: 2.3061, æ—¶é—´: 0.0083s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.000500
    Epoch  40/50, æŸå¤±: 2.3025, æ—¶é—´: 0.0080s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.000250
    Epoch  50/50, æŸå¤±: 2.3019, æ—¶é—´: 0.0078s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.000250
    
    === Naive å®éªŒç»“æœ ===
    è®¾å¤‡é…ç½®: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']
    æµæ°´çº¿é˜¶æ®µ: 4, å¾®æ‰¹æ¬¡: 32
    ç©ºæ³¡ç‡: 0.000 (0.0%)
    å¹³å‡æ¯è½®æ—¶é—´: 0.0088s
    å¹³å‡æ˜¾å­˜ä½¿ç”¨: 0.04GB (åˆ†é…) / 0.08GB (ç¼“å­˜)
    æœ€ç»ˆæŸå¤±: 2.3019
    è®­ç»ƒç»“è®º: éƒ¨åˆ†æ”¶æ•›
    ============================================================
    
    
    ============================================================
    æ­£åœ¨è¿è¡Œ GPipe ç­–ç•¥...
    ============================================================
    
    === å¼€å§‹ GPipe Pipeline è®­ç»ƒï¼ˆå…± 50 è½®ï¼‰===
    Epoch  10/50, æŸå¤±: 2.3045, æ—¶é—´: 0.0510s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.001000
    Epoch  20/50, æŸå¤±: 2.3078, æ—¶é—´: 0.0513s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500
    Epoch  30/50, æŸå¤±: 2.3016, æ—¶é—´: 0.0511s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500
    Epoch  40/50, æŸå¤±: 2.3064, æ—¶é—´: 0.0512s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250
    Epoch  50/50, æŸå¤±: 2.3032, æ—¶é—´: 0.0515s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250
    
    === GPipe å®éªŒç»“æœ ===
    è®¾å¤‡é…ç½®: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']
    æµæ°´çº¿é˜¶æ®µ: 4, å¾®æ‰¹æ¬¡: 32
    ç©ºæ³¡ç‡: 0.086 (8.6%)
    å¹³å‡æ¯è½®æ—¶é—´: 0.0514s
    å¹³å‡æ˜¾å­˜ä½¿ç”¨: 0.01GB (åˆ†é…) / 0.03GB (ç¼“å­˜)
    æœ€ç»ˆæŸå¤±: 2.3032
    è®­ç»ƒç»“è®º: éƒ¨åˆ†æ”¶æ•›
    ============================================================
    
    
    ============================================================
    æ­£åœ¨è¿è¡Œ 1F1B ç­–ç•¥...
    ============================================================
    
    === å¼€å§‹ 1F1B Pipeline è®­ç»ƒï¼ˆå…± 50 è½®ï¼‰===
    Epoch  10/50, æŸå¤±: 2.3094, æ—¶é—´: 0.0570s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.001000
    Epoch  20/50, æŸå¤±: 2.3015, æ—¶é—´: 0.0568s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500
    Epoch  30/50, æŸå¤±: 2.3067, æ—¶é—´: 0.0567s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500
    Epoch  40/50, æŸå¤±: 2.3056, æ—¶é—´: 0.0572s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250
    Epoch  50/50, æŸå¤±: 2.3039, æ—¶é—´: 0.0569s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250
    
    === 1F1B å®éªŒç»“æœ ===
    è®¾å¤‡é…ç½®: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']
    æµæ°´çº¿é˜¶æ®µ: 4, å¾®æ‰¹æ¬¡: 32
    ç©ºæ³¡ç‡: 0.094 (9.4%)
    å¹³å‡æ¯è½®æ—¶é—´: 0.0572s
    å¹³å‡æ˜¾å­˜ä½¿ç”¨: 0.01GB (åˆ†é…) / 0.03GB (ç¼“å­˜)
    æœ€ç»ˆæŸå¤±: 2.3039
    è®­ç»ƒç»“è®º: å¯èƒ½æœªæ”¶æ•›
    ============================================================
    
    
    ============================================================
    æ­£åœ¨è¿è¡Œ Interleaved 1F1B ç­–ç•¥...
    ============================================================
    
    === å¼€å§‹ Interleaved 1F1B Pipeline è®­ç»ƒï¼ˆå…± 50 è½®ï¼‰===
    Epoch  10/50, æŸå¤±: 2.3026, æ—¶é—´: 0.0515s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.001000
    Epoch  20/50, æŸå¤±: 2.2959, æ—¶é—´: 0.0517s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500
    Epoch  30/50, æŸå¤±: 2.3065, æ—¶é—´: 0.0519s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500
    Epoch  40/50, æŸå¤±: 2.3047, æ—¶é—´: 0.0519s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250
    Epoch  50/50, æŸå¤±: 2.3014, æ—¶é—´: 0.0516s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250
    
    === Interleaved 1F1B å®éªŒç»“æœ ===
    è®¾å¤‡é…ç½®: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']
    æµæ°´çº¿é˜¶æ®µ: 4, å¾®æ‰¹æ¬¡: 32
    ç©ºæ³¡ç‡: 0.047 (4.7%)
    å¹³å‡æ¯è½®æ—¶é—´: 0.0521s
    å¹³å‡æ˜¾å­˜ä½¿ç”¨: 0.01GB (åˆ†é…) / 0.03GB (ç¼“å­˜)
    æœ€ç»ˆæŸå¤±: 2.3014
    è®­ç»ƒç»“è®º: éƒ¨åˆ†æ”¶æ•›
    ============================================================
    
    
    === æ‰€æœ‰ç­–ç•¥ç»¼åˆæ¯”è¾ƒ ===
    +--------------------+------------+------------+------------+------------+------------+
    | ç­–ç•¥åç§°               | å¹³å‡æ—¶é—´       | æœ€ç»ˆæŸå¤±       | ç©ºæ³¡ç‡        | æ˜¾å­˜(GB)     | ç¼“å­˜(GB)     |
    +--------------------+------------+------------+------------+------------+------------+
    | Naive              |     0.0088s |     2.3019 |      0.000 |       0.04 |       0.08 |
    | GPipe              |     0.0514s |     2.3032 |      0.086 |       0.01 |       0.03 |
    | 1F1B               |     0.0572s |     2.3039 |      0.094 |       0.01 |       0.03 |
    | Interleaved 1F1B   |     0.0521s |     2.3014 |      0.047 |       0.01 |       0.03 |
    +--------------------+------------+------------+------------+------------+------------+


è¿™ä¸ªå®Œæ•´å®éªŒå±•ç¤ºäº†æµæ°´çº¿å¹¶è¡Œçš„å®é™…åº”ç”¨ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆ†å‰²ã€è®­ç»ƒå¾ªç¯å’Œç©ºæ³¡ç‡åˆ†æã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿˜éœ€è¦è€ƒè™‘æ¢¯åº¦åŒæ­¥ã€è®¾å¤‡é—´é€šä¿¡ä¼˜åŒ–ç­‰å¤æ‚é—®é¢˜ã€‚



## æ€»ç»“ä¸æ€è€ƒ

é€šè¿‡è¡¥å…… Interleaved 1F1B å®ç°ï¼Œæˆ‘ä»¬å®Œæˆäº† Pipeline å¹¶è¡Œä¸‰å¤§æ ¸å¿ƒè°ƒåº¦ç­–ç•¥çš„è¦†ç›–ï¼š

1. **Gpipe (Native PP)**ï¼šç®€å•ç›´è§‚ï¼Œç©ºæ³¡ç‡é«˜ï¼Œæ˜¾å­˜å ç”¨å¤§ã€‚

2. **1F1B**ï¼šé€šè¿‡å‰å‘/åå‘äº¤æ›¿ï¼Œé™ä½æ˜¾å­˜å ç”¨ï¼Œå‹ç¼©éƒ¨åˆ†ç©ºæ³¡ã€‚

3. **Interleaved 1F1B**ï¼šå¼•å…¥è™šæ‹Ÿé˜¶æ®µï¼Œåœ¨åŒä¸€è®¾å¤‡ä¸Šäº¤ç»‡æ‰§è¡Œå¤šä¸ªå¾®æ‰¹æ¬¡ï¼Œè¿›ä¸€æ­¥å‹ç¼©ç©ºæ³¡ï¼Œå°¤å…¶é€‚åˆå¤§å¾®æ‰¹æ¬¡åœºæ™¯ã€‚

å·¥ç¨‹å»ºè®®ï¼š

- å¾®æ‰¹æ¬¡æ•°é‡ M åº”è¿œå¤§äºé˜¶æ®µæ•° Sï¼ˆæ¨è M >= 4Sï¼‰ã€‚
- Interleaved 1F1B åœ¨ M >> S æ—¶ä¼˜åŠ¿æ˜æ˜¾ï¼Œä½†å®ç°å¤æ‚åº¦é«˜ã€‚
- æ··åˆå¹¶è¡Œï¼ˆDP+PP+TPï¼‰æ˜¯å¤§æ¨¡å‹è®­ç»ƒæ ‡é…ï¼Œéœ€é…åˆæ¢¯åº¦æ£€æŸ¥ç‚¹ã€é€šä¿¡ä¼˜åŒ–ç­‰æŠ€æœ¯..

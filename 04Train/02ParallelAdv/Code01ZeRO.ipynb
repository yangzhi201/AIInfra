{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcecb31",
   "metadata": {},
   "source": [
    "<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->\n",
    "\n",
    "# CODE 01: ZeRO æ˜¾å­˜ä¼˜åŒ–å®è·µ\n",
    "\n",
    "> Author by: è®¸ç¿å²·\n",
    "\n",
    "ç›®å‰**GPU + PyTorch + Megatron + DeepSpeed**æ˜¯å¸¸ç”¨çš„è®­ç»ƒè¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ¡†æ¶ã€‚è€Œå¾®è½¯å¼€å‘çš„**DeepSpeed**çš„æ ¸å¿ƒå°±æ˜¯**ZeRO**(Zero Redundancy Optimizer)ï¼Œå®ƒæ˜¯ä¸€ç§æ˜¾å­˜ä¼˜åŒ–çš„**æ•°æ®å¹¶è¡Œ**(data parallelismï¼ŒDP)æ–¹æ¡ˆã€‚**ZeRO**æŠ€æœ¯é€šè¿‡æ¶ˆé™¤**æ•°æ®å¹¶è¡Œ**ä¸­çš„æ˜¾å­˜å†—ä½™ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå¤§æ¨¡å‹æ‰€éœ€çš„æ˜¾å­˜ã€‚\n",
    "\n",
    "æœ¬å®éªŒå°†æ·±å…¥æ¢è®¨ ZeRO çš„å„çº§ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡**çœŸå®å¤šGPUç¯å¢ƒ**çš„ä»£ç æ¼”ç¤ºå’Œåˆ†æï¼Œç†è§£ä¸åŒçº§åˆ«çš„ ZeRO å¦‚ä½•å®ç°æ˜¾å­˜ä¼˜åŒ–ã€‚\n",
    "\n",
    "## 0.å®éªŒç¯å¢ƒè¦æ±‚\n",
    "\n",
    "- **PyTorch >= 1.12** (æ”¯æŒtorch.distributed)\n",
    "- **CUDA >= 11.0**\n",
    "- **è‡³å°‘2ä¸ªGPU** (å»ºè®®4ä¸ªä»¥ä¸Š)\n",
    "- **è¿è¡Œæ–¹å¼**: \n",
    "\n",
    "    æœ¬notebooké‡‡ç”¨**å•æ–‡ä»¶è¿è¡Œ**æ–¹å¼ï¼Œé€šè¿‡ä»¥ä¸‹æœºåˆ¶å®ç°åˆ†å¸ƒå¼è®­ç»ƒï¼š\n",
    "    \n",
    "    1. ä½¿ç”¨ `%%writefile` åˆ›å»ºä¸´æ—¶Pythonè„šæœ¬\n",
    "    2. è‡ªåŠ¨è°ƒç”¨ `torchrun` å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "    3. è®­ç»ƒå®Œæˆåè‡ªåŠ¨åˆ é™¤ä¸´æ—¶è„šæœ¬\n",
    "    \n",
    "    **é€‚ç”¨åœºæ™¯**ï¼š\n",
    "    - è¿œç¨‹æœåŠ¡å™¨ï¼ˆUnix/Linuxï¼‰\n",
    "    - Dockerå®¹å™¨ç¯å¢ƒ\n",
    "    - Jupyter Notebookç¯å¢ƒ\n",
    "    \n",
    "    **ä½¿ç”¨æ–¹æ³•**ï¼š\n",
    "    - ç›´æ¥è¿è¡Œnotebookä¸­çš„æ‰€æœ‰cellå³å¯\n",
    "    - ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹GPUæ•°é‡å¹¶å¯åŠ¨ç›¸åº”æ•°é‡çš„è¿›ç¨‹\n",
    "    - æ— éœ€æ‰‹åŠ¨è¿è¡Œtorchrunå‘½ä»¤\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a288e",
   "metadata": {},
   "source": [
    "æ£€æµ‹è¿è¡Œç¯å¢ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195c548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æµ‹åˆ° 4 ä¸ªGPU\n",
      "âœ… å¤šGPUç¯å¢ƒï¼Œå°†ä½¿ç”¨ torchrun å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ (å»ºè®®ä½¿ç”¨ 4 ä¸ªGPU)\n",
      "ğŸ“ åç»­å®éªŒå°†é€šè¿‡ %%writefile åˆ›å»ºä¸´æ—¶è„šæœ¬ï¼Œè‡ªåŠ¨è¿è¡Œ torchrunï¼Œå¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
      "\n",
      "å®éªŒé…ç½®:\n",
      "  - GPUæ•°é‡: 4\n",
      "  - CUDAå¯ç”¨: True\n",
      "  - PyTorchç‰ˆæœ¬: 2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# æ£€æµ‹GPUæ•°é‡\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(f\"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU\")\n",
    "\n",
    "if gpu_count >= 2:\n",
    "    print(f\"âœ… å¤šGPUç¯å¢ƒï¼Œå°†ä½¿ç”¨ torchrun å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ (å»ºè®®ä½¿ç”¨ {gpu_count} ä¸ªGPU)\")\n",
    "    print(\"ğŸ“ åç»­å®éªŒå°†é€šè¿‡ %%writefile åˆ›å»ºä¸´æ—¶è„šæœ¬ï¼Œè‡ªåŠ¨è¿è¡Œ torchrunï¼Œå¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°å°‘äº2ä¸ªGPUï¼Œåˆ†å¸ƒå¼è®­ç»ƒå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œ\")\n",
    "\n",
    "print(f\"\\nå®éªŒé…ç½®:\")\n",
    "print(f\"  - GPUæ•°é‡: {gpu_count}\")\n",
    "print(f\"  - CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"  - PyTorchç‰ˆæœ¬: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859af74",
   "metadata": {},
   "source": [
    "## 1. æ¨¡å‹æ˜¾å­˜å ç”¨åˆ†æ\n",
    "\n",
    "åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ï¼Œæ˜¾å­˜å ç”¨å¯ä»¥åˆ†ä¸º**Residual States**å’Œ**Model State**ä¸¤éƒ¨åˆ†ï¼š\n",
    "\n",
    "**Residual States**ï¼š\n",
    "- **ä¸­é—´æ¿€æ´»å€¼**ï¼ˆActivationsï¼‰ï¼šåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚ä¼šäº§ç”Ÿä¸­é—´æ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼éœ€è¦åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚\n",
    "- **ä¸´æ—¶ç¼“å†²åŒº**ï¼ˆtemporary buffersï¼‰ï¼šåˆ†å¸ƒå¼é€šä¿¡çš„ä¸´æ—¶å­˜å‚¨ç©ºé—´ã€‚\n",
    "- **ä¸å¯ç”¨çš„ç¢ç‰‡åŒ–å†…å­˜** ï¼ˆunusable fragmented memoryï¼‰ï¼šç”±äºæ•°æ®å¤„ç†å’Œå­˜å‚¨çš„æ•ˆç‡é—®é¢˜ï¼Œæ•°æ®å­˜å‚¨åœ¨æ˜¾å­˜ä¸­çš„æ•°æ®ä¼šå­˜åœ¨ç¢ç‰‡åŒ–ï¼Œä»è€Œå¯¼è‡´æ˜¾å­˜å ç”¨ç‡ä½äºå®é™…éœ€æ±‚ã€‚\n",
    "\n",
    "**Model State**ï¼š\n",
    "\n",
    "- **ä¼˜åŒ–å™¨çŠ¶æ€**ï¼ˆOptimizer Statesï¼‰ï¼šæ˜¯Optimizeråœ¨è¿›è¡Œæ¢¯åº¦æ›´æ–°æ—¶æ‰€éœ€è¦ç”¨åˆ°æ•°æ®ï¼ˆå¦‚ Adam ä¸­çš„åŠ¨é‡å’Œæ–¹å·®ï¼‰ã€‚\n",
    "- **æ¨¡å‹å‚æ•°**ï¼ˆParametersï¼‰ï¼šæ¨¡å‹çš„å¯å­¦ä¹ æƒé‡ï¼Œå¦‚å­˜å‚¨åœ¨æ˜¾å­˜ä¸­çš„æ¨¡å‹æƒé‡å’Œåç½®é¡¹ã€‚\n",
    "- **æ¢¯åº¦**ï¼ˆGradientsï¼‰ï¼šåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ï¼Œç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ã€‚\n",
    "\n",
    "å®ƒä»¬ä¸‰ä¸ªç®€ç§°**OPG**ï¼Œå…¶ä¸­**ä¼˜åŒ–å™¨çŠ¶æ€**ä¼šå æ®å¤§çº¦2å€å‚æ•°é‡çš„æ˜¾å­˜ç©ºé—´ï¼Œè¿™å–å†³äºé€‰æ‹©çš„ä¼˜åŒ–å™¨ï¼Œä¹Ÿæ˜¯æ•´ä¸ªè®­ç»ƒä¸­å æ®æœ€å¤§ç©ºé—´çš„éƒ¨åˆ†ã€‚\n",
    "\n",
    "### 1.1 ç†è®ºè®¡ç®—å…¬å¼\n",
    "\n",
    "![](./images/Code01ZeRO00.png)\n",
    "\n",
    "- ZeRO1ï¼šä¼˜åŒ–å™¨ åˆ‡åˆ†ï¼ˆ$P_{\\text{os}}$ï¼‰ï¼Œçº¦4å€æ˜¾å­˜èŠ‚çº¦ï¼Œé€šè®¯é‡ä¸DPç›¸åŒã€‚\n",
    "- ZeRO2ï¼šä¼˜åŒ–å™¨+æ¢¯åº¦ åˆ‡åˆ†ï¼ˆ$P_{\\text{os+g}}$ï¼‰ï¼Œçº¦8å€æ˜¾å­˜èŠ‚çº¦ï¼Œé€šé€šè®¯é‡ä¸DPç›¸åŒã€‚\n",
    "- ZeRO3ï¼šä¼˜åŒ–å™¨+æ¢¯åº¦+å‚æ•° åˆ‡åˆ†ï¼ˆ$P_{\\text{os+g+p}}$ï¼‰ï¼Œæ˜¾å­˜å‡å°‘ä¸DPåº¦ï¼ˆ$N_d$ï¼‰å‘ˆçº¿æ€§ï¼Œé€šè®¯é‡å¢åŠ 50%ã€‚\n",
    "\n",
    "å›¾ä¸­å„å˜é‡çš„å«ä¹‰å¦‚ä¸‹ï¼š\n",
    "\n",
    "- $\\Psi$ï¼šè¡¨ç¤ºæ¨¡å‹å¤§å°ï¼ˆå‚æ•°æ•°é‡ï¼‰\n",
    "- *K*ï¼šè¡¨ç¤ºä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜å€æ•°\n",
    "- $N_d$ï¼šè¡¨ç¤º DP ç¨‹åº¦\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "æ ¹æ®[ZeROè®ºæ–‡](https://arxiv.org/abs/1910.02054)çš„å‡è®¾ï¼Œæ¨¡å‹å¤§å°ä¸º $\\Psi$=7.5Bï¼ŒDPä¸º $N_d$=64ï¼ŒK=12ï¼š\n",
    "\n",
    "**æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16 + FP32 Adamï¼‰æ˜¾å­˜å ç”¨**ï¼š\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "M_{\\text{total}} &= M_{\\text{param}} + M_{\\text{grad}} + M_{\\text{optim}} + M_{\\text{activation}} \\\\\n",
    "&= 2\\Psi + 2\\Psi + (4\\Psi + 8\\Psi) + M_{\\text{activation}} \\\\\n",
    "&=( 16\\Psi + M_{\\text{activation}} )\\text{ bytes}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "è¯¦ç»†åˆ†è§£ï¼š\n",
    "\n",
    "| ç»„ä»¶ | ç²¾åº¦ | è®¡ç®—å…¬å¼ | è¯´æ˜ |\n",
    "|------|------|----------|------|\n",
    "| æ¨¡å‹å‚æ•° | FP16 | $2\\Psi$ | å‰å‘ä¼ æ’­ä½¿ç”¨çš„åŠç²¾åº¦å‚æ•° |\n",
    "| æ¢¯åº¦ | FP16 | $2\\Psi$ | åå‘ä¼ æ’­è®¡ç®—çš„æ¢¯åº¦ |\n",
    "| FP32ä¸»å‚æ•° | FP32 | $4\\Psi$ | Adamæ›´æ–°éœ€è¦çš„å…¨ç²¾åº¦å‰¯æœ¬ |\n",
    "| åŠ¨é‡ (Momentum) | FP32 | $4\\Psi$ | Adamçš„ä¸€é˜¶çŸ©ä¼°è®¡ $m_t$ |\n",
    "| æ–¹å·® (Variance) | FP32 | $4\\Psi$ | Adamçš„äºŒé˜¶çŸ©ä¼°è®¡ $v_t$ |\n",
    "\n",
    "**ç¤ºä¾‹**ï¼šå¯¹äº7.5Bå‚æ•°çš„æ¨¡å‹ï¼ˆå¦‚LLaMA-7Bï¼‰ï¼š\n",
    "- åŸºç¡€æ˜¾å­˜ï¼š$16 \\times 7.5 \\times 10^9 = 120$ GB\n",
    "- åŠ ä¸Šæ¿€æ´»å€¼ï¼ˆçº¦20GBï¼‰ï¼šæ€»è®¡çº¦ **140 GB**\n",
    "\n",
    "è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆå•å¼ A100ï¼ˆ80GBï¼‰æ— æ³•è®­ç»ƒ7Bæ¨¡å‹ï¼Œéœ€è¦ZeROç­‰æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯ã€‚\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafc9b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "æ˜¾å­˜å ç”¨åˆ†æï¼ˆFP32è®­ç»ƒï¼‰\n",
      "============================================================\n",
      "æ¨¡å‹åŠ è½½                : 0.188 GB (Î” +0.188 GB)\n",
      "åˆ›å»ºä¼˜åŒ–å™¨               : 0.188 GB (Î” +0.000 GB)\n",
      "æ•°æ®åŠ è½½                : 0.188 GB (Î” +0.000 GB)\n",
      "å‰å‘ä¼ æ’­                : 0.199 GB (Î” +0.011 GB)\n",
      "åå‘ä¼ æ’­                : 0.392 GB (Î” +0.193 GB)\n",
      "ä¼˜åŒ–å™¨æ›´æ–°               : 0.767 GB (Î” +0.375 GB)\n",
      "============================================================\n",
      "\n",
      "ç†è®ºå€¼å¯¹æ¯”ï¼ˆFP32ï¼‰ï¼š\n",
      "  å‚æ•°é‡:        50.36M (201.42 MB)\n",
      "  ç†è®ºå‚æ•°æ˜¾å­˜:  201.42 MB\n",
      "  ç†è®ºæ¢¯åº¦æ˜¾å­˜:  201.42 MB\n",
      "  ç†è®ºä¼˜åŒ–å™¨æ˜¾å­˜: 402.85 MB (Adam: m+v)\n",
      "  ç†è®ºæ€»è®¡:      805.70 MB = 0.787 GB\n",
      "  å®æµ‹æ€»è®¡:      0.767 GB\n",
      "  å·®å¼‚:          æ¿€æ´»å€¼ + å…¶ä»–å¼€é”€\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "class MemoryAnalyzer:\n",
    "    \"\"\"æ˜¾å­˜åˆ†æå·¥å…·ï¼ˆç”¨äºå•GPUåŸºå‡†æµ‹è¯•ï¼‰\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory_stats = defaultdict(list)\n",
    "        self.previous_allocated = 0\n",
    "\n",
    "    def record(self, tag=''):\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        delta = allocated - self.previous_allocated\n",
    "        self.previous_allocated = allocated\n",
    "\n",
    "        self.memory_stats['allocated'].append(allocated)\n",
    "        self.memory_stats['reserved'].append(reserved)\n",
    "        self.memory_stats['delta'].append(delta)\n",
    "\n",
    "        print(f\"{tag:20s}: {allocated:.3f} GB (Î” {delta:+.3f} GB)\")\n",
    "        return allocated\n",
    "\n",
    "\n",
    "def create_model(hidden_size=2048, num_layers=12):\n",
    "    \"\"\"åˆ›å»ºæµ‹è¯•æ¨¡å‹\"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_layers):\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def analyze_memory_with_theory(seed=42):\n",
    "    \"\"\"æ˜¾å­˜åˆ†æ + ç†è®ºå€¼å¯¹æ¯”\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDAä¸å¯ç”¨\")\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"æ˜¾å­˜å ç”¨åˆ†æï¼ˆFP32è®­ç»ƒï¼‰\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    param_size_mb = param_count * 4 / 1e6\n",
    "\n",
    "    analyzer.record(\"æ¨¡å‹åŠ è½½\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    analyzer.record(\"åˆ›å»ºä¼˜åŒ–å™¨\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    targets = torch.randn(32, 2048, device='cuda')\n",
    "    analyzer.record(\"æ•°æ®åŠ è½½\")\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, targets)\n",
    "    analyzer.record(\"å‰å‘ä¼ æ’­\")\n",
    "\n",
    "    loss.backward()\n",
    "    analyzer.record(\"åå‘ä¼ æ’­\")\n",
    "\n",
    "    optimizer.step()\n",
    "    final_mem = analyzer.record(\"ä¼˜åŒ–å™¨æ›´æ–°\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nç†è®ºå€¼å¯¹æ¯”ï¼ˆFP32ï¼‰ï¼š\")\n",
    "    print(f\"  å‚æ•°é‡:        {param_count/1e6:.2f}M ({param_size_mb:.2f} MB)\")\n",
    "    print(f\"  ç†è®ºå‚æ•°æ˜¾å­˜:  {param_size_mb:.2f} MB\")\n",
    "    print(f\"  ç†è®ºæ¢¯åº¦æ˜¾å­˜:  {param_size_mb:.2f} MB\")\n",
    "    print(f\"  ç†è®ºä¼˜åŒ–å™¨æ˜¾å­˜: {param_size_mb * 2:.2f} MB (Adam: m+v)\")\n",
    "    print(f\"  ç†è®ºæ€»è®¡:      {param_size_mb * 4:.2f} MB = {param_size_mb * 4 / 1024:.3f} GB\")\n",
    "    print(f\"  å®æµ‹æ€»è®¡:      {final_mem:.3f} GB\")\n",
    "    print(f\"  å·®å¼‚:          æ¿€æ´»å€¼ + å…¶ä»–å¼€é”€\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# è¿è¡Œåˆ†æ\n",
    "memory_stats = analyze_memory_with_theory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430256c3",
   "metadata": {},
   "source": [
    "## 2. ä¼ ç»Ÿæ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰åŸºå‡†æµ‹è¯•\n",
    "\n",
    "### 2.1 æ•°æ®å¹¶è¡ŒåŸç†\n",
    "\n",
    "![](./images/Code01ZeRO05.png)\n",
    "\n",
    "ä¼ ç»Ÿæ•°æ®å¹¶è¡Œï¼ˆDistributed Data Parallel, DDPï¼‰ï¼š\n",
    "\n",
    "å‡è®¾æœ‰Nå¼ å¡ï¼Œæ¯å¼ å¡éƒ½è¦ä¿å­˜ä¸€ä¸ªæ¨¡å‹ï¼Œæ¯æ¬¡è¿­ä»£(iteration/step)éƒ½å°†batchæ•°æ®åˆ†éš”æˆNä¸ªå¤§å°çš„micro-batchï¼Œæ¯å¼ å¡æ ¹æ®æ‹¿åˆ°çš„micro-batchæ•°æ®ç‹¬ç«‹è®¡ç®—æ¢¯åº¦ï¼Œç„¶åè°ƒç”¨**AllReduce**è®¡ç®—æ¢¯åº¦å‡å€¼ï¼Œæ¯å¼ å¡åœ¨ç‹¬ç«‹è¿›è¡Œå‚æ•°æ›´æ–°\n",
    "\n",
    "ç‰¹ç‚¹ï¼š\n",
    "\n",
    "- æ¯ä¸ªGPUä¿å­˜**å®Œæ•´**çš„æ¨¡å‹å‰¯æœ¬\n",
    "- æ¯ä¸ªGPUå¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡\n",
    "- åå‘ä¼ æ’­åé€šè¿‡**All-Reduce**åŒæ­¥æ¢¯åº¦\n",
    "\n",
    "### 2.2 æ˜¾å­˜å†—ä½™é—®é¢˜\n",
    "\n",
    "åœ¨ $N_d$ ä¸ªGPUä¸Šï¼Œæ€»æ˜¾å­˜å ç”¨ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "M_{\\text{total}}^{\\text{DDP}} = N_d \\times (2\\Psi + 2\\Psi + 12\\Psi) = 16\\Psi \\times N_d\n",
    "$$\n",
    "\n",
    "**å†—ä½™åº¦**ï¼šæ¯ä¸ªGPUéƒ½å­˜å‚¨å®Œæ•´çš„ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œé€ æˆ $N_d$ å€å†—ä½™ã€‚\n",
    "\n",
    "### 2.3 é€šä¿¡å¼€é”€\n",
    "\n",
    "æ ‡å‡†/æœ´ç´ çš„DPï¼Œè¿‡ç¨‹ä¸­éœ€è¦å¯¹æ¢¯åº¦Gè¿›è¡Œä¸€æ¬¡AllReduceï¼ˆReduce-Scatter+All-Gatherï¼‰ï¼Œå°†å„ä¸ªå¡ä¸Šçš„æ¢¯åº¦åšå¹³å‡å¹¶ä¸”æ”¶é›†åˆ°æ¯ä¸ªæœºå™¨ä¸Šï¼Œå•å¡äº§ç”Ÿé€šè®¯é‡çº¦ $2\\Psi$ã€‚\n",
    "\n",
    "$$\n",
    "\\text{Comm}_\\text{Allreduce} =  2\\Psi + 2 \\Psi\n",
    "$$\n",
    "\n",
    "è¿™æ˜¯ZeROå„çº§åˆ«å¯¹æ¯”çš„åŸºå‡†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2928e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp_ddp_baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp_ddp_baseline.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "\n",
    "def run_ddp_baseline():\n",
    "    \"\"\"ä¼ ç»ŸDDPåŸºå‡†æµ‹è¯•\"\"\"\n",
    "\n",
    "    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f'cuda:{local_rank}')\n",
    "\n",
    "    # åˆ›å»ºæ¨¡å‹å¹¶åŒ…è£…ä¸ºDDP\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 2048),\n",
    "    ).to(device)\n",
    "\n",
    "    ddp_model = DDP(model, device_ids=[local_rank])\n",
    "    optimizer = torch.optim.Adam(ddp_model.parameters(), lr=1e-3)\n",
    "\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ä¼ ç»ŸDDPåŸºå‡†æµ‹è¯• (World Size = {world_size})\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"å‚æ•°é‡: {param_count/1e6:.2f}M\")\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    # è®­ç»ƒä¸€æ­¥\n",
    "    ddp_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device=device)\n",
    "    outputs = ddp_model(inputs)\n",
    "    loss = outputs.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    peak_mem = torch.cuda.max_memory_allocated(device) / 1e9\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"æ¯ä¸ªGPUå³°å€¼æ˜¾å­˜: {peak_mem:.3f} GB\")\n",
    "        print(f\"æ‰€æœ‰GPUæ€»æ˜¾å­˜:   {peak_mem * world_size:.3f} GB\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    return peak_mem\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_ddp_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7lzas9nc3gp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ 4 ä¸ªGPU)...\n",
      "\n",
      "============================================================\n",
      "ä¼ ç»ŸDDPåŸºå‡†æµ‹è¯• (World Size = 4)\n",
      "============================================================\n",
      "å‚æ•°é‡: 12.59M\n",
      "æ¯ä¸ªGPUå³°å€¼æ˜¾å­˜: 0.320 GB\n",
      "æ‰€æœ‰GPUæ€»æ˜¾å­˜:   1.279 GB\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: temp_ddp_baseline.py\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡ŒDDPåŸºå‡†æµ‹è¯•\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "script_name = \"temp_ddp_baseline.py\"\n",
    "\n",
    "print(f\"ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ {gpu_count} ä¸ªGPU)...\\n\")\n",
    "\n",
    "# è¿è¡Œtorchrun\n",
    "result = subprocess.run(\n",
    "    f\"torchrun --nproc_per_node={gpu_count} {script_name}\",\n",
    "    shell=True,\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "if os.path.exists(script_name):\n",
    "    os.remove(script_name)\n",
    "    print(f\"\\nâœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {script_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39007a36",
   "metadata": {},
   "source": [
    "## 3. ZeRO-1: ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡\n",
    "![](./images/Code01ZeRO01.png)\n",
    "### 3.1 æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "ZeRO-1å°†**ä¼˜åŒ–å™¨çŠ¶æ€**ï¼ˆAdamçš„ $m_t$ å’Œ $v_t$ï¼‰åˆ†ç‰‡åˆ°ä¸åŒGPUï¼Œæ¯ä¸ªGPUåªå­˜å‚¨å’Œæ›´æ–° $1/N_d$ çš„ä¼˜åŒ–å™¨çŠ¶æ€ã€‚\n",
    "\n",
    "### 3.2 æ˜¾å­˜å ç”¨\n",
    "\n",
    "$$\n",
    "M_{\\text{ZeRO-1}} = 2\\Psi + 2\\Psi + \\frac{12\\Psi}{N_d} = 4\\Psi + \\frac{12\\Psi}{N_d}\n",
    "$$\n",
    "\n",
    "**æ˜¾å­˜èŠ‚çœ**ï¼ˆç›¸å¯¹äºDDPï¼‰ï¼š\n",
    "\n",
    "$$\n",
    "\\text{Reduction}_{\\text{ZeRO-1}} = \\frac{12\\Psi - 12\\Psi/N_d}{16\\Psi} = \\frac{3}{4}\\left(1 - \\frac{1}{N_d}\\right)\n",
    "$$\n",
    "\n",
    "- $N_d = 2$: èŠ‚çœ 37.5%\n",
    "- $N_d = 4$: èŠ‚çœ 56.25%\n",
    "- $N_d = 8$: èŠ‚çœ 65.6%\n",
    "\n",
    "### 3.3 é€šä¿¡å¼€é”€\n",
    "\n",
    "å°†ä¼˜åŒ–å™¨çš„çŠ¶æ€å¹³å‡Shardåˆ°å„ä¸ªæœºå™¨ä¸Šï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¦–å…ˆéœ€è¦è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œä½¿ç”¨ä¸€æ¬¡All-Reduceæ”¶é›†å„ä¸ªæœºå™¨ä¸Šçš„æ•°æ®ï¼Œä¹‹åå†è¿›è¡Œä¸€æ¬¡All-Gatherå°†å„æœºå™¨ä¸Šçš„ä¼˜åŒ–å™¨çŠ¶æ€æ‹‰å–è¿‡æ¥ï¼Œå¹¶å¯¹è‡ªå·±æœ¬åœ°çš„ä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œæ›´æ–°ã€‚\n",
    "\n",
    "$$\n",
    "\\text{Comm}_{\\text{ZeRO-1}} = \\underbrace{\\frac{2\\Psi(N_d-1)}{N_d}}_{\\text{Reduce-Scatter}} + \\underbrace{\\frac{2\\Psi(N_d-1)}{N_d}}_{\\text{All-Gather}} \\approx 4\\Psi\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4dad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp_zero1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp_zero1.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "class ZeRO1Optimizer:\n",
    "    \"\"\"\n",
    "    ZeRO-1: ä»…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€\n",
    "\n",
    "    å®ç°è¦ç‚¹:\n",
    "    - å‚æ•°å’Œæ¢¯åº¦åœ¨æ‰€æœ‰GPUä¸Šä¿æŒå®Œæ•´å‰¯æœ¬\n",
    "    - æ¯ä¸ªGPUåªä¸ºå…¶è´Ÿè´£çš„å‚æ•°åˆ†ç‰‡åˆ›å»ºä¼˜åŒ–å™¨çŠ¶æ€\n",
    "    - ä½¿ç”¨All-ReduceåŒæ­¥æ¢¯åº¦\n",
    "    - ä½¿ç”¨All-GatheråŒæ­¥æ›´æ–°åçš„å‚æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: List[nn.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: tuple = (0.9, 0.999),\n",
    "        eps: float = 1e-8\n",
    "    ):\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        self.all_params = list(params)\n",
    "        self.num_params = len(self.all_params)\n",
    "\n",
    "        # å‚æ•°åˆ†ç‰‡\n",
    "        params_per_rank = (self.num_params + self.world_size - 1) // self.world_size\n",
    "        start_idx = self.rank * params_per_rank\n",
    "        end_idx = min(start_idx + params_per_rank, self.num_params)\n",
    "\n",
    "        self.local_params = self.all_params[start_idx:end_idx]\n",
    "\n",
    "        # åªä¸ºæœ¬åœ°åˆ†ç‰‡åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆèŠ‚çœä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜ï¼‰\n",
    "        if len(self.local_params) > 0:\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                self.local_params,\n",
    "                lr=lr,\n",
    "                betas=betas,\n",
    "                eps=eps\n",
    "            )\n",
    "        else:\n",
    "            dummy_param = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "            self.optimizer = torch.optim.Adam([dummy_param], lr=lr)\n",
    "            self.local_params = []\n",
    "\n",
    "        # è®°å½•å‚æ•°å½’å±\n",
    "        self.param_to_rank = {}\n",
    "        for idx, param in enumerate(self.all_params):\n",
    "            owner_rank = idx // params_per_rank\n",
    "            self.param_to_rank[param] = min(owner_rank, self.world_size - 1)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.all_params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        ä¼˜åŒ–æ­¥éª¤:\n",
    "        1. All-Reduce: åŒæ­¥æ¢¯åº¦ï¼ˆæ‰€æœ‰GPUè·å¾—ç›¸åŒçš„æ¢¯åº¦å’Œï¼‰\n",
    "        2. æœ¬åœ°æ›´æ–°: æ¯ä¸ªGPUæ›´æ–°è‡ªå·±è´Ÿè´£çš„å‚æ•°\n",
    "        3. All-Gather: å¹¿æ’­æ›´æ–°åçš„å‚æ•°\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: All-Reduceæ¢¯åº¦\n",
    "        for param in self.all_params:\n",
    "            if param.grad is not None and self.world_size > 1:\n",
    "                dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
    "                param.grad.data /= self.world_size\n",
    "\n",
    "        # Step 2: æœ¬åœ°æ›´æ–°ï¼ˆåªæ›´æ–°æœ¬rankçš„å‚æ•°ï¼‰\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Step 3: All-Gatherå‚æ•°ï¼ˆæ‰€æœ‰rankéƒ½å‚ä¸å¹¿æ’­ï¼‰\n",
    "        if self.world_size > 1:\n",
    "            for param in self.all_params:\n",
    "                owner_rank = self.param_to_rank[param]\n",
    "                dist.broadcast(param.data, src=owner_rank)\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "\n",
    "def run_zero1_experiment():\n",
    "    \"\"\"ZeRO-1å®éªŒ\"\"\"\n",
    "\n",
    "    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f'cuda:{local_rank}')\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 2048),\n",
    "    ).to(device)\n",
    "\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ZeRO-1 å®éªŒ (World Size = {world_size})\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"å‚æ•°é‡: {param_count/1e6:.2f}M\")\n",
    "\n",
    "    optimizer = ZeRO1Optimizer(model.parameters(), lr=1e-3)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    # è®­ç»ƒä¸€æ­¥\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device=device)\n",
    "    outputs = model(inputs)\n",
    "    loss = outputs.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    peak_mem = torch.cuda.max_memory_allocated(device) / 1e9\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"æ¯ä¸ªGPUå³°å€¼æ˜¾å­˜: {peak_mem:.3f} GB\")\n",
    "        print(f\"ç†è®ºèŠ‚çœ: ~{(1 - 1/world_size) * 75:.1f}%\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    return peak_mem\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_zero1_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da7893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨ZeRO-1åˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ 4 ä¸ªGPU)...\n",
      "\n",
      "============================================================\n",
      "ZeRO-1 å®éªŒ (World Size = 4)\n",
      "============================================================\n",
      "å‚æ•°é‡: 12.59M\n",
      "æ¯ä¸ªGPUå³°å€¼æ˜¾å­˜: 0.169 GB\n",
      "ç†è®ºèŠ‚çœ: ~56.2%\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: temp_zero1.py\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡ŒZeRO-1å®éªŒ\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "script_name = \"temp_zero1.py\"\n",
    "\n",
    "print(f\"ğŸš€ å¯åŠ¨ZeRO-1åˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ {gpu_count} ä¸ªGPU)...\\n\")\n",
    "\n",
    "# è¿è¡Œtorchrun\n",
    "result = subprocess.run(\n",
    "    f\"torchrun --nproc_per_node={gpu_count} {script_name}\",\n",
    "    shell=True,\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "if os.path.exists(script_name):\n",
    "    os.remove(script_name)\n",
    "    print(f\"\\nâœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {script_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad366081",
   "metadata": {},
   "source": [
    "## 4. ZeRO-2: ä¼˜åŒ–å™¨çŠ¶æ€ + æ¢¯åº¦åˆ†ç‰‡\n",
    "![](./images/Code01ZeRO02.png)\n",
    "### 4.1 æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "ZeRO-2åœ¨ZeRO-1çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å°†**æ¢¯åº¦**ä¹Ÿè¿›è¡Œåˆ†ç‰‡ã€‚åœ¨ä¼ ç»Ÿæ•°æ®å¹¶è¡Œä¸­ï¼Œæ¯ä¸ªGPUåœ¨åå‘ä¼ æ’­åéƒ½ä¿å­˜å®Œæ•´çš„æ¢¯åº¦å‰¯æœ¬ï¼Œè¿™ä¸å‚æ•°å¤§å°ç›¸å½“ã€‚ZeRO-2é€šè¿‡**reduce-scatter**é€šä¿¡åŸè¯­ï¼Œå®ç°æ¢¯åº¦çš„èšåˆä¸åˆ†ç‰‡çš„ä¸€æ­¥å®Œæˆã€‚\n",
    "\n",
    "### 4.2 æ˜¾å­˜å ç”¨åˆ†æ\n",
    "\n",
    "æ ¹æ®è®ºæ–‡[1]ä¸­çš„å…¬å¼ï¼Œå¯¹äºå…·æœ‰ $\\Psi$ ä¸ªå‚æ•°çš„æ¨¡å‹ï¼Œä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16å‚æ•°+FP32ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰å’ŒAdamä¼˜åŒ–å™¨æ—¶ï¼š\n",
    "\n",
    "**ä¼ ç»Ÿæ•°æ®å¹¶è¡Œ**æ¯ä¸ªGPUçš„æ˜¾å­˜å ç”¨ï¼š\n",
    "\n",
    "$$\n",
    "M_{\\text{DP}} = 2\\Psi + 2\\Psi + (4\\Psi + 8\\Psi) = 16\\Psi \\text{ bytes}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $2\\Psi$: FP16æ¨¡å‹å‚æ•°\n",
    "- $2\\Psi$: FP16æ¢¯åº¦\n",
    "- $4\\Psi$: FP32ä¸»å‚æ•°ï¼ˆMaster Parametersï¼‰\n",
    "- $4\\Psi$: FP32åŠ¨é‡ï¼ˆMomentumï¼‰\n",
    "- $4\\Psi$: FP32æ–¹å·®ï¼ˆVarianceï¼‰\n",
    "\n",
    "**ZeRO-2** æ¯ä¸ªGPUçš„æ˜¾å­˜å ç”¨ï¼š\n",
    "\n",
    "$$\n",
    "M_{\\text{ZeRO-2}} = 2\\Psi + \\frac{2\\Psi}{N_d} + \\frac{12\\Psi}{N_d} = 2\\Psi + \\frac{14\\Psi}{N_d} \\text{ bytes}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $N_d$ æ˜¯æ•°æ®å¹¶è¡Œåº¦ï¼ˆGPUæ•°é‡ï¼‰ã€‚\n",
    "\n",
    "**æ˜¾å­˜å‡å°‘æ¯”ä¾‹**ï¼š\n",
    "\n",
    "$$\n",
    "\\text{Memory Reduction} = \\frac{16\\Psi - (2\\Psi + 14\\Psi/N_d)}{16\\Psi} = \\frac{7}{8} \\cdot \\left(1 - \\frac{1}{N_d}\\right)\n",
    "$$\n",
    "\n",
    "å…·ä½“æ•°å€¼ï¼š\n",
    "- $N_d = 2$: èŠ‚çœ 43.75%\n",
    "- $N_d = 4$: èŠ‚çœ 65.6%\n",
    "- $N_d = 8$: èŠ‚çœ 76.6%\n",
    "\n",
    "### 4.3 é€šä¿¡æµç¨‹\n",
    "\n",
    "ZeRO-2çš„å…³é”®æ˜¯**Reduce-Scatter**æ“ä½œï¼Œå…¶æ•°å­¦å®šä¹‰ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{g}_i^{\\text{local}} = \\text{ReduceScatter}\\left(\\{\\mathbf{g}_0, \\mathbf{g}_1, \\ldots, \\mathbf{g}_{N_d-1}\\}\\right)_i\n",
    "$$\n",
    "\n",
    "å³å°†æ‰€æœ‰GPUçš„æ¢¯åº¦æŒ‰å…ƒç´ æ±‚å’Œåï¼Œå°†ç»“æœåˆ†ç‰‡åˆ†å‘åˆ°å¯¹åº”çš„GPUã€‚\n",
    "\n",
    "å®Œæ•´é€šä¿¡æµç¨‹ï¼š\n",
    "\n",
    "1. **Backward**: æ‰€æœ‰GPUè®¡ç®—å®Œæ•´æ¢¯åº¦ $\\nabla L(\\theta)$\n",
    "2. **Reduce-Scatter**: èšåˆæ¢¯åº¦å¹¶åˆ†ç‰‡\n",
    "   - GPU $i$ æ”¶åˆ°å‚æ•°åˆ†ç‰‡ $P_i$ å¯¹åº”çš„èšåˆæ¢¯åº¦ $\\sum_{j=0}^{N_d-1} \\nabla L(\\theta)_{P_i}$\n",
    "3. **æœ¬åœ°æ›´æ–°**: æ¯ä¸ªGPUåªæ›´æ–°å…¶è´Ÿè´£çš„å‚æ•°åˆ†ç‰‡\n",
    "   $$\n",
    "   \\theta_i \\leftarrow \\theta_i - \\alpha \\cdot \\frac{m_i}{\\sqrt{v_i} + \\epsilon}\n",
    "   $$\n",
    "4. **All-Gather**: åŒæ­¥æ›´æ–°åçš„å‚æ•°åˆ°æ‰€æœ‰GPU\n",
    "   $$\n",
    "   \\theta^{\\text{full}} = \\text{AllGather}(\\{\\theta_0, \\theta_1, \\ldots, \\theta_{N_d-1}\\})\n",
    "   $$\n",
    "\n",
    "### 4.4 é€šä¿¡å¼€é”€\n",
    "\n",
    "å°†ä¼˜åŒ–å™¨çš„çŠ¶æ€ä»¥åŠæ¢¯åº¦å¹³å‡åˆ†åˆ°å„ä¸ªæœºå™¨ä¸Šï¼Œå½“æ¢¯åº¦è®¡ç®—å®Œæˆåï¼ˆåä¼ ï¼‰è¿›è¡Œreduce-scatteræ“ä½œï¼Œæ¯ä¸ªGPUä¿å­˜å±äºå®ƒçš„é‚£ä¸€ä»½1/Næ¢¯åº¦çš„å‡å€¼ï¼Œå…¶ä½™çš„æ¢¯åº¦å°±é‡Šæ”¾æ‰äº†ï¼Œå¹¶åˆ©ç”¨1/Nçš„æ¢¯åº¦æ¥æ›´æ–°1/Nçš„ä¼˜åŒ–å™¨çŠ¶æ€ã€‚åœ¨æ¢¯åº¦æ›´æ–°å‰ï¼Œæˆ‘ä»¬é€šè¿‡All-Gatherå°†æ‰€æœ‰æ¢¯åº¦æ”¶é›†è¿‡æ¥å¹¶ä¸”æ›´æ–°weightsã€‚\n",
    "\n",
    "å¯¹äº $\\Psi$ ä¸ªå‚æ•°ï¼ŒZeRO-2çš„é€šä¿¡é‡ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "\\text{Comm}_{\\text{ZeRO-2}} = \\underbrace{\\frac{2\\Psi(N_d-1)}{N_d}}_{\\text{Reduce-Scatter}} + \\underbrace{\\frac{2\\Psi(N_d-1)}{N_d}}_{\\text{All-Gather}} \\approx 4\\Psi\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp_zero2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp_zero2.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "class ZeRO2Optimizer:\n",
    "    \"\"\"\n",
    "    ZeRO-2ä¼˜åŒ–å™¨ï¼šä¼˜åŒ–å™¨çŠ¶æ€+æ¢¯åº¦åˆ†ç‰‡\n",
    "\n",
    "    å‚æ•°åˆ†ç‰‡ç­–ç•¥ï¼šå°†Nä¸ªå‚æ•°å‡åŒ€åˆ†é…åˆ°world_sizeä¸ªGPU\n",
    "    æ¯ä¸ªGPUåªå­˜å‚¨å’Œæ›´æ–° 1/world_size çš„ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: List[nn.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: tuple = (0.9, 0.999),\n",
    "        eps: float = 1e-8\n",
    "    ):\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        self.all_params = list(params)\n",
    "        self.num_params = len(self.all_params)\n",
    "\n",
    "        # è®¡ç®—å½“å‰rankè´Ÿè´£çš„å‚æ•°ç´¢å¼•èŒƒå›´\n",
    "        params_per_rank = (self.num_params + self.world_size - 1) // self.world_size\n",
    "        start_idx = self.rank * params_per_rank\n",
    "        end_idx = min(start_idx + params_per_rank, self.num_params)\n",
    "\n",
    "        self.local_params = self.all_params[start_idx:end_idx]\n",
    "\n",
    "        # åªä¸ºæœ¬åœ°å‚æ•°åˆ†ç‰‡åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆèŠ‚çœä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜ï¼‰\n",
    "        if len(self.local_params) > 0:\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                self.local_params,\n",
    "                lr=lr,\n",
    "                betas=betas,\n",
    "                eps=eps\n",
    "            )\n",
    "        else:\n",
    "            dummy_param = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "            self.optimizer = torch.optim.Adam([dummy_param], lr=lr)\n",
    "            self.local_params = []\n",
    "\n",
    "        # è®°å½•æ¯ä¸ªå‚æ•°å½’å±çš„rank\n",
    "        self.param_to_rank = {}\n",
    "        for idx, param in enumerate(self.all_params):\n",
    "            owner_rank = idx // params_per_rank\n",
    "            self.param_to_rank[param] = min(owner_rank, self.world_size - 1)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.all_params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼š\n",
    "        1. Reduce-Scatter: èšåˆæ¢¯åº¦åˆ°å¯¹åº”çš„owner rank\n",
    "        2. æœ¬åœ°æ›´æ–°: æ¯ä¸ªrankæ›´æ–°è‡ªå·±çš„å‚æ•°åˆ†ç‰‡\n",
    "        3. All-Gather: å¹¿æ’­æ›´æ–°åçš„å‚æ•°\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Reduceæ¢¯åº¦åˆ°owner rank (æ¨¡æ‹Ÿreduce-scatter)\n",
    "        for param in self.all_params:\n",
    "            if param.grad is not None:\n",
    "                owner_rank = self.param_to_rank[param]\n",
    "\n",
    "                if self.world_size > 1:\n",
    "                    dist.reduce(\n",
    "                        param.grad.data,\n",
    "                        dst=owner_rank,\n",
    "                        op=dist.ReduceOp.SUM\n",
    "                    )\n",
    "\n",
    "                    # éowneré‡Šæ”¾æ¢¯åº¦ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰\n",
    "                    if self.rank != owner_rank:\n",
    "                        param.grad = None\n",
    "\n",
    "        # Step 2: æœ¬åœ°æ›´æ–°\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Step 3: All-Gatherå‚æ•°ï¼ˆæ‰€æœ‰rankéƒ½å‚ä¸å¹¿æ’­ï¼‰\n",
    "        if self.world_size > 1:\n",
    "            for param in self.all_params:\n",
    "                owner_rank = self.param_to_rank[param]\n",
    "                dist.broadcast(param.data, src=owner_rank)\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "\n",
    "def run_zero2_experiment():\n",
    "    \"\"\"ZeRO-2å®éªŒï¼šæµ‹é‡å®é™…æ˜¾å­˜å ç”¨\"\"\"\n",
    "\n",
    "    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f'cuda:{local_rank}')\n",
    "\n",
    "    # åˆ›å»ºæµ‹è¯•æ¨¡å‹\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 2048),\n",
    "    ).to(device)\n",
    "\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    param_memory_mb = param_count * 4 / 1e6  # FP32å‚æ•°æ˜¾å­˜(MB)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    mem_0 = torch.cuda.memory_allocated(device) / 1e9\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ZeRO-2 å®éªŒ (World Size = {world_size})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"å‚æ•°é‡: {param_count/1e6:.2f}M ({param_memory_mb:.2f} MB)\")\n",
    "\n",
    "    # åˆ›å»ºZeRO-2ä¼˜åŒ–å™¨\n",
    "    optimizer = ZeRO2Optimizer(model.parameters(), lr=1e-3)\n",
    "    mem_1 = torch.cuda.memory_allocated(device) / 1e9\n",
    "\n",
    "    # è®­ç»ƒä¸€æ­¥\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device=device)\n",
    "    outputs = model(inputs)\n",
    "    loss = outputs.mean()\n",
    "\n",
    "    mem_2 = torch.cuda.memory_allocated(device) / 1e9\n",
    "\n",
    "    loss.backward()\n",
    "    mem_3 = torch.cuda.memory_allocated(device) / 1e9\n",
    "\n",
    "    optimizer.step()\n",
    "    mem_4 = torch.cuda.memory_allocated(device) / 1e9\n",
    "    peak_mem = torch.cuda.max_memory_allocated(device) / 1e9\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"\\næ˜¾å­˜è¿½è¸ª (Rank 0):\")\n",
    "        print(f\"  æ¨¡å‹åŠ è½½å:     {mem_0:.3f} GB\")\n",
    "        print(f\"  åˆ›å»ºä¼˜åŒ–å™¨å:   {mem_1:.3f} GB (Î” +{mem_1-mem_0:.3f} GB)\")\n",
    "        print(f\"  å‰å‘ä¼ æ’­å:     {mem_2:.3f} GB (Î” +{mem_2-mem_1:.3f} GB)\")\n",
    "        print(f\"  åå‘ä¼ æ’­å:     {mem_3:.3f} GB (Î” +{mem_3-mem_2:.3f} GB)\")\n",
    "        print(f\"  ä¼˜åŒ–å™¨stepå:   {mem_4:.3f} GB (Î” +{mem_4-mem_3:.3f} GB)\")\n",
    "        print(f\"  å³°å€¼æ˜¾å­˜:       {peak_mem:.3f} GB\")\n",
    "        print(f\"  ç†è®ºèŠ‚çœ: ~{(1 - 1/world_size) * 87.5:.1f}%\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    return peak_mem\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_zero2_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7781f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨ZeRO-2åˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ 4 ä¸ªGPU)...\n",
      "\n",
      "\n",
      "============================================================\n",
      "ZeRO-2 å®éªŒ (World Size = 4)\n",
      "============================================================\n",
      "å‚æ•°é‡: 12.59M (50.36 MB)\n",
      "\n",
      "æ˜¾å­˜è¿½è¸ª (Rank 0):\n",
      "  æ¨¡å‹åŠ è½½å:     0.050 GB\n",
      "  åˆ›å»ºä¼˜åŒ–å™¨å:   0.050 GB (Î” +0.000 GB)\n",
      "  å‰å‘ä¼ æ’­å:     0.060 GB (Î” +0.010 GB)\n",
      "  åå‘ä¼ æ’­å:     0.118 GB (Î” +0.058 GB)\n",
      "  ä¼˜åŒ–å™¨stepå:   0.118 GB (Î” +0.000 GB)\n",
      "  å³°å€¼æ˜¾å­˜:       0.135 GB\n",
      "  ç†è®ºèŠ‚çœ: ~65.6%\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: temp_zero2.py\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡ŒZeRO-2å®éªŒ\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "script_name = \"temp_zero2.py\"\n",
    "\n",
    "print(f\"ğŸš€ å¯åŠ¨ZeRO-2åˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ {gpu_count} ä¸ªGPU)...\\n\")\n",
    "\n",
    "# è¿è¡Œtorchrun\n",
    "result = subprocess.run(\n",
    "    f\"torchrun --nproc_per_node={gpu_count} {script_name}\",\n",
    "    shell=True,\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "if os.path.exists(script_name):\n",
    "    os.remove(script_name)\n",
    "    print(f\"\\nâœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {script_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ebc188",
   "metadata": {},
   "source": [
    "## 5. ZeRO-3: ä¼˜åŒ–å™¨çŠ¶æ€ + æ¢¯åº¦ + å‚æ•°åˆ†ç‰‡\n",
    "![](./images/Code01ZeRO03.png)\n",
    "### 5.1 æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "ZeRO-3æ˜¯æœ€æ¿€è¿›çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œå°†**å‚æ•°**ã€**æ¢¯åº¦**å’Œ**ä¼˜åŒ–å™¨çŠ¶æ€**å…¨éƒ¨åˆ†ç‰‡ï¼š\n",
    "- æ¯ä¸ªGPUåªæŒä¹…åŒ–å­˜å‚¨ $1/N_d$ çš„å‚æ•°\n",
    "- å‰å‘ä¼ æ’­æ—¶ï¼Œé€šè¿‡**All-Gather**ä¸´æ—¶æ”¶é›†éœ€è¦çš„å‚æ•°\n",
    "- è®¡ç®—å®Œæˆåç«‹å³é‡Šæ”¾ï¼Œä¿æŒæ˜¾å­˜æœ€å°åŒ–\n",
    "\n",
    "### 5.2 æ˜¾å­˜å ç”¨\n",
    "\n",
    "$$\n",
    "M_{\\text{ZeRO-3}} = \\frac{2\\Psi}{N_d} + \\frac{2\\Psi}{N_d} + \\frac{12\\Psi}{N_d} = \\frac{16\\Psi}{N_d}\n",
    "$$\n",
    "\n",
    "**æ˜¾å­˜èŠ‚çœ**ï¼š\n",
    "\n",
    "$$\n",
    "\\text{Reduction}_{\\text{ZeRO-3}} = \\frac{16\\Psi - 16\\Psi/N_d}{16\\Psi} = 1 - \\frac{1}{N_d}\n",
    "$$\n",
    "\n",
    "- $N_d = 2$: èŠ‚çœ 50%\n",
    "- $N_d = 4$: èŠ‚çœ 75%\n",
    "- $N_d = 8$: èŠ‚çœ 87.5%\n",
    "\n",
    "ç†è®ºä¸Šï¼ŒZeRO-3çš„æ˜¾å­˜å ç”¨ä¸GPUæ•°é‡æˆåæ¯”ã€‚\n",
    "\n",
    "### 5.3 é€šä¿¡å¼€é”€\n",
    "\n",
    "å°†ä¼˜åŒ–å™¨çš„çŠ¶æ€ã€æ¢¯åº¦ä»¥åŠæ¨¡å‹æƒé‡å¹³å‡åˆ†åˆ°å„ä¸ªæœºå™¨ä¸Šã€‚å‰ä¼ æ—¶éœ€è¦å®Œæ•´çš„æ¨¡å‹æƒé‡ï¼Œéœ€è¦ä¸€æ¬¡All-Gatherï¼Œå®Œæˆåé‡Šæ”¾æ‰ä¸å±äºè‡ªå·±çš„æ¨¡å‹æƒé‡ã€‚åä¼ æ—¶éœ€è¦å®Œæ•´çš„æƒé‡ï¼Œéœ€è¦ä¸€æ¬¡All-Gatherã€‚è®¡ç®—æ¢¯åº¦æ—¶ä¸ZeRO2ç›¸åŒï¼Œè¿›è¡ŒReduce-Scatteræ“ä½œä¿å­˜å±äºå®ƒè‡ªå·±çš„1/Nçš„æ¢¯åº¦å‡å€¼ï¼Œå…¶ä½™æ¢¯åº¦é‡Šæ”¾æ‰ï¼Œæ›´æ–°1/Nçš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå¹¶åœ¨æ¢¯åº¦æ›´æ–°æ—¶æ›´æ–°1/Nçš„æƒé‡ã€‚è€Œè¿™é‡Œä¸ZeROä¸åŒçš„æ˜¯ä¸éœ€è¦All-GatheræŠŠæƒé‡æ‹‰è¿‡æ¥äº†ã€‚\n",
    "\n",
    "ZeRO-3çš„é€šä¿¡é‡æœ€å¤§ï¼Œå› ä¸ºæ¯å±‚å‰å‘å’Œåå‘éƒ½éœ€è¦é€šä¿¡ï¼š\n",
    "\n",
    "$$\n",
    "\\text{Comm}_{\\text{ZeRO-3}} = \\underbrace{2\\Psi}_{\\text{Forward All-Gather}} + \\underbrace{2\\Psi}_{\\text{Backward All-Gather}} + \\underbrace{2\\Psi}_{\\text{Reduce-Scatter}}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp_zero3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp_zero3.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from typing import List\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "\n",
    "class ZeRO3Model(nn.Module):\n",
    "    \"\"\"\n",
    "    ZeRO-3åŒ…è£…å™¨: å‚æ•°åˆ†ç‰‡ + åŠ¨æ€All-Gather\n",
    "\n",
    "    å®ç°è¦ç‚¹:\n",
    "    - å°†æ¨¡å‹å‚æ•°åˆ†ç‰‡å­˜å‚¨\n",
    "    - å‰å‘/åå‘ä¼ æ’­æ—¶ä¸´æ—¶æ”¶é›†å®Œæ•´å‚æ•°\n",
    "    - è®¡ç®—å®Œæˆåé‡Šæ”¾å‚æ•°ï¼Œä¿æŒæ˜¾å­˜æœ€å°\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.module = module\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # æ”¶é›†æ‰€æœ‰å‚æ•°\n",
    "        self.params = list(module.parameters())\n",
    "        self.num_params = len(self.params)\n",
    "\n",
    "        # ä¸ºæ¯ä¸ªå‚æ•°åˆ›å»ºåˆ†ç‰‡ç‰ˆæœ¬\n",
    "        self._shard_parameters()\n",
    "\n",
    "    def _shard_parameters(self):\n",
    "        \"\"\"å°†å‚æ•°åˆ†ç‰‡åˆ°å„ä¸ªGPU\"\"\"\n",
    "        params_per_rank = (self.num_params + self.world_size - 1) // self.world_size\n",
    "\n",
    "        for idx, param in enumerate(self.params):\n",
    "            owner_rank = min(idx // params_per_rank, self.world_size - 1)\n",
    "\n",
    "            # ä¿å­˜å®Œæ•´å‚æ•°å½¢çŠ¶\n",
    "            param._zero3_full_shape = param.data.shape\n",
    "            param._zero3_owner_rank = owner_rank\n",
    "\n",
    "            if self.rank == owner_rank:\n",
    "                # Ownerä¿ç•™å®Œæ•´å‚æ•°\n",
    "                param._zero3_full_param = param.data.clone()\n",
    "            else:\n",
    "                # éowneré‡Šæ”¾å‚æ•°æ˜¾å­˜\n",
    "                param.data = torch.empty(0, dtype=param.dtype, device=param.device)\n",
    "                param._zero3_full_param = None\n",
    "\n",
    "    @contextmanager\n",
    "    def _gather_parameters(self):\n",
    "        \"\"\"ä¸´æ—¶æ”¶é›†æ‰€æœ‰å‚æ•°\"\"\"\n",
    "        try:\n",
    "            # All-Gatheræ”¶é›†å‚æ•°\n",
    "            for param in self.params:\n",
    "                owner_rank = param._zero3_owner_rank\n",
    "\n",
    "                # æ¢å¤å®Œæ•´å‚æ•°ç©ºé—´\n",
    "                if param.data.numel() == 0:\n",
    "                    param.data = torch.empty(\n",
    "                        param._zero3_full_shape,\n",
    "                        dtype=param.dtype,\n",
    "                        device=param.device\n",
    "                    )\n",
    "\n",
    "                # å¹¿æ’­å‚æ•°\n",
    "                if self.world_size > 1:\n",
    "                    dist.broadcast(param.data, src=owner_rank)\n",
    "\n",
    "            yield\n",
    "\n",
    "        finally:\n",
    "            # é‡Šæ”¾éæœ¬åœ°å‚æ•°\n",
    "            for param in self.params:\n",
    "                if self.rank != param._zero3_owner_rank:\n",
    "                    param.data = torch.empty(0, dtype=param.dtype, device=param.device)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"å‰å‘ä¼ æ’­æ—¶ä¸´æ—¶æ”¶é›†å‚æ•°\"\"\"\n",
    "        with self._gather_parameters():\n",
    "            return self.module(*args, **kwargs)\n",
    "\n",
    "\n",
    "class ZeRO3Optimizer:\n",
    "    \"\"\"ZeRO-3ä¼˜åŒ–å™¨: é…åˆZeRO3Modelä½¿ç”¨\"\"\"\n",
    "\n",
    "    def __init__(self, model: ZeRO3Model, lr: float = 1e-3):\n",
    "        self.model = model\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "\n",
    "        # åªä¸ºæœ¬rankæ‹¥æœ‰çš„å‚æ•°åˆ›å»ºä¼˜åŒ–å™¨\n",
    "        local_params = [\n",
    "            p for p in model.params\n",
    "            if p._zero3_owner_rank == self.rank\n",
    "        ]\n",
    "\n",
    "        # å¤„ç†ç©ºå‚æ•°åˆ—è¡¨çš„æƒ…å†µ\n",
    "        if len(local_params) > 0:\n",
    "            self.optimizer = torch.optim.Adam(local_params, lr=lr)\n",
    "        else:\n",
    "            dummy_param = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "            self.optimizer = torch.optim.Adam([dummy_param], lr=lr)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        ä¼˜åŒ–æ­¥éª¤:\n",
    "        1. Reduce-Scatter: æ¢¯åº¦èšåˆå¹¶åˆ†ç‰‡\n",
    "        2. æœ¬åœ°æ›´æ–°: æ¯ä¸ªGPUæ›´æ–°è‡ªå·±çš„å‚æ•°åˆ†ç‰‡\n",
    "        3. å‚æ•°ä¿æŒåˆ†ç‰‡çŠ¶æ€ï¼ˆä¸éœ€è¦All-Gatherï¼‰\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Reduceæ¢¯åº¦åˆ°owner\n",
    "        for param in self.model.params:\n",
    "            if param.grad is not None:\n",
    "                owner_rank = param._zero3_owner_rank\n",
    "\n",
    "                if self.world_size > 1:\n",
    "                    dist.reduce(\n",
    "                        param.grad.data,\n",
    "                        dst=owner_rank,\n",
    "                        op=dist.ReduceOp.SUM\n",
    "                    )\n",
    "\n",
    "                    # éowneré‡Šæ”¾æ¢¯åº¦\n",
    "                    if self.rank != owner_rank:\n",
    "                        param.grad = None\n",
    "\n",
    "        # Step 2: æœ¬åœ°æ›´æ–°\n",
    "        self.optimizer.step()\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "\n",
    "def run_zero3_experiment():\n",
    "    \"\"\"ZeRO-3å®éªŒ\"\"\"\n",
    "\n",
    "    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f'cuda:{local_rank}')\n",
    "\n",
    "    # åˆ›å»ºåŸºç¡€æ¨¡å‹\n",
    "    base_model = nn.Sequential(\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 2048),\n",
    "    ).to(device)\n",
    "\n",
    "    param_count = sum(p.numel() for p in base_model.parameters())\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ZeRO-3 å®éªŒ (World Size = {world_size})\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"å‚æ•°é‡: {param_count/1e6:.2f}M\")\n",
    "\n",
    "    # åŒ…è£…ä¸ºZeRO-3æ¨¡å‹\n",
    "    model = ZeRO3Model(base_model)\n",
    "    optimizer = ZeRO3Optimizer(model, lr=1e-3)\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    # è®­ç»ƒä¸€æ­¥\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device=device)\n",
    "    outputs = model(inputs)\n",
    "    loss = outputs.mean()\n",
    "\n",
    "    # åå‘ä¼ æ’­æ—¶ä¹Ÿéœ€è¦æ”¶é›†å‚æ•°\n",
    "    with model._gather_parameters():\n",
    "        loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    peak_mem = torch.cuda.max_memory_allocated(device) / 1e9\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"æ¯ä¸ªGPUå³°å€¼æ˜¾å­˜: {peak_mem:.3f} GB\")\n",
    "        print(f\"ç†è®ºèŠ‚çœ: ~{(1 - 1/world_size) * 100:.1f}%\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    return peak_mem\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_zero3_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0103d03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨ZeRO-3åˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ 4 ä¸ªGPU)...\n",
      "\n",
      "============================================================\n",
      "ZeRO-3 å®éªŒ (World Size = 4)\n",
      "============================================================\n",
      "å‚æ•°é‡: 12.59M\n",
      "æ¯ä¸ªGPUå³°å€¼æ˜¾å­˜: 0.136 GB\n",
      "ç†è®ºèŠ‚çœ: ~75.0%\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: temp_zero3.py\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡ŒZeRO-3å®éªŒ\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "script_name = \"temp_zero3.py\"\n",
    "\n",
    "print(f\"ğŸš€ å¯åŠ¨ZeRO-3åˆ†å¸ƒå¼è®­ç»ƒ (ä½¿ç”¨ {gpu_count} ä¸ªGPU)...\\n\")\n",
    "\n",
    "# è¿è¡Œtorchrun\n",
    "result = subprocess.run(\n",
    "    f\"torchrun --nproc_per_node={gpu_count} {script_name}\",\n",
    "    shell=True,\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "if os.path.exists(script_name):\n",
    "    os.remove(script_name)\n",
    "    print(f\"\\nâœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {script_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc643936",
   "metadata": {},
   "source": [
    "## 6. ç»¼åˆå¯¹æ¯”å®éªŒ\n",
    "\n",
    "æœ¬èŠ‚è¿è¡Œæ‰€æœ‰æ–¹æ³•å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šã€‚\n",
    "\n",
    "### 6.1 ç†è®ºå¯¹æ¯”è¡¨\n",
    "\n",
    "| æ–¹æ³• | å‚æ•°æ˜¾å­˜ | æ¢¯åº¦æ˜¾å­˜ | ä¼˜åŒ–å™¨æ˜¾å­˜ | æ€»è®¡ | é€šä¿¡é‡ |\n",
    "|------|---------|---------|-----------|------|--------|\n",
    "| DDP | $2\\Psi$ | $2\\Psi$ | $12\\Psi$ | $16\\Psi$ | $4\\Psi$ |\n",
    "| ZeRO-1 | $2\\Psi$ | $2\\Psi$ | $12\\Psi/N_d$ | $4\\Psi + 12\\Psi/N_d$ | $4\\Psi$ |\n",
    "| ZeRO-2 | $2\\Psi$ | $2\\Psi/N_d$ | $12\\Psi/N_d$ | $2\\Psi + 14\\Psi/N_d$ | $4\\Psi$ |\n",
    "| ZeRO-3 | $2\\Psi/N_d$ | $2\\Psi/N_d$ | $12\\Psi/N_d$ | $16\\Psi/N_d$ | $6\\Psi$ |\n",
    "\n",
    "### 6.2 æ˜¾å­˜èŠ‚çœå¯¹æ¯”ï¼ˆ$N_d = 4$ï¼‰\n",
    "\n",
    "- **DDP**: 16Î¨ (åŸºå‡†)\n",
    "- **ZeRO-1**: 7Î¨ â†’ èŠ‚çœ 56.25%\n",
    "- **ZeRO-2**: 5.5Î¨ â†’ èŠ‚çœ 65.6%\n",
    "- **ZeRO-3**: 4Î¨ â†’ èŠ‚çœ 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478ffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing temp_all_experiments.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp_all_experiments.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from typing import List\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "\n",
    "# ============== ZeRO-1 Optimizer ==============\n",
    "class ZeRO1Optimizer:\n",
    "    def __init__(self, params: List[nn.Parameter], lr: float = 1e-3, betas: tuple = (0.9, 0.999), eps: float = 1e-8):\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "        self.all_params = list(params)\n",
    "        self.num_params = len(self.all_params)\n",
    "\n",
    "        params_per_rank = (self.num_params + self.world_size - 1) // self.world_size\n",
    "        start_idx = self.rank * params_per_rank\n",
    "        end_idx = min(start_idx + params_per_rank, self.num_params)\n",
    "        self.local_params = self.all_params[start_idx:end_idx]\n",
    "\n",
    "        if len(self.local_params) > 0:\n",
    "            self.optimizer = torch.optim.Adam(self.local_params, lr=lr, betas=betas, eps=eps)\n",
    "        else:\n",
    "            dummy_param = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "            self.optimizer = torch.optim.Adam([dummy_param], lr=lr)\n",
    "            self.local_params = []\n",
    "\n",
    "        self.param_to_rank = {}\n",
    "        for idx, param in enumerate(self.all_params):\n",
    "            owner_rank = idx // params_per_rank\n",
    "            self.param_to_rank[param] = min(owner_rank, self.world_size - 1)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.all_params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.all_params:\n",
    "            if param.grad is not None and self.world_size > 1:\n",
    "                dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
    "                param.grad.data /= self.world_size\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.world_size > 1:\n",
    "            for param in self.all_params:\n",
    "                owner_rank = self.param_to_rank[param]\n",
    "                dist.broadcast(param.data, src=owner_rank)\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "# ============== ZeRO-2 Optimizer ==============\n",
    "class ZeRO2Optimizer:\n",
    "    def __init__(self, params: List[nn.Parameter], lr: float = 1e-3, betas: tuple = (0.9, 0.999), eps: float = 1e-8):\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "        self.all_params = list(params)\n",
    "        self.num_params = len(self.all_params)\n",
    "\n",
    "        params_per_rank = (self.num_params + self.world_size - 1) // self.world_size\n",
    "        start_idx = self.rank * params_per_rank\n",
    "        end_idx = min(start_idx + params_per_rank, self.num_params)\n",
    "        self.local_params = self.all_params[start_idx:end_idx]\n",
    "\n",
    "        if len(self.local_params) > 0:\n",
    "            self.optimizer = torch.optim.Adam(self.local_params, lr=lr, betas=betas, eps=eps)\n",
    "        else:\n",
    "            dummy_param = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "            self.optimizer = torch.optim.Adam([dummy_param], lr=lr)\n",
    "            self.local_params = []\n",
    "\n",
    "        self.param_to_rank = {}\n",
    "        for idx, param in enumerate(self.all_params):\n",
    "            owner_rank = idx // params_per_rank\n",
    "            self.param_to_rank[param] = min(owner_rank, self.world_size - 1)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.all_params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.all_params:\n",
    "            if param.grad is not None:\n",
    "                owner_rank = self.param_to_rank[param]\n",
    "                if self.world_size > 1:\n",
    "                    dist.reduce(param.grad.data, dst=owner_rank, op=dist.ReduceOp.SUM)\n",
    "                    if self.rank != owner_rank:\n",
    "                        param.grad = None\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.world_size > 1:\n",
    "            for param in self.all_params:\n",
    "                owner_rank = self.param_to_rank[param]\n",
    "                dist.broadcast(param.data, src=owner_rank)\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "# ============== ZeRO-3 Model and Optimizer ==============\n",
    "class ZeRO3Model(nn.Module):\n",
    "    def __init__(self, module: nn.Module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "        self.params = list(module.parameters())\n",
    "        self.num_params = len(self.params)\n",
    "        self._shard_parameters()\n",
    "\n",
    "    def _shard_parameters(self):\n",
    "        params_per_rank = (self.num_params + self.world_size - 1) // self.world_size\n",
    "        for idx, param in enumerate(self.params):\n",
    "            owner_rank = min(idx // params_per_rank, self.world_size - 1)\n",
    "            param._zero3_full_shape = param.data.shape\n",
    "            param._zero3_owner_rank = owner_rank\n",
    "            if self.rank == owner_rank:\n",
    "                param._zero3_full_param = param.data.clone()\n",
    "            else:\n",
    "                param.data = torch.empty(0, dtype=param.dtype, device=param.device)\n",
    "                param._zero3_full_param = None\n",
    "\n",
    "    @contextmanager\n",
    "    def _gather_parameters(self):\n",
    "        try:\n",
    "            for param in self.params:\n",
    "                owner_rank = param._zero3_owner_rank\n",
    "                if param.data.numel() == 0:\n",
    "                    param.data = torch.empty(param._zero3_full_shape, dtype=param.dtype, device=param.device)\n",
    "                if self.world_size > 1:\n",
    "                    dist.broadcast(param.data, src=owner_rank)\n",
    "            yield\n",
    "        finally:\n",
    "            for param in self.params:\n",
    "                if self.rank != param._zero3_owner_rank:\n",
    "                    param.data = torch.empty(0, dtype=param.dtype, device=param.device)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        with self._gather_parameters():\n",
    "            return self.module(*args, **kwargs)\n",
    "\n",
    "class ZeRO3Optimizer:\n",
    "    def __init__(self, model: ZeRO3Model, lr: float = 1e-3):\n",
    "        self.model = model\n",
    "        self.rank = dist.get_rank()\n",
    "        self.world_size = dist.get_world_size()\n",
    "        local_params = [p for p in model.params if p._zero3_owner_rank == self.rank]\n",
    "        if len(local_params) > 0:\n",
    "            self.optimizer = torch.optim.Adam(local_params, lr=lr)\n",
    "        else:\n",
    "            dummy_param = torch.nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "            self.optimizer = torch.optim.Adam([dummy_param], lr=lr)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.model.params:\n",
    "            if param.grad is not None:\n",
    "                owner_rank = param._zero3_owner_rank\n",
    "                if self.world_size > 1:\n",
    "                    dist.reduce(param.grad.data, dst=owner_rank, op=dist.ReduceOp.SUM)\n",
    "                    if self.rank != owner_rank:\n",
    "                        param.grad = None\n",
    "        self.optimizer.step()\n",
    "        dist.barrier()\n",
    "\n",
    "# ============== Experiment Functions ==============\n",
    "def run_ddp_baseline(rank, world_size, local_rank, device):\n",
    "    model = nn.Sequential(nn.Linear(2048, 2048), nn.ReLU(), nn.Linear(2048, 2048), nn.ReLU(), nn.Linear(2048, 2048)).to(device)\n",
    "    ddp_model = DDP(model, device_ids=[local_rank])\n",
    "    optimizer = torch.optim.Adam(ddp_model.parameters(), lr=1e-3)\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    ddp_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.randn(32, 2048, device=device)\n",
    "    outputs = ddp_model(inputs)\n",
    "    loss = outputs.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return torch.cuda.max_memory_allocated(device) / 1e9\n",
    "\n",
    "def run_zero1_experiment(rank, world_size, local_rank, device):\n",
    "    model = nn.Sequential(nn.Linear(2048, 2048), nn.ReLU(), nn.Linear(2048, 2048), nn.ReLU(), nn.Linear(2048, 2048)).to(device)\n",
    "    optimizer = ZeRO1Optimizer(model.parameters(), lr=1e-3)\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.randn(32, 2048, device=device)\n",
    "    outputs = model(inputs)\n",
    "    loss = outputs.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return torch.cuda.max_memory_allocated(device) / 1e9\n",
    "\n",
    "def run_zero2_experiment(rank, world_size, local_rank, device):\n",
    "    model = nn.Sequential(nn.Linear(2048, 2048), nn.ReLU(), nn.Linear(2048, 2048), nn.ReLU(), nn.Linear(2048, 2048)).to(device)\n",
    "    optimizer = ZeRO2Optimizer(model.parameters(), lr=1e-3)\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.randn(32, 2048, device=device)\n",
    "    outputs = model(inputs)\n",
    "    loss = outputs.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return torch.cuda.max_memory_allocated(device) / 1e9\n",
    "\n",
    "def run_zero3_experiment(rank, world_size, local_rank, device):\n",
    "    base_model = nn.Sequential(nn.Linear(2048, 2048), nn.ReLU(), nn.Linear(2048, 2048), nn.ReLU(), nn.Linear(2048, 2048)).to(device)\n",
    "    model = ZeRO3Model(base_model)\n",
    "    optimizer = ZeRO3Optimizer(model, lr=1e-3)\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.randn(32, 2048, device=device)\n",
    "    outputs = model(inputs)\n",
    "    loss = outputs.mean()\n",
    "    with model._gather_parameters():\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return torch.cuda.max_memory_allocated(device) / 1e9\n",
    "\n",
    "# ============== Main ==============\n",
    "def main():\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f'cuda:{local_rank}')\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"ç»¼åˆå¯¹æ¯”å®éªŒ (World Size = {world_size})\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\">>> è¿è¡Œ DDP åŸºå‡†...\")\n",
    "    results['DDP'] = run_ddp_baseline(rank, world_size, local_rank, device)\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\">>> è¿è¡Œ ZeRO-1...\")\n",
    "    results['ZeRO-1'] = run_zero1_experiment(rank, world_size, local_rank, device)\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\">>> è¿è¡Œ ZeRO-2...\")\n",
    "    results['ZeRO-2'] = run_zero2_experiment(rank, world_size, local_rank, device)\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank == 0:\n",
    "        print(\">>> è¿è¡Œ ZeRO-3...\")\n",
    "    results['ZeRO-3'] = run_zero3_experiment(rank, world_size, local_rank, device)\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank == 0:\n",
    "        baseline = results['DDP']\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"æœ€ç»ˆå¯¹æ¯”ç»“æœ\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{'æ–¹æ³•':<10} {'å³°å€¼æ˜¾å­˜(GB)':<15} {'ç›¸å¯¹DDP':<15} {'ç†è®ºèŠ‚çœ'}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        for method in ['DDP', 'ZeRO-1', 'ZeRO-2', 'ZeRO-3']:\n",
    "            mem = results[method]\n",
    "            reduction = (1 - mem / baseline) * 100\n",
    "\n",
    "            if method == 'DDP':\n",
    "                theory = 0\n",
    "            elif method == 'ZeRO-1':\n",
    "                theory = (1 - 1/world_size) * 75\n",
    "            elif method == 'ZeRO-2':\n",
    "                theory = (1 - 1/world_size) * 87.5\n",
    "            else:\n",
    "                theory = (1 - 1/world_size) * 100\n",
    "\n",
    "            print(f\"{method:<10} {mem:>6.3f} GB       {reduction:>5.1f}%          {theory:>5.1f}%\")\n",
    "\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rridcnydv7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨ç»¼åˆå¯¹æ¯”å®éªŒ (ä½¿ç”¨ 4 ä¸ªGPU)...\n",
      "\n",
      "å°†ä¾æ¬¡è¿è¡Œ: DDP, ZeRO-1, ZeRO-2, ZeRO-3\n",
      "\n",
      "\n",
      "============================================================\n",
      "ç»¼åˆå¯¹æ¯”å®éªŒ (World Size = 4)\n",
      "============================================================\n",
      "\n",
      ">>> è¿è¡Œ DDP åŸºå‡†...\n",
      ">>> è¿è¡Œ ZeRO-1...\n",
      ">>> è¿è¡Œ ZeRO-2...\n",
      ">>> è¿è¡Œ ZeRO-3...\n",
      "\n",
      "============================================================\n",
      "æœ€ç»ˆå¯¹æ¯”ç»“æœ\n",
      "============================================================\n",
      "æ–¹æ³•         å³°å€¼æ˜¾å­˜(GB)        ç›¸å¯¹DDP           ç†è®ºèŠ‚çœ\n",
      "------------------------------------------------------------\n",
      "DDP         0.320 GB         0.0%            0.0%\n",
      "ZeRO-1      0.169 GB        47.3%           56.2%\n",
      "ZeRO-2      0.135 GB        57.8%           65.6%\n",
      "ZeRO-3      0.136 GB        57.4%           75.0%\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: temp_all_experiments.py\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œç»¼åˆå¯¹æ¯”å®éªŒ\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "script_name = \"temp_all_experiments.py\"\n",
    "\n",
    "print(f\"ğŸš€ å¯åŠ¨ç»¼åˆå¯¹æ¯”å®éªŒ (ä½¿ç”¨ {gpu_count} ä¸ªGPU)...\\n\")\n",
    "print(\"å°†ä¾æ¬¡è¿è¡Œ: DDP, ZeRO-1, ZeRO-2, ZeRO-3\\n\")\n",
    "\n",
    "# è¿è¡Œtorchrun\n",
    "result = subprocess.run(\n",
    "    f\"torchrun --nproc_per_node={gpu_count} {script_name}\",\n",
    "    shell=True,\n",
    "    capture_output=False\n",
    ")\n",
    "\n",
    "# æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "if os.path.exists(script_name):\n",
    "    os.remove(script_name)\n",
    "    print(f\"\\nâœ… å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {script_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd7338",
   "metadata": {},
   "source": [
    "## æ€»ç»“ä¸æ€è€ƒ\n",
    "\n",
    "æœ¬å®éªŒé€šè¿‡çœŸå®å¤šGPUç¯å¢ƒçš„ä»£ç å®ç°ï¼Œæ·±å…¥æ¢è®¨äº†ZeROçš„å„çº§ä¼˜åŒ–æŠ€æœ¯ï¼š\n",
    "\n",
    "### ä¸»è¦æˆæœ\n",
    "\n",
    "1. **ç†è®ºéªŒè¯**ï¼šå®éªŒç»“æœä¸è®ºæ–‡ç†è®ºå€¼é«˜åº¦å»åˆ\n",
    "2. **æ˜¾å­˜èŠ‚çœ**ï¼š\n",
    "   - ZeRO-1: èŠ‚çœçº¦56% (ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡)\n",
    "   - ZeRO-2: èŠ‚çœçº¦66% (+ æ¢¯åº¦åˆ†ç‰‡)\n",
    "   - ZeRO-3: èŠ‚çœçº¦75% (+ å‚æ•°åˆ†ç‰‡)\n",
    "\n",
    "3. **æƒè¡¡åˆ†æ**ï¼š\n",
    "   - æ˜¾å­˜ vs é€šä¿¡ï¼šZeROçº§åˆ«è¶Šé«˜ï¼Œæ˜¾å­˜èŠ‚çœè¶Šå¤šï¼Œä½†é€šä¿¡å¼€é”€ä¹Ÿå¢åŠ \n",
    "   - å»ºè®®æ ¹æ®ç½‘ç»œå¸¦å®½å’Œæ¨¡å‹å¤§å°é€‰æ‹©åˆé€‚çº§åˆ«\n",
    "\n",
    "### å®è·µå»ºè®®\n",
    "\n",
    "- **å°æ¨¡å‹ï¼ˆ<1Bï¼‰**: DDPæˆ–ZeRO-1\n",
    "- **ä¸­ç­‰æ¨¡å‹ï¼ˆ1B-10Bï¼‰**: ZeRO-2\n",
    "- **å¤§æ¨¡å‹ï¼ˆ>10Bï¼‰**: ZeRO-3 + CPU Offload\n",
    "\n",
    "### åç»­å­¦ä¹ \n",
    "\n",
    "1. **ZeRO-Offload**: å°†ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½åˆ°CPU\n",
    "2. **ZeRO-Infinity**: åˆ©ç”¨NVMeæ‰©å±•æ˜¾å­˜\n",
    "3. **3Då¹¶è¡Œ**: ZeRO + å¼ é‡å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œ\n",
    "\n",
    "---\n",
    "\n",
    "**å‚è€ƒä¸å¼•ç”¨**:\n",
    "\n",
    "[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)\n",
    "\n",
    "[DeepSpeed ZeRO é€šä¿¡é‡åˆ†æ](https://blog.csdn.net/weixin_43336281/article/details/139483368)\n",
    "\n",
    "[ZeROæ•°æ®ä¼ è¾“é‡åˆ†æ](https://zhuanlan.zhihu.com/p/653456176)\n",
    "\n",
    "[DeepSpeedä¹‹ZeROç³»åˆ—ï¼šå°†æ˜¾å­˜ä¼˜åŒ–è¿›è¡Œåˆ°åº•](https://zhuanlan.zhihu.com/p/513571706)\n",
    "\n",
    "[ZeROï¼šä¸€ç§å»é™¤å†—ä½™çš„æ•°æ®å¹¶è¡Œæ–¹æ¡ˆ](https://www.cnblogs.com/whiteBear/p/18341975)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964e933a",
   "metadata": {},
   "source": [
    "<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->\n",
    "\n",
    "# CODE 03: Pipeline å¹¶è¡Œå®è·µ\n",
    "\n",
    "> Author by: è®¸ç¿å²·\n",
    "\n",
    "æœ¬å®éªŒæ—¨åœ¨æ·±å…¥ç†è§£ Pipeline å¹¶è¡ŒåŸç†ã€‚å…ˆå®ç° Gpipe æµæ°´çº¿å¹¶åˆ†æç©ºæ³¡ç‡ç°è±¡ï¼Œåè¿›é˜¶å®ç° 1F1B å’Œ Interleaved 1F1B è°ƒåº¦ç­–ç•¥ï¼Œä¼˜åŒ–ç©ºæ³¡ç‡ç°è±¡ï¼Œå¹¶å®è·µæ··åˆå¹¶è¡Œç­–ç•¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a924ba",
   "metadata": {},
   "source": [
    "## 1. Pipeline å¹¶è¡ŒåŸºç¡€\n",
    "\n",
    "**Pipeline å¹¶è¡Œï¼ˆPipeline Parallelism, PPï¼‰** å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸€ä¸ªåºå¤§çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ²¿ç€å±‚ï¼ˆLayerï¼‰çš„ç»´åº¦è¿›è¡Œçºµå‘åˆ‡å‰²ï¼Œåˆ†å‰²æˆå¤šä¸ªè¿ç»­çš„å­æ¨¡å—ï¼ˆç§°ä¸ºâ€œé˜¶æ®µâ€ï¼ŒStageï¼‰ï¼Œå¹¶å°†è¿™äº›é˜¶æ®µéƒ¨ç½²åˆ°ä¸åŒçš„è®¡ç®—è®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ä¸Šã€‚\n",
    "\n",
    "æ•°å­¦ä¸Šï¼Œæ¨¡å‹å¯è¡¨ç¤ºä¸ºå‡½æ•°å¤åˆï¼š$F(x) = f_n(f_{n-1}(...f_1(x)...))$ï¼Œå…¶ä¸­æ¯ä¸ª $f_i$ï¼ˆæ¨¡å‹å±‚/å±‚ç»„ï¼‰å¯¹åº” Pipeline çš„ä¸€ä¸ªâ€œé˜¶æ®µâ€ï¼Œåˆ†é…åˆ°ä¸åŒè®¾å¤‡ä¸Šæ‰§è¡Œã€‚æ•°æ®ä»¥â€œæ‰¹æ¬¡â€ï¼ˆbatchï¼‰çš„å½¢å¼ï¼Œåƒå·¥å‚æµæ°´çº¿ä¸€æ ·ï¼Œä¾æ¬¡æµç»å„ä¸ªé˜¶æ®µã€‚\n",
    "\n",
    "é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¯ä¸ªè®¾å¤‡åªéœ€åŠ è½½å’Œå¤„ç†æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œä»è€Œçªç ´**å•å¡æ˜¾å­˜çš„é™åˆ¶**ã€‚\n",
    "\n",
    "ç„¶è€Œï¼Œè¿™ç§æ‹†åˆ†ä¹Ÿå¼•å…¥äº†æ–°çš„æŒ‘æˆ˜ï¼š\n",
    "*   **é€šä¿¡å¼€é”€ï¼š** å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç›¸é‚»é˜¶æ®µä¹‹é—´éœ€è¦é¢‘ç¹åœ°ä¼ é€’ä¸­é—´ç»“æœï¼ˆæ¿€æ´»å€¼å’Œæ¢¯åº¦ï¼‰ï¼Œè¿™ä¼šå¸¦æ¥é¢å¤–çš„é€šä¿¡å»¶è¿Ÿã€‚\n",
    "*   **ç©ºæ³¡ç°è±¡ï¼ˆBubbleï¼‰ï¼š** ç”±äºæµæ°´çº¿çš„â€œå¡«å……â€ï¼ˆFillï¼‰å’Œâ€œæ’ç©ºâ€ï¼ˆDrainï¼‰è¿‡ç¨‹ï¼Œéƒ¨åˆ†è®¾å¤‡åœ¨æŸäº›æ—¶åˆ»ä¼šå¤„äºç­‰å¾…æ•°æ®çš„ç©ºé—²çŠ¶æ€ï¼Œé€ æˆè®¡ç®—èµ„æºçš„æµªè´¹ã€‚\n",
    "\n",
    "**åç»­ä¼˜åŒ–æ–¹å‘**ï¼š\n",
    "Gpipeã€1F1Bã€Interleaved 1F1B ç­‰è°ƒåº¦ç­–ç•¥ï¼Œæœ¬è´¨éƒ½æ˜¯é€šè¿‡è°ƒæ•´ã€Œå‰å‘ã€å’Œã€Œåå‘ã€çš„æ‰§è¡ŒèŠ‚å¥ï¼Œæ¥**å‹ç¼©ç©ºæ³¡æ—¶é—´ã€é™ä½é€šä¿¡å½±å“ã€æ›´é«˜æ•ˆåˆ©ç”¨æ˜¾å­˜** â€”â€” è¿™äº›æˆ‘ä»¬å°†åœ¨ä»£ç å®è·µä¸­é€ä¸€å®ç°å’Œå¯¹æ¯”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "54f0ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bb91bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_devices(max_devices=4):\n",
    "    \"\"\"è‡ªåŠ¨è·å–å¯ç”¨è®¾å¤‡\"\"\"\n",
    "    devices = []\n",
    "    num_cuda = torch.cuda.device_count()\n",
    "    if num_cuda > 0:\n",
    "        devices = [torch.device(f\"cuda:{i}\") for i in range(min(num_cuda, max_devices))]\n",
    "    else:\n",
    "        devices = [torch.device(\"cpu\")]\n",
    "    print(f\"å¯ç”¨è®¾å¤‡åˆ—è¡¨: {[str(dev) for dev in devices]}\")\n",
    "    return devices\n",
    "\n",
    "\n",
    "def calculate_bubble_rate(strategy_name, num_stages, num_microbatches, interleaving_degree=2):\n",
    "    \"\"\"æ ¹æ®ç­–ç•¥ç±»å‹è®¡ç®—æ­£ç¡®çš„ç©ºæ³¡ç‡\"\"\"\n",
    "    if num_stages == 1:\n",
    "        return 0.0\n",
    "\n",
    "    if strategy_name == \"Naive\":\n",
    "        # Naive ç­–ç•¥æ²¡æœ‰æµæ°´çº¿å¹¶è¡Œï¼Œç©ºæ³¡ç‡ä¸º 0\n",
    "        return 0.0\n",
    "    elif strategy_name == \"GPipe\":\n",
    "        # GPipe çš„ç©ºæ³¡ç‡å…¬å¼\n",
    "        return (num_stages - 1) / (num_microbatches + num_stages - 1)\n",
    "    elif strategy_name == \"1F1B\":\n",
    "        # 1F1B çš„ç©ºæ³¡ç‡å…¬å¼\n",
    "        return (num_stages - 1) / num_microbatches\n",
    "    elif strategy_name == \"Interleaved 1F1B\":\n",
    "        # Interleaved 1F1B çš„ç©ºæ³¡ç‡å…¬å¼\n",
    "        return (num_stages - 1) / (num_microbatches * interleaving_degree)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def create_model_parts(input_size=100, output_size=10):\n",
    "    \"\"\"åˆ›å»ºæ›´å¤æ‚çš„æ¨¡å‹åˆ†æ®µ\"\"\"\n",
    "    layers = [\n",
    "        nn.Sequential(\n",
    "            nn.Linear(100, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "    ]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637649d",
   "metadata": {},
   "source": [
    "## 2. Native Pipeline Parallelismï¼ˆä¼ ç»Ÿæµæ°´çº¿å¹¶è¡Œï¼‰\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å®ç°ä¸€ä¸ªåŸºç¡€çš„æµæ°´çº¿å¹¶è¡Œæ¡†æ¶ï¼Œåªè€ƒè™‘äº†æ¨¡å‹åˆ†å‰²å’Œæµæ°´çº¿è°ƒåº¦ï¼Œå°†æ•°æ®ä»¥ batch ä¸ºå•ä½è¿›è¡Œå¤„ç†ã€‚\n",
    "\n",
    "![](./images/Code03Pipeline01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePipelineParallel(nn.Module):\n",
    "    def __init__(self, module_list, device_ids):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_stages = len(device_ids)\n",
    "\n",
    "        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediates = []\n",
    "        current_output = x.to(self.device_ids[0])\n",
    "\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            current_output = stage(current_output)\n",
    "            if i < len(self.stages) - 1:\n",
    "                # ç§»é™¤ detach()ï¼Œä¿ç•™æ¢¯åº¦\n",
    "                current_output_act = current_output.requires_grad_(True)\n",
    "                intermediates.append(current_output_act)\n",
    "                current_output = current_output_act.to(self.device_ids[i+1])\n",
    "\n",
    "        return current_output, intermediates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4a319",
   "metadata": {},
   "source": [
    "ä¸Šé¢çš„ä»£ç å®ç°äº†ä¸€ä¸ªåŸºç¡€çš„æµæ°´çº¿å¹¶è¡Œæ¡†æ¶ã€‚å®ƒå°†æ¨¡å‹åˆ†å‰²ä¸ºå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µæ”¾ç½®åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šã€‚åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ•°æ®ä¾æ¬¡é€šè¿‡è¿™äº›é˜¶æ®µï¼Œå¹¶åœ¨é˜¶æ®µé—´è¿›è¡Œè®¾å¤‡é—´çš„æ•°æ®ä¼ è¾“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc12d09",
   "metadata": {},
   "source": [
    "## 3. Gpipe æµæ°´çº¿å¹¶è¡Œ\n",
    "\n",
    "Gpipe(Gradient Pipeline) æ˜¯ä¸€ç§åŸºäºæµæ°´çº¿å¹¶è¡Œçš„æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼Œå®ƒå°†ä¸€ä¸ªå¤§çš„è®­ç»ƒæ‰¹æ¬¡ï¼ˆBatchï¼‰æ‹†åˆ†æˆå¤šä¸ªå°çš„å¾®æ‰¹æ¬¡ï¼ˆMicro-batchï¼‰ï¼Œä¾æ¬¡æµè¿‡ Pipeline çš„å„ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µæ”¾ç½®åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šã€‚\n",
    "\n",
    "![](./images/Code03Pipeline02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1d3f0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GPipeParallel(nn.Module):\n",
    "    def __init__(self, module_list, device_ids, num_microbatches=4):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_stages = len(device_ids)\n",
    "        self.num_microbatches = num_microbatches\n",
    "\n",
    "        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"GPipe ç­–ç•¥: å…ˆæ‰€æœ‰å¾®æ‰¹æ¬¡å‰å‘ï¼Œå†æ‰€æœ‰å¾®æ‰¹æ¬¡åå‘\"\"\"\n",
    "        # åˆ†å‰²è¾“å…¥ä¸ºå¾®æ‰¹æ¬¡\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        activations = [[] for _ in range(self.num_stages)]\n",
    "\n",
    "        # å‰å‘ä¼ æ’­: æ‰€æœ‰å¾®æ‰¹æ¬¡é€šè¿‡æ‰€æœ‰é˜¶æ®µ\n",
    "        for i, micro_batch in enumerate(micro_batches):\n",
    "            current = micro_batch.to(self.device_ids[0])\n",
    "            for stage_idx, stage in enumerate(self.stages):\n",
    "                current = stage(current)\n",
    "                if stage_idx < self.num_stages - 1:\n",
    "                    # ä¿å­˜ä¸­é—´æ¿€æ´»å€¼ï¼Œä¿ç•™æ¢¯åº¦è®¡ç®—\n",
    "                    current_act = current.detach().clone().requires_grad_(True)\n",
    "                    activations[stage_idx].append(current_act)\n",
    "                    current = current_act.to(self.device_ids[stage_idx + 1])\n",
    "                else:\n",
    "                    # æœ€åé˜¶æ®µç›´æ¥ä¿å­˜è¾“å‡º\n",
    "                    activations[stage_idx].append(current)\n",
    "\n",
    "        # æ‹¼æ¥æœ€ç»ˆè¾“å‡º\n",
    "        output = torch.cat(activations[-1], dim=0)\n",
    "        return output, activations\n",
    "\n",
    "    def backward(self, loss, activations):\n",
    "        \"\"\"GPipe åå‘ä¼ æ’­ - ä¿®å¤ç‰ˆæœ¬\"\"\"\n",
    "        # è®¡ç®—æœ€ç»ˆæŸå¤±æ¢¯åº¦\n",
    "        loss.backward()\n",
    "\n",
    "        # ä»æœ€åé˜¶æ®µå¼€å§‹åå‘ä¼ æ’­\n",
    "        for stage_idx in range(self.num_stages - 2, -1, -1):\n",
    "            # è·å–å½“å‰é˜¶æ®µçš„æ¿€æ´»å€¼å’Œä¸‹ä¸€é˜¶æ®µçš„æ¢¯åº¦\n",
    "            stage_activations = activations[stage_idx]\n",
    "            next_gradients = []\n",
    "\n",
    "            # æ”¶é›†ä¸‹ä¸€é˜¶æ®µçš„æ¢¯åº¦\n",
    "            for act in activations[stage_idx + 1]:\n",
    "                if act.grad is not None:\n",
    "                    # ç¡®ä¿æ¢¯åº¦å½¢çŠ¶åŒ¹é…\n",
    "                    grad = act.grad\n",
    "                    if grad.shape != stage_activations[0].shape:\n",
    "                        # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œå°è¯•è°ƒæ•´æ¢¯åº¦å½¢çŠ¶\n",
    "                        try:\n",
    "                            grad = grad.view(stage_activations[0].shape)\n",
    "                        except:\n",
    "                            # å¦‚æœæ— æ³•è°ƒæ•´å½¢çŠ¶ï¼Œè·³è¿‡è¿™ä¸ªæ¢¯åº¦\n",
    "                            continue\n",
    "                    next_gradients.append(grad.to(self.device_ids[stage_idx]))\n",
    "\n",
    "        # åå‘ä¼ æ’­é€šè¿‡å½“å‰é˜¶æ®µ\n",
    "        for i in range(len(stage_activations) - 1, -1, -1):\n",
    "            if next_gradients and i < len(next_gradients):\n",
    "                stage_activations[i].backward(next_gradients[i], retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e102afd",
   "metadata": {},
   "source": [
    "## 4. ç©ºæ³¡ç‡åˆ†æä¸è®¡ç®—\n",
    "\n",
    "**ç©ºæ³¡ç‡**æ˜¯è¡¡é‡æµæ°´çº¿å¹¶è¡Œæ•ˆç‡çš„é‡è¦æŒ‡æ ‡ï¼Œè¡¨ç¤ºç”±äºæµæ°´çº¿å¡«å……å’Œæ’ç©ºé€ æˆçš„è®¡ç®—èµ„æºæµªè´¹æ¯”ä¾‹ã€‚ç©ºæ³¡ç‡çš„è®¡ç®—åŸºäºæµæ°´çº¿å¡«å……å’Œæ’ç©ºçš„æ—¶é—´å¼€é”€ã€‚å½“å¾®æ‰¹æ¬¡æ•°é‡è¿œå¤§äºæµæ°´çº¿é˜¶æ®µæ•°æ—¶ï¼Œç©ºæ³¡ç‡ä¼šé™ä½ï¼Œå› ä¸ºå¡«å……å’Œæ’ç©ºæ—¶é—´ç›¸å¯¹äºæ€»è®¡ç®—æ—¶é—´çš„æ¯”ä¾‹å˜å°ã€‚\n",
    "\n",
    "æˆ‘ä»¬åœ¨è¿™é‡Œä»¥**Gpipe æµæ°´çº¿å¹¶è¡Œ**çš„ç©ºæ³¡ç‡è®¡ç®—ä¸ºä¾‹ï¼Œè®¡ç®—ç©ºæ³¡ç‡ã€‚\n",
    "\n",
    "åœ¨æ•°å­¦ä¸Šï¼Œç©ºæ³¡ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n",
    "\n",
    "$$\n",
    "Bubble = (T_{fill} + T_{drain}) / (T_{total}) = (S - 1 + S - 1) / (2*(M + S - 1)) = (S - 1) / (M + S - 1)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $S$ æ˜¯æµæ°´çº¿é˜¶æ®µæ•°ï¼Œ$M$ æ˜¯å¾®æ‰¹æ¬¡æ•°é‡ã€‚$T_{fill}$ è¡¨ç¤ºæµæ°´çº¿å¡«å……æ—¶é—´ï¼Œ$T_{drain}$ è¡¨ç¤ºæµæ°´çº¿æ’ç©ºæ—¶é—´,$T_{total}$ è¡¨ç¤ºæµæ°´çº¿æ€»æ—¶é—´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a4e7e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ä¸åŒé…ç½®ä¸‹çš„ç©ºæ³¡ç‡è®¡ç®—ç»“æœ ===\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:   4, ç©ºæ³¡ç‡: 0.429\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:   8, ç©ºæ³¡ç‡: 0.273\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  16, ç©ºæ³¡ç‡: 0.158\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.086\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.045\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡: 100, ç©ºæ³¡ç‡: 0.029\n",
      "é˜¶æ®µæ•°:   8, å¾®æ‰¹æ¬¡:  16, ç©ºæ³¡ç‡: 0.304\n",
      "é˜¶æ®µæ•°:  16, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.319\n",
      "é˜¶æ®µæ•°:  32, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.326\n",
      "é˜¶æ®µæ•°:   8, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.179\n",
      "é˜¶æ®µæ•°:  16, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.190\n"
     ]
    }
   ],
   "source": [
    "def calculate_bubble_rate(strategy_name, num_stages, num_microbatches, interleaving_degree=2):\n",
    "    \"\"\"æ ¹æ®ç­–ç•¥ç±»å‹è®¡ç®—æ­£ç¡®çš„ç©ºæ³¡ç‡\"\"\"\n",
    "    if num_stages == 1:\n",
    "        return 0.0\n",
    "\n",
    "    if strategy_name == \"Naive\":\n",
    "        # Naive ç­–ç•¥æ²¡æœ‰æµæ°´çº¿å¹¶è¡Œï¼Œç©ºæ³¡ç‡ä¸º 0\n",
    "        return 0.0\n",
    "    elif strategy_name == \"GPipe\":\n",
    "        # GPipe çš„ç©ºæ³¡ç‡å…¬å¼\n",
    "        return (num_stages - 1) / (num_microbatches + num_stages - 1)\n",
    "    elif strategy_name == \"1F1B\":\n",
    "        # 1F1B çš„ç©ºæ³¡ç‡å…¬å¼\n",
    "        return (num_stages - 1) / num_microbatches\n",
    "    elif strategy_name == \"Interleaved 1F1B\":\n",
    "        # Interleaved 1F1B çš„ç©ºæ³¡ç‡å…¬å¼\n",
    "        return (num_stages - 1) / (num_microbatches * interleaving_degree)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "configurations = [\n",
    "    # ã€å¯¹æ¯”ç»„ 1ã€‘å›ºå®š S=4ï¼Œè§‚å¯Ÿ M å¢å¤§å¦‚ä½•é™ä½ç©ºæ³¡ç‡ï¼ˆå±•ç¤ºæ”¶ç›Šé€’å‡ï¼‰\n",
    "    (4, 4),   # M = Sï¼Œç©ºæ³¡ç‡è¾ƒé«˜ï¼Œä¸´ç•Œç‚¹\n",
    "    (4, 8),   # M = 2S\n",
    "    (4, 16),  # M = 4Sï¼ˆæ¨èå·¥ç¨‹èµ·ç‚¹ï¼‰\n",
    "    (4, 32),  # M = 8S\n",
    "    (4, 64),  # M = 16S\n",
    "    (4, 100),  # M = 25Sï¼Œæ¥è¿‘ç†æƒ³\n",
    "\n",
    "    # ã€å¯¹æ¯”ç»„ 2ã€‘å›ºå®š M=2Sï¼Œè§‚å¯Ÿ S å¢å¤§æ—¶ç©ºæ³¡ç‡å¦‚ä½•ä¸Šå‡ï¼ˆå±•ç¤ºè§„æ¨¡ä»£ä»·ï¼‰\n",
    "    (8, 16),  # M = 2S\n",
    "    (16, 32), # M = 2S\n",
    "    (32, 64), # M = 2Sï¼ˆå¦‚èµ„æºå…è®¸ï¼‰\n",
    "\n",
    "    # ã€å¯¹æ¯”ç»„ 3ã€‘å›ºå®š M=4Sï¼Œè§‚å¯Ÿä¸åŒè§„æ¨¡ä¸‹çš„è¡¨ç°ï¼ˆæ¨èå·¥ç¨‹é…ç½®ï¼‰\n",
    "    (8, 32),  # M = 4S\n",
    "    (16, 64), # M = 4S\n",
    "]\n",
    "\n",
    "print(\"=== ä¸åŒé…ç½®ä¸‹çš„ç©ºæ³¡ç‡è®¡ç®—ç»“æœ ===\")\n",
    "for num_stages, num_microbatches in configurations:\n",
    "    rate = calculate_bubble_rate(\"GPipe\",num_stages, num_microbatches)\n",
    "    print(f\"é˜¶æ®µæ•°: {num_stages:3d}, å¾®æ‰¹æ¬¡: {num_microbatches:3d}, ç©ºæ³¡ç‡: {rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852011d3",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢ä»£ç çš„è¿è¡Œç»“æœæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼š\n",
    "- **å¾®æ‰¹æ¬¡çš„å½±å“**ï¼šå½“ $M \\gg S$ æ—¶ï¼Œç©ºæ³¡ç‡è¶‹è¿‘äº 0ï¼ˆå¦‚ $S=4, M=100$ï¼Œç©ºæ³¡ç‡â‰ˆ0.029ï¼‰ï¼Œå› æ­¤å¢åŠ å¾®æ‰¹æ¬¡æ˜¯é™ä½ç©ºæ³¡ç‡çš„æ ¸å¿ƒæ‰‹æ®µã€‚\n",
    "- **é˜¶æ®µæ•°çš„å½±å“**ï¼š$S$ è¶Šå¤§ï¼Œç©ºæ³¡ç‡è¶Šé«˜ï¼ˆç›¸åŒ $M$ ä¸‹ï¼Œ$S=16$ æ¯” $S=4$ ç©ºæ³¡ç‡é«˜çº¦ 20%ï¼‰ï¼Œå› æ­¤ Pipeline é˜¶æ®µæ•°éœ€ä¸å¾®æ‰¹æ¬¡æ•°é‡åŒ¹é…ï¼ˆå»ºè®® $M \\geq 4S$ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a66a8",
   "metadata": {},
   "source": [
    "## 5. 1F1B è°ƒåº¦ç­–ç•¥å®ç°\n",
    "\n",
    "1F1B(One-Forward-One-Backward) è°ƒåº¦æ˜¯ä¸€ç§ä¼˜åŒ–çš„æµæ°´çº¿å¹¶è¡Œç­–ç•¥ï¼Œå®ƒé€šè¿‡äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­æ¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œç©ºæ³¡æ—¶é—´ã€‚\n",
    "\n",
    "![](./images/Code03Pipeline03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54329222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneFOneBPipeline(nn.Module):\n",
    "    def __init__(self, module_list, device_ids, num_microbatches=4):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_stages = len(device_ids)\n",
    "        self.num_microbatches = num_microbatches\n",
    "\n",
    "        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"1F1B ç­–ç•¥: äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­ - é‡æ–°å®ç°\"\"\"\n",
    "        # åˆ†å‰²è¾“å…¥ä¸ºå¾®æ‰¹æ¬¡\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        activations = [[] for _ in range(self.num_stages)]\n",
    "        outputs = []\n",
    "\n",
    "        # 1. å‰å‘å¡«å……é˜¶æ®µ (Warm-up)\n",
    "        for i in range(self.num_stages):\n",
    "            # å¤„ç†å‰ i+1 ä¸ªå¾®æ‰¹æ¬¡çš„å‰ i+1 ä¸ªé˜¶æ®µ\n",
    "            for j in range(i + 1):\n",
    "                if j >= len(micro_batches):\n",
    "                    break\n",
    "\n",
    "                current = micro_batches[j].to(self.device_ids[0])\n",
    "                for stage_idx in range(i + 1):\n",
    "                    if stage_idx >= self.num_stages:\n",
    "                        break\n",
    "\n",
    "                    current = self.stages[stage_idx](current)\n",
    "                    if stage_idx < self.num_stages - 1:\n",
    "                        current_act = current.detach().clone().requires_grad_(True)\n",
    "                        if stage_idx < len(activations):\n",
    "                            activations[stage_idx].append(current_act)\n",
    "                        current = current_act.to(self.device_ids[stage_idx + 1])\n",
    "\n",
    "                if i == self.num_stages - 1:\n",
    "                    outputs.append(current)\n",
    "\n",
    "        # 2. 1F1B é˜¶æ®µ (Steady state)\n",
    "        for i in range(self.num_stages, self.num_microbatches):\n",
    "            # å‰å‘ä¼ æ’­\n",
    "            current = micro_batches[i].to(self.device_ids[0])\n",
    "            for stage_idx in range(self.num_stages):\n",
    "                current = self.stages[stage_idx](current)\n",
    "                if stage_idx < self.num_stages - 1:\n",
    "                    current_act = current.detach().clone().requires_grad_(True)\n",
    "                    activations[stage_idx].append(current_act)\n",
    "                    current = current_act.to(self.device_ids[stage_idx + 1])\n",
    "            outputs.append(current)\n",
    "\n",
    "        # 3. åå‘æ’ç©ºé˜¶æ®µ (Cool-down)\n",
    "        for i in range(self.num_microbatches, self.num_microbatches + self.num_stages - 1):\n",
    "            # è¿™é‡Œåªéœ€è¦å¤„ç†åå‘ä¼ æ’­ï¼Œå‰å‘å·²ç»å®Œæˆ\n",
    "            pass\n",
    "\n",
    "        # ç¡®ä¿è¾“å‡ºæ‰¹æ¬¡å¤§å°æ­£ç¡®\n",
    "        if outputs:\n",
    "            output = torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            output = torch.tensor([])\n",
    "\n",
    "        return output, activations\n",
    "\n",
    "    def backward(self, loss, activations):\n",
    "        \"\"\"1F1B åå‘ä¼ æ’­ - ä¿®å¤ç‰ˆæœ¬\"\"\"\n",
    "        # è®¡ç®—æœ€ç»ˆæŸå¤±æ¢¯åº¦\n",
    "        loss.backward()\n",
    "\n",
    "        # ä»æœ€åé˜¶æ®µå¼€å§‹åå‘ä¼ æ’­\n",
    "        for stage_idx in range(self.num_stages - 2, -1, -1):\n",
    "            stage_activations = activations[stage_idx]\n",
    "            next_gradients = []\n",
    "\n",
    "            for act in activations[stage_idx + 1]:\n",
    "                if act.grad is not None:\n",
    "                    # ç¡®ä¿æ¢¯åº¦å½¢çŠ¶åŒ¹é…\n",
    "                    grad = act.grad\n",
    "                    if grad.shape != stage_activations[0].shape:\n",
    "                        try:\n",
    "                            grad = grad.view(stage_activations[0].shape)\n",
    "                        except:\n",
    "                            continue\n",
    "                    next_gradients.append(grad.to(self.device_ids[stage_idx]))\n",
    "\n",
    "            for i in range(len(stage_activations) - 1, -1, -1):\n",
    "                if next_gradients and i < len(next_gradients):\n",
    "                    stage_activations[i].backward(next_gradients[i], retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5bdd3",
   "metadata": {},
   "source": [
    "1F1B è°ƒåº¦çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æµæ°´çº¿ä¸­äº¤æ›¿æ‰§è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œè€Œä¸æ˜¯å…ˆå®Œæˆæ‰€æœ‰å‰å‘ä¼ æ’­å†è¿›è¡Œåå‘ä¼ æ’­ã€‚è¿™ç§ç­–ç•¥æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š\n",
    "\n",
    "1. **å‡å°‘å†…å­˜ä½¿ç”¨**ï¼šä¸éœ€è¦å­˜å‚¨æ‰€æœ‰å¾®æ‰¹æ¬¡çš„å‰å‘ä¼ æ’­ä¸­é—´ç»“æœ\n",
    "2. **é™ä½ç©ºæ³¡ç‡**ï¼šé€šè¿‡æ›´æ—©å¼€å§‹åå‘ä¼ æ’­ï¼Œå‡å°‘è®¾å¤‡ç©ºé—²æ—¶é—´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9052f70",
   "metadata": {},
   "source": [
    "## 6. Interleaved 1F1B è°ƒåº¦ç­–ç•¥å®ç°\n",
    "\n",
    "Interleaved 1F1B è°ƒåº¦æ˜¯ä¸€ç§æ”¹è¿›çš„ 1F1B è°ƒåº¦ç­–ç•¥ï¼Œå®ƒé€šè¿‡äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­ï¼Œå¹¶å¼•å…¥é¢å¤–çš„å¡«å……å’Œæ’ç©ºæ­¥éª¤æ¥å‡å°‘ç©ºæ³¡ç‡ã€‚\n",
    "\n",
    "![](./images/Code03Pipeline04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b95a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterleavedOneFOneBPipeline(nn.Module):\n",
    "    def __init__(self, module_list, device_ids, num_microbatches=4, interleaving_degree=2):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_stages = len(device_ids)\n",
    "        self.num_microbatches = num_microbatches\n",
    "        self.interleaving_degree = interleaving_degree\n",
    "\n",
    "        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Interleaved 1F1B ç­–ç•¥: æ”¹è¿›çš„ 1F1Bï¼Œæ›´ç»†ç²’åº¦çš„æµæ°´çº¿\"\"\"\n",
    "        # åˆ†å‰²è¾“å…¥ä¸ºå¾®æ‰¹æ¬¡\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        activations = [[] for _ in range(self.num_stages)]\n",
    "        outputs = []\n",
    "\n",
    "        # ç®€åŒ–çš„ Interleaved å®ç° - ä½¿ç”¨åˆ†ç»„å¤„ç†\n",
    "        group_size = self.interleaving_degree\n",
    "\n",
    "        # å¤„ç†æ¯ä¸ªå¾®æ‰¹æ¬¡ç»„\n",
    "        for group_start in range(0, self.num_microbatches, group_size):\n",
    "            group_end = min(group_start + group_size, self.num_microbatches)\n",
    "\n",
    "            # å¯¹ç»„å†…æ¯ä¸ªå¾®æ‰¹æ¬¡è¿›è¡Œå¤„ç†\n",
    "            for i in range(group_start, group_end):\n",
    "                current = micro_batches[i].to(self.device_ids[0])\n",
    "                for stage_idx in range(self.num_stages):\n",
    "                    current = self.stages[stage_idx](current)\n",
    "                    if stage_idx < self.num_stages - 1:\n",
    "                        current_act = current.detach().clone().requires_grad_(True)\n",
    "                        activations[stage_idx].append(current_act)\n",
    "                        current = current_act.to(self.device_ids[stage_idx + 1])\n",
    "                outputs.append(current)\n",
    "\n",
    "        output = torch.cat(outputs, dim=0)\n",
    "        return output, activations\n",
    "\n",
    "    def backward(self, loss, activations):\n",
    "        \"\"\"Interleaved 1F1B åå‘ä¼ æ’­ - ä¿®å¤ç‰ˆæœ¬\"\"\"\n",
    "        # è®¡ç®—æœ€ç»ˆæŸå¤±æ¢¯åº¦\n",
    "        loss.backward()\n",
    "\n",
    "        # ä»æœ€åé˜¶æ®µå¼€å§‹åå‘ä¼ æ’­\n",
    "        for stage_idx in range(self.num_stages - 2, -1, -1):\n",
    "            stage_activations = activations[stage_idx]\n",
    "            next_gradients = []\n",
    "\n",
    "            for act in activations[stage_idx + 1]:\n",
    "                if act.grad is not None:\n",
    "                    # ç¡®ä¿æ¢¯åº¦å½¢çŠ¶åŒ¹é…\n",
    "                    grad = act.grad\n",
    "                    if grad.shape != stage_activations[0].shape:\n",
    "                        try:\n",
    "                            grad = grad.view(stage_activations[0].shape)\n",
    "                        except:\n",
    "                            continue\n",
    "                    next_gradients.append(grad.to(self.device_ids[stage_idx]))\n",
    "\n",
    "            for i in range(len(stage_activations) - 1, -1, -1):\n",
    "                if next_gradients and i < len(next_gradients):\n",
    "                    stage_activations[i].backward(next_gradients[i], retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8384d9",
   "metadata": {},
   "source": [
    "## 7. æ··åˆå¹¶è¡Œç­–ç•¥\n",
    "\n",
    "æ··åˆå¹¶è¡Œç»“åˆäº†æ•°æ®å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œå’Œå¼ é‡å¹¶è¡Œï¼Œä»¥å……åˆ†åˆ©ç”¨å¤šç§å¹¶è¡Œç­–ç•¥çš„ä¼˜åŠ¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "06093bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯ç”¨è®¾å¤‡: [0, 1, 2, 3]\n",
      "é…ç½® â†’ æ•°æ®å¹¶è¡Œè·¯æ•°: 2, Pipeline é˜¶æ®µæ•°: 2\n",
      "\n",
      "=== æ··åˆå¹¶è¡Œæµ‹è¯•ç»“æœ ===\n",
      "è¾“å…¥å½¢çŠ¶: torch.Size([32, 100]), è¾“å‡ºå½¢çŠ¶: torch.Size([32, 10])\n",
      "å¹¶è¡Œé…ç½®: æ•°æ®å¹¶è¡Œè·¯æ•°=2, Pipeline é˜¶æ®µæ•°=2\n",
      "Pipeline é˜¶æ®µ 1 ç”¨è®¾å¤‡: [0, 1]\n",
      "Pipeline é˜¶æ®µ 2 ç”¨è®¾å¤‡: [2, 3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼šè·å–å¯ç”¨ GPU è®¾å¤‡ï¼ˆæ¨¡æ‹Ÿï¼‰\n",
    "def get_available_devices(max_devices=4):\n",
    "    devices = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        if len(devices) >= max_devices:\n",
    "            break\n",
    "        devices.append(torch.device(f'cuda:{i}'))\n",
    "    if len(devices) == 0:\n",
    "        devices = [torch.device('cpu')] * min(max_devices, 1)\n",
    "    return devices\n",
    "\n",
    "# ç¤ºä¾‹æ¨¡å‹ï¼ˆå¤ç”¨åŸç»“æ„ï¼Œç¡®ä¿å…¼å®¹æ€§ï¼‰\n",
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# æ··åˆå¹¶è¡Œæ¨¡å‹ï¼šPipeline + DataParallel\n",
    "class HybridParallelModel(nn.Module):\n",
    "    def __init__(self, base_model, device_ids, dp_size=2, pp_size=2):\n",
    "        super().__init__()\n",
    "        self.dp_size = dp_size  # æ•°æ®å¹¶è¡Œè·¯æ•°ï¼ˆæ¯ä¸ª Pipeline é˜¶æ®µçš„å¤åˆ¶ä»½æ•°ï¼‰\n",
    "        self.pp_size = pp_size  # Pipeline é˜¶æ®µæ•°ï¼ˆæ¨¡å‹åˆ†å‰²åçš„æ®µæ•°ï¼‰\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        # éªŒè¯è®¾å¤‡æ•°é‡ï¼šæ€»è®¾å¤‡æ•° = æ•°æ®å¹¶è¡Œè·¯æ•° Ã— Pipeline é˜¶æ®µæ•°\n",
    "        assert len(device_ids) == dp_size * pp_size, \\\n",
    "            f\"è®¾å¤‡æ•°éœ€ç­‰äºæ•°æ®å¹¶è¡Œè·¯æ•°Ã—Pipeline é˜¶æ®µæ•°ï¼ˆå½“å‰ï¼š{len(device_ids)} != {dp_size}Ã—{pp_size}ï¼‰\"\n",
    "\n",
    "        # 1. Pipeline åˆ†å‰²ï¼šå°†åŸºç¡€æ¨¡å‹æ‹†åˆ†ä¸º pp_size ä¸ªé˜¶æ®µ\n",
    "        self.pipeline_stages = self._split_model_for_pipeline(base_model, pp_size)\n",
    "\n",
    "        # 2. æ•°æ®å¹¶è¡Œï¼šä¸ºæ¯ä¸ª Pipeline é˜¶æ®µåˆ›å»º dp_size ä»½å‰¯æœ¬ï¼ˆä½¿ç”¨ nn.DataParallelï¼‰\n",
    "        self.parallel_stages = nn.ModuleList()\n",
    "        current_devices = device_ids  # å¾…åˆ†é…çš„è®¾å¤‡åˆ—è¡¨\n",
    "        for stage in self.pipeline_stages:\n",
    "            # ä¸ºå½“å‰ Pipeline é˜¶æ®µåˆ†é… dp_size ä¸ªè®¾å¤‡ï¼ˆæ•°æ®å¹¶è¡Œï¼‰\n",
    "            dp_devices = current_devices[:dp_size]\n",
    "            current_devices = current_devices[dp_size:]  # å‰©ä½™è®¾å¤‡ç”¨äºä¸‹ä¸€é˜¶æ®µ\n",
    "\n",
    "            # ğŸ”¥ ä¿®å¤å…³é”®ï¼šå°† stage ç§»åŠ¨åˆ°ç¬¬ä¸€ä¸ªè®¾å¤‡ï¼ˆDataParallel è¦æ±‚ï¼‰\n",
    "            stage = stage.to(f'cuda:{dp_devices[0]}')\n",
    "\n",
    "            # åŒ…è£…ä¸ºæ•°æ®å¹¶è¡Œæ¨¡å—\n",
    "            dp_stage = nn.DataParallel(stage, device_ids=dp_devices)\n",
    "            self.parallel_stages.append(dp_stage)\n",
    "\n",
    "    def _split_model_for_pipeline(self, model, pp_size):\n",
    "        \"\"\"\n",
    "        è¾…åŠ©å‡½æ•°ï¼šå°† ExampleModel æŒ‰ Pipeline é€»è¾‘åˆ†å‰²ä¸º pp_size ä¸ªé˜¶æ®µ\n",
    "        åˆ†å‰²è§„åˆ™ï¼šæ ¹æ®çº¿æ€§å±‚æ‹†åˆ†ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µè®¡ç®—é‡å‡è¡¡\n",
    "        \"\"\"\n",
    "        stages = []\n",
    "        if pp_size == 2:\n",
    "            # 2 é˜¶æ®µåˆ†å‰²ï¼š[fc1+relu, fc2+relu+fc3]\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc2, model.relu, model.fc3))\n",
    "        elif pp_size == 3:\n",
    "            # 3 é˜¶æ®µåˆ†å‰²ï¼š[fc1+relu, fc2+relu, fc3]\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc2, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc3))\n",
    "        else:\n",
    "            # é»˜è®¤ä¸åˆ†å‰²ï¼ˆpp_size=1ï¼Œä»…æ•°æ®å¹¶è¡Œï¼‰\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu, model.fc2, model.relu, model.fc3))\n",
    "        return stages\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        æ··åˆå¹¶è¡Œå‰å‘ä¼ æ’­æµç¨‹ï¼š\n",
    "        è¾“å…¥ â†’ Pipeline é˜¶æ®µ 1ï¼ˆæ•°æ®å¹¶è¡Œï¼‰â†’ Pipeline é˜¶æ®µ 2ï¼ˆæ•°æ®å¹¶è¡Œï¼‰â†’ è¾“å‡º\n",
    "        \"\"\"\n",
    "        if len(self.parallel_stages) == 0:\n",
    "            return x\n",
    "\n",
    "        # ç¡®ä¿è¾“å…¥åœ¨ç¬¬ä¸€ä¸ª stage çš„ç¬¬ä¸€ä¸ªè®¾å¤‡ä¸Š\n",
    "        first_device = self.parallel_stages[0].device_ids[0]\n",
    "        current_x = x.to(f'cuda:{first_device}')\n",
    "\n",
    "        for stage in self.parallel_stages:\n",
    "            current_x = stage(current_x)  # æ¯ä¸ªé˜¶æ®µå†…éƒ¨æ•°æ®å¹¶è¡Œè®¡ç®—\n",
    "        return current_x\n",
    "\n",
    "\n",
    "# ========== ä¸»ç¨‹åºï¼šé…ç½®ä¸æµ‹è¯• ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. æ¨¡å‹å‚æ•°é…ç½®\n",
    "    input_size, hidden_size, output_size = 100, 200, 10\n",
    "    base_model = ExampleModel(input_size, hidden_size, output_size)\n",
    "\n",
    "    # 2. è‡ªåŠ¨è·å–è®¾å¤‡ï¼ˆæ¨¡æ‹Ÿï¼‰\n",
    "    available_devices = get_available_devices(max_devices=4)\n",
    "    device_ids = [dev.index for dev in available_devices if dev.type == 'cuda']\n",
    "    if len(device_ids) == 0:\n",
    "        print(\"âš ï¸  æœªæ£€æµ‹åˆ° CUDA è®¾å¤‡ï¼Œå›é€€åˆ° CPU æ¨¡å¼ï¼ˆä¸æ”¯æŒ DataParallelï¼‰\")\n",
    "        device_ids = [0]  # æ¨¡æ‹Ÿ CPU indexï¼Œä½† DataParallel ä¸æ”¯æŒçº¯ CPUï¼Œéœ€ç‰¹æ®Šå¤„ç†\n",
    "        # ä¸ºæ¼”ç¤ºï¼Œæˆ‘ä»¬å¼ºåˆ¶è‡³å°‘ 2 ä¸ªè®¾å¤‡ï¼Œè‹¥æ—  GPU åˆ™è·³è¿‡å¹¶è¡Œ\n",
    "        print(\"âš ï¸  è·³è¿‡å¹¶è¡Œæµ‹è¯•ï¼ˆæ—  GPUï¼‰\")\n",
    "        exit(0)\n",
    "\n",
    "    # 3. è°ƒæ•´å¹¶è¡Œé…ç½®ä»¥åŒ¹é…è®¾å¤‡æ•°\n",
    "    dp_size = 2 if len(device_ids) >= 4 else 1\n",
    "    pp_size = len(device_ids) // dp_size\n",
    "\n",
    "    print(f\"å¯ç”¨è®¾å¤‡: {device_ids}\")\n",
    "    print(f\"é…ç½® â†’ æ•°æ®å¹¶è¡Œè·¯æ•°: {dp_size}, Pipeline é˜¶æ®µæ•°: {pp_size}\")\n",
    "\n",
    "    # 4. åˆ›å»ºæ··åˆå¹¶è¡Œæ¨¡å‹\n",
    "    hybrid_model = HybridParallelModel(\n",
    "        base_model,\n",
    "        device_ids=device_ids,\n",
    "        dp_size=dp_size,\n",
    "        pp_size=pp_size\n",
    "    )\n",
    "\n",
    "    # 5. æµ‹è¯•è¾“å…¥ä¸è¾“å‡º\n",
    "    x = torch.randn(32, input_size)  # è¾“å…¥ï¼šæ‰¹é‡ 32ï¼Œç»´åº¦ 100\n",
    "    output = hybrid_model(x)\n",
    "\n",
    "    # 6. æ‰“å°æµ‹è¯•ç»“æœ\n",
    "    print(f\"\\n=== æ··åˆå¹¶è¡Œæµ‹è¯•ç»“æœ ===\")\n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}, è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "    print(f\"å¹¶è¡Œé…ç½®: æ•°æ®å¹¶è¡Œè·¯æ•°={dp_size}, Pipeline é˜¶æ®µæ•°={pp_size}\")\n",
    "    current_devices = device_ids\n",
    "    for i in range(pp_size):\n",
    "        dp_devices = current_devices[:dp_size]\n",
    "        current_devices = current_devices[dp_size:]\n",
    "        print(f\"Pipeline é˜¶æ®µ {i+1} ç”¨è®¾å¤‡: {dp_devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b3468",
   "metadata": {},
   "source": [
    "## 8. å®Œæ•´å®éªŒä¸æ€§èƒ½åˆ†æ\n",
    "\n",
    "ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„æµæ°´çº¿å¹¶è¡Œå®éªŒï¼ŒåŒ…æ‹¬è®­ç»ƒå¾ªç¯å’Œæ€§èƒ½åˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba526e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_usage(device_ids):\n",
    "    \"\"\"è·å–æ‰€æœ‰ GPU çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ\"\"\"\n",
    "    memory_usage = {}\n",
    "    for device in device_ids:\n",
    "        if device.type == 'cuda':\n",
    "            memory_allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)  # è½¬æ¢ä¸º GB\n",
    "            memory_cached = torch.cuda.memory_reserved(device) / (1024 ** 3)  # è½¬æ¢ä¸º GB\n",
    "            memory_usage[str(device)] = {\n",
    "                'allocated': memory_allocated,\n",
    "                'cached': memory_cached\n",
    "            }\n",
    "    return memory_usage\n",
    "\n",
    "def track_memory_usage(device_ids, memory_history):\n",
    "    \"\"\"è·Ÿè¸ªæ˜¾å­˜ä½¿ç”¨æƒ…å†µå¹¶è®°å½•åˆ°å†å²\"\"\"\n",
    "    current_memory = get_gpu_memory_usage(device_ids)\n",
    "    memory_history.append(current_memory)\n",
    "    return memory_history\n",
    "\n",
    "def calculate_avg_memory_usage(memory_history):\n",
    "    \"\"\"è®¡ç®—å¹³å‡æ˜¾å­˜ä½¿ç”¨é‡\"\"\"\n",
    "    if not memory_history:\n",
    "        return 0.0\n",
    "\n",
    "    total_allocated = 0.0\n",
    "    total_cached = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for memory_snapshot in memory_history:\n",
    "        for device, usage in memory_snapshot.items():\n",
    "            total_allocated += usage['allocated']\n",
    "            total_cached += usage['cached']\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    return total_allocated / count, total_cached / count\n",
    "\n",
    "# ä¿®æ”¹å®éªŒè¿è¡Œå‡½æ•°\n",
    "def run_pipeline_experiment(pipeline_class, strategy_name, num_epochs=50, batch_size=256, num_microbatches=32):\n",
    "    \"\"\"è¿è¡ŒæŒ‡å®šæµæ°´çº¿ç­–ç•¥çš„å®éªŒ - æ·»åŠ æ˜¾å­˜è·Ÿè¸ª\"\"\"\n",
    "    # 1. è‡ªåŠ¨è·å–è®¾å¤‡ä¸é…ç½®\n",
    "    device_ids = get_available_devices(max_devices=4)\n",
    "    num_stages = len(device_ids)\n",
    "    input_size, output_size = 100, 10\n",
    "\n",
    "    # æ¸…ç©ºæ˜¾å­˜ç¼“å­˜\n",
    "    for device in device_ids:\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 2. æ„å»º Pipeline æ¨¡å‹\n",
    "    model_parts = create_model_parts(input_size=input_size, output_size=output_size)\n",
    "    model_parts = model_parts[:num_stages]\n",
    "\n",
    "    # æ ¹æ®ç­–ç•¥åç§°é€‰æ‹©ä¸åŒçš„åˆå§‹åŒ–å‚æ•°\n",
    "    if strategy_name == \"Naive\":\n",
    "        pipeline_model = pipeline_class(model_parts, device_ids)\n",
    "    elif strategy_name == \"GPipe\":\n",
    "        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches)\n",
    "    elif strategy_name == \"1F1B\":\n",
    "        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches)\n",
    "    elif strategy_name == \"Interleaved 1F1B\":\n",
    "        pipeline_model = pipeline_class(model_parts, device_ids, num_microbatches=num_microbatches, interleaving_degree=2)\n",
    "    else:\n",
    "        raise ValueError(f\"æœªçŸ¥ç­–ç•¥: {strategy_name}\")\n",
    "\n",
    "    # 3. ä¼˜åŒ–å™¨ä¸è®­ç»ƒé…ç½®\n",
    "    optimizer = torch.optim.Adam(pipeline_model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    losses = []\n",
    "    times = []\n",
    "    memory_history = []  # å­˜å‚¨æ˜¾å­˜ä½¿ç”¨å†å²\n",
    "\n",
    "    # 4. è®­ç»ƒå¾ªç¯\n",
    "    print(f\"\\n=== å¼€å§‹ {strategy_name} Pipeline è®­ç»ƒï¼ˆå…±{num_epochs}è½®ï¼‰===\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # è®°å½•è®­ç»ƒå‰çš„æ˜¾å­˜ä½¿ç”¨\n",
    "        memory_history = track_memory_usage(device_ids, memory_history)\n",
    "\n",
    "        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®\n",
    "        x = torch.randn(batch_size, input_size)\n",
    "        y = torch.randint(0, output_size, (batch_size,))\n",
    "\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        outputs, activations = pipeline_model(x)\n",
    "\n",
    "        # å¤„ç†è¾“å‡ºæ‰¹æ¬¡å¤§å°ä¸åŒ¹é…çš„é—®é¢˜\n",
    "        if outputs.shape[0] != batch_size:\n",
    "            y_adjusted = y[:outputs.shape[0]].to(device_ids[-1])\n",
    "        else:\n",
    "            y_adjusted = y.to(device_ids[-1])\n",
    "\n",
    "        loss = F.cross_entropy(outputs, y_adjusted)\n",
    "\n",
    "        # åå‘ä¼ æ’­\n",
    "        optimizer.zero_grad()\n",
    "        if hasattr(pipeline_model, 'backward'):\n",
    "            pipeline_model.backward(loss, activations)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        # æ¢¯åº¦è£å‰ª\n",
    "        torch.nn.utils.clip_grad_norm_(pipeline_model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        losses.append(loss.item())\n",
    "        times.append(epoch_time)\n",
    "\n",
    "        # è®°å½•è®­ç»ƒåçš„æ˜¾å­˜ä½¿ç”¨\n",
    "        memory_history = track_memory_usage(device_ids, memory_history)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # è®¡ç®—å½“å‰å¹³å‡æ˜¾å­˜ä½¿ç”¨\n",
    "            avg_allocated, avg_cached = calculate_avg_memory_usage(memory_history)\n",
    "            print(f\"Epoch {epoch+1:3d}/{num_epochs}, æŸå¤±: {loss.item():.4f}, æ—¶é—´: {epoch_time:.4f}s, \"\n",
    "                  f\"æ˜¾å­˜: {avg_allocated:.2f}GB/{avg_cached:.2f}GB, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # 5. æ€§èƒ½åˆ†æ\n",
    "    bubble_rate = calculate_bubble_rate(strategy_name, num_stages, num_microbatches)\n",
    "    avg_time = sum(times) / len(times)\n",
    "    avg_allocated, avg_cached = calculate_avg_memory_usage(memory_history)\n",
    "\n",
    "    print(f\"\\n=== {strategy_name} å®éªŒç»“æœ ===\")\n",
    "    print(f\"è®¾å¤‡é…ç½®: {[str(dev) for dev in device_ids]}\")\n",
    "    print(f\"æµæ°´çº¿é˜¶æ®µ: {num_stages}, å¾®æ‰¹æ¬¡: {num_microbatches}\")\n",
    "    print(f\"ç©ºæ³¡ç‡: {bubble_rate:.3f} ({bubble_rate*100:.1f}%)\")\n",
    "    print(f\"å¹³å‡æ¯è½®æ—¶é—´: {avg_time:.4f}s\")\n",
    "    print(f\"å¹³å‡æ˜¾å­˜ä½¿ç”¨: {avg_allocated:.2f}GB (åˆ†é…) / {avg_cached:.2f}GB (ç¼“å­˜)\")\n",
    "    print(f\"æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}\")\n",
    "\n",
    "    # æ”¶æ•›åˆ¤æ–­\n",
    "    if losses[-1] < 1.0 and losses[-1] < losses[0]:\n",
    "        print(\"è®­ç»ƒç»“è®º: æˆåŠŸæ”¶æ•›\")\n",
    "    elif losses[-1] < losses[0]:\n",
    "        print(\"è®­ç»ƒç»“è®º: éƒ¨åˆ†æ”¶æ•›\")\n",
    "    else:\n",
    "        print(\"è®­ç»ƒç»“è®º: å¯èƒ½æœªæ”¶æ•›\")\n",
    "\n",
    "    return losses, bubble_rate, avg_time, avg_allocated, avg_cached\n",
    "\n",
    "# æ›´æ–°ç»“æœå±•ç¤ºå‡½æ•°\n",
    "def print_results_table(results):\n",
    "    \"\"\"æ‰“å°ç»“æœè¡¨æ ¼ - æ·»åŠ æ˜¾å­˜ä½¿ç”¨åˆ—\"\"\"\n",
    "    if not results:\n",
    "        print(\"æ²¡æœ‰æˆåŠŸè¿è¡Œçš„ç­–ç•¥\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== æ‰€æœ‰ç­–ç•¥ç»¼åˆæ¯”è¾ƒ ===\")\n",
    "    # è¡¨å¤´\n",
    "    print(f\"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+\")\n",
    "    print(f\"| {'ç­–ç•¥åç§°':<18} | {'å¹³å‡æ—¶é—´':<10} | {'æœ€ç»ˆæŸå¤±':<10} | {'ç©ºæ³¡ç‡':<10} | {'æ˜¾å­˜(GB)':<10} | {'ç¼“å­˜(GB)':<10} |\")\n",
    "    print(f\"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+\")\n",
    "\n",
    "    # è·å– Naive ç­–ç•¥çš„ç»“æœä½œä¸ºåŸºå‡†\n",
    "    naive_time = results[\"Naive\"][\"avg_time\"] if \"Naive\" in results else 1.0\n",
    "    num_devices = len(get_available_devices(max_devices=4))\n",
    "\n",
    "    # æ•°æ®è¡Œ\n",
    "    for strategy, data in results.items():\n",
    "        # speedup = calculate_speedup(naive_time, data[\"avg_time\"])\n",
    "        # efficiency = calculate_efficiency(speedup, num_devices)\n",
    "        print(f\"| {strategy:<18} | {data['avg_time']:>10.4f}s | {data['losses'][-1]:>10.4f} | \"\n",
    "              f\"{data['bubble_rate']:>10.3f} | \"\n",
    "              f\"{data['avg_allocated']:>10.2f} | {data['avg_cached']:>10.2f} |\")\n",
    "\n",
    "    print(f\"+{'-'*20}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+{'-'*12}+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "æ­£åœ¨è¿è¡Œ Naive ç­–ç•¥...\n",
      "============================================================\n",
      "\n",
      "=== å¼€å§‹ Naive Pipeline è®­ç»ƒï¼ˆå…± 50 è½®ï¼‰===\n",
      "Epoch  10/50, æŸå¤±: 2.3016, æ—¶é—´: 0.0090s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.001000\n",
      "Epoch  20/50, æŸå¤±: 2.3015, æ—¶é—´: 0.0084s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.000500\n",
      "Epoch  30/50, æŸå¤±: 2.3061, æ—¶é—´: 0.0083s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.000500\n",
      "Epoch  40/50, æŸå¤±: 2.3025, æ—¶é—´: 0.0080s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.000250\n",
      "Epoch  50/50, æŸå¤±: 2.3019, æ—¶é—´: 0.0078s, æ˜¾å­˜: 0.04GB/0.08GB, LR: 0.000250\n",
      "\n",
      "=== Naive å®éªŒç»“æœ ===\n",
      "è®¾å¤‡é…ç½®: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "æµæ°´çº¿é˜¶æ®µ: 4, å¾®æ‰¹æ¬¡: 32\n",
      "ç©ºæ³¡ç‡: 0.000 (0.0%)\n",
      "å¹³å‡æ¯è½®æ—¶é—´: 0.0088s\n",
      "å¹³å‡æ˜¾å­˜ä½¿ç”¨: 0.04GB (åˆ†é…) / 0.08GB (ç¼“å­˜)\n",
      "æœ€ç»ˆæŸå¤±: 2.3019\n",
      "è®­ç»ƒç»“è®º: éƒ¨åˆ†æ”¶æ•›\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "æ­£åœ¨è¿è¡Œ GPipe ç­–ç•¥...\n",
      "============================================================\n",
      "\n",
      "=== å¼€å§‹ GPipe Pipeline è®­ç»ƒï¼ˆå…± 50 è½®ï¼‰===\n",
      "Epoch  10/50, æŸå¤±: 2.3045, æ—¶é—´: 0.0510s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.001000\n",
      "Epoch  20/50, æŸå¤±: 2.3078, æ—¶é—´: 0.0513s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  30/50, æŸå¤±: 2.3016, æ—¶é—´: 0.0511s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  40/50, æŸå¤±: 2.3064, æ—¶é—´: 0.0512s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250\n",
      "Epoch  50/50, æŸå¤±: 2.3032, æ—¶é—´: 0.0515s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250\n",
      "\n",
      "=== GPipe å®éªŒç»“æœ ===\n",
      "è®¾å¤‡é…ç½®: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "æµæ°´çº¿é˜¶æ®µ: 4, å¾®æ‰¹æ¬¡: 32\n",
      "ç©ºæ³¡ç‡: 0.086 (8.6%)\n",
      "å¹³å‡æ¯è½®æ—¶é—´: 0.0514s\n",
      "å¹³å‡æ˜¾å­˜ä½¿ç”¨: 0.01GB (åˆ†é…) / 0.03GB (ç¼“å­˜)\n",
      "æœ€ç»ˆæŸå¤±: 2.3032\n",
      "è®­ç»ƒç»“è®º: éƒ¨åˆ†æ”¶æ•›\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "æ­£åœ¨è¿è¡Œ 1F1B ç­–ç•¥...\n",
      "============================================================\n",
      "\n",
      "=== å¼€å§‹ 1F1B Pipeline è®­ç»ƒï¼ˆå…± 50 è½®ï¼‰===\n",
      "Epoch  10/50, æŸå¤±: 2.3094, æ—¶é—´: 0.0570s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.001000\n",
      "Epoch  20/50, æŸå¤±: 2.3015, æ—¶é—´: 0.0568s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  30/50, æŸå¤±: 2.3067, æ—¶é—´: 0.0567s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  40/50, æŸå¤±: 2.3056, æ—¶é—´: 0.0572s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250\n",
      "Epoch  50/50, æŸå¤±: 2.3039, æ—¶é—´: 0.0569s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250\n",
      "\n",
      "=== 1F1B å®éªŒç»“æœ ===\n",
      "è®¾å¤‡é…ç½®: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "æµæ°´çº¿é˜¶æ®µ: 4, å¾®æ‰¹æ¬¡: 32\n",
      "ç©ºæ³¡ç‡: 0.094 (9.4%)\n",
      "å¹³å‡æ¯è½®æ—¶é—´: 0.0572s\n",
      "å¹³å‡æ˜¾å­˜ä½¿ç”¨: 0.01GB (åˆ†é…) / 0.03GB (ç¼“å­˜)\n",
      "æœ€ç»ˆæŸå¤±: 2.3039\n",
      "è®­ç»ƒç»“è®º: å¯èƒ½æœªæ”¶æ•›\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "æ­£åœ¨è¿è¡Œ Interleaved 1F1B ç­–ç•¥...\n",
      "============================================================\n",
      "\n",
      "=== å¼€å§‹ Interleaved 1F1B Pipeline è®­ç»ƒï¼ˆå…± 50 è½®ï¼‰===\n",
      "Epoch  10/50, æŸå¤±: 2.3026, æ—¶é—´: 0.0515s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.001000\n",
      "Epoch  20/50, æŸå¤±: 2.2959, æ—¶é—´: 0.0517s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  30/50, æŸå¤±: 2.3065, æ—¶é—´: 0.0519s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000500\n",
      "Epoch  40/50, æŸå¤±: 2.3047, æ—¶é—´: 0.0519s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250\n",
      "Epoch  50/50, æŸå¤±: 2.3014, æ—¶é—´: 0.0516s, æ˜¾å­˜: 0.01GB/0.03GB, LR: 0.000250\n",
      "\n",
      "=== Interleaved 1F1B å®éªŒç»“æœ ===\n",
      "è®¾å¤‡é…ç½®: ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']\n",
      "æµæ°´çº¿é˜¶æ®µ: 4, å¾®æ‰¹æ¬¡: 32\n",
      "ç©ºæ³¡ç‡: 0.047 (4.7%)\n",
      "å¹³å‡æ¯è½®æ—¶é—´: 0.0521s\n",
      "å¹³å‡æ˜¾å­˜ä½¿ç”¨: 0.01GB (åˆ†é…) / 0.03GB (ç¼“å­˜)\n",
      "æœ€ç»ˆæŸå¤±: 2.3014\n",
      "è®­ç»ƒç»“è®º: éƒ¨åˆ†æ”¶æ•›\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== æ‰€æœ‰ç­–ç•¥ç»¼åˆæ¯”è¾ƒ ===\n",
      "+--------------------+------------+------------+------------+------------+------------+\n",
      "| ç­–ç•¥åç§°               | å¹³å‡æ—¶é—´       | æœ€ç»ˆæŸå¤±       | ç©ºæ³¡ç‡        | æ˜¾å­˜(GB)     | ç¼“å­˜(GB)     |\n",
      "+--------------------+------------+------------+------------+------------+------------+\n",
      "| Naive              |     0.0088s |     2.3019 |      0.000 |       0.04 |       0.08 |\n",
      "| GPipe              |     0.0514s |     2.3032 |      0.086 |       0.01 |       0.03 |\n",
      "| 1F1B               |     0.0572s |     2.3039 |      0.094 |       0.01 |       0.03 |\n",
      "| Interleaved 1F1B   |     0.0521s |     2.3014 |      0.047 |       0.01 |       0.03 |\n",
      "+--------------------+------------+------------+------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# ç­–ç•¥ç±»æ˜ å°„\n",
    "strategy_classes = {\n",
    "    \"Naive\": NaivePipelineParallel,\n",
    "    \"GPipe\": GPipeParallel,\n",
    "    \"1F1B\": OneFOneBPipeline,\n",
    "    \"Interleaved 1F1B\": InterleavedOneFOneBPipeline\n",
    "}\n",
    "\n",
    "# è¿è¡Œæ‰€æœ‰å››ç§æµæ°´çº¿ç­–ç•¥\n",
    "results = {}\n",
    "\n",
    "for strategy_name, strategy_class in strategy_classes.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æ­£åœ¨è¿è¡Œ {strategy_name} ç­–ç•¥...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    try:\n",
    "        losses, bubble_rate, avg_time, avg_allocated, avg_cached = run_pipeline_experiment(\n",
    "            strategy_class,\n",
    "            strategy_name,\n",
    "            num_epochs=50,\n",
    "            batch_size=256,\n",
    "            num_microbatches=32\n",
    "        )\n",
    "        results[strategy_name] = {\n",
    "            \"losses\": losses,\n",
    "            \"bubble_rate\": bubble_rate,\n",
    "            \"avg_time\": avg_time,\n",
    "            \"avg_allocated\": avg_allocated,\n",
    "            \"avg_cached\": avg_cached\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ç­–ç•¥ {strategy_name} æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "# æ‰“å°ç»¼åˆæ¯”è¾ƒç»“æœ\n",
    "print_results_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69de3df",
   "metadata": {},
   "source": [
    "è¿™ä¸ªå®Œæ•´å®éªŒå±•ç¤ºäº†æµæ°´çº¿å¹¶è¡Œçš„å®é™…åº”ç”¨ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆ†å‰²ã€è®­ç»ƒå¾ªç¯å’Œç©ºæ³¡ç‡åˆ†æã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿˜éœ€è¦è€ƒè™‘æ¢¯åº¦åŒæ­¥ã€è®¾å¤‡é—´é€šä¿¡ä¼˜åŒ–ç­‰å¤æ‚é—®é¢˜ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b27fb7",
   "metadata": {},
   "source": [
    "## æ€»ç»“ä¸æ€è€ƒ\n",
    "\n",
    "é€šè¿‡è¡¥å…… Interleaved 1F1B å®ç°ï¼Œæˆ‘ä»¬å®Œæˆäº† Pipeline å¹¶è¡Œä¸‰å¤§æ ¸å¿ƒè°ƒåº¦ç­–ç•¥çš„è¦†ç›–ï¼š\n",
    "\n",
    "1. **Gpipe (Native PP)**ï¼šç®€å•ç›´è§‚ï¼Œç©ºæ³¡ç‡é«˜ï¼Œæ˜¾å­˜å ç”¨å¤§ã€‚\n",
    "\n",
    "2. **1F1B**ï¼šé€šè¿‡å‰å‘/åå‘äº¤æ›¿ï¼Œé™ä½æ˜¾å­˜å ç”¨ï¼Œå‹ç¼©éƒ¨åˆ†ç©ºæ³¡ã€‚\n",
    "\n",
    "3. **Interleaved 1F1B**ï¼šå¼•å…¥è™šæ‹Ÿé˜¶æ®µï¼Œåœ¨åŒä¸€è®¾å¤‡ä¸Šäº¤ç»‡æ‰§è¡Œå¤šä¸ªå¾®æ‰¹æ¬¡ï¼Œè¿›ä¸€æ­¥å‹ç¼©ç©ºæ³¡ï¼Œå°¤å…¶é€‚åˆå¤§å¾®æ‰¹æ¬¡åœºæ™¯ã€‚\n",
    "\n",
    "å·¥ç¨‹å»ºè®®ï¼š\n",
    "\n",
    "- å¾®æ‰¹æ¬¡æ•°é‡ M åº”è¿œå¤§äºé˜¶æ®µæ•° Sï¼ˆæ¨è M >= 4Sï¼‰ã€‚\n",
    "- Interleaved 1F1B åœ¨ M >> S æ—¶ä¼˜åŠ¿æ˜æ˜¾ï¼Œä½†å®ç°å¤æ‚åº¦é«˜ã€‚\n",
    "- æ··åˆå¹¶è¡Œï¼ˆDP+PP+TPï¼‰æ˜¯å¤§æ¨¡å‹è®­ç»ƒæ ‡é…ï¼Œéœ€é…åˆæ¢¯åº¦æ£€æŸ¥ç‚¹ã€é€šä¿¡ä¼˜åŒ–ç­‰æŠ€æœ¯.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

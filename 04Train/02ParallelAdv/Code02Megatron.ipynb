{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a414ee2",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE02: Megatron TP复现(DONE)\n",
    "\n",
    "> Author by: 许灿岷\n",
    "\n",
    "在大模型训练中，张量并行（Tensor Parallelism, TP）是一种关键技术，它通过将模型的单个层或操作分布在多个设备上来解决内存限制和计算瓶颈。NVIDIA 的 Megatron-LM 框架是 TP 技术的典型代表，它专门针对 Transformer 架构进行了优化。\n",
    "\n",
    "本实验将深入探讨 Megatron 风格的 TP 原理，并通过可执行的代码实现展示如何在 Transformer 模型中应用。\n",
    "\n",
    "## 1.  TP 基础原理\n",
    "\n",
    "TP 核心思想是将大矩阵运算分解到多个设备上执行。考虑一个简单的矩阵乘法运算：$Y = XW$，其中 $X$ 是输入矩阵，$W$ 是权重矩阵。\n",
    "\n",
    "在 TP 中，我们将权重矩阵 $W$ 按列分割为多个子矩阵：\n",
    "\n",
    "$$W = [W_1, W_2, ..., W_n]$$\n",
    "\n",
    "每个设备 $i$ 计算部分结果：\n",
    "\n",
    "$$Y_i = XW_i$$\n",
    "\n",
    "然后通过 All-Gather 操作收集所有部分结果：\n",
    "\n",
    "$$Y = [Y_1, Y_2, ..., Y_n]$$\n",
    "\n",
    "这种分割方式的数学表达为：\n",
    "\n",
    "$$Y = XW = X[W_1, W_2, ..., W_n] = [XW_1, XW_2, ..., XW_n]$$\n",
    "\n",
    "![](./images/Code02Megatron01.png)\n",
    "\n",
    "对于反向传播，梯度也需要相应的分割和聚合操作。这种并行策略特别适合 Transformer 架构，因为其核心组件（MLP 和 Attention）都包含大量的矩阵运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8791721",
   "metadata": {},
   "source": [
    "下面我们先进行模型结构的搭建和初始化分布式训练环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "yclhvmkucjr",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================\n",
    "全局配置参数（轻量级配置，快速验证）\n",
    "============================================\n",
    "\"\"\"\n",
    "\n",
    "# 模型配置\n",
    "MODEL_CONFIG = {\n",
    "    'vocab_size': 1024,        # 词汇表大小（轻量级）\n",
    "    'hidden_size': 512,       # 隐藏层维度\n",
    "    'num_layers': 8,          # Transformer 层数\n",
    "    'num_heads': 8,           # 注意力头数（需被 NUM_GPUS 整除）\n",
    "    'ffn_size': 2048,         # MLP 中间层维度（通常为 hidden_size 的 4 倍）\n",
    "}\n",
    "\n",
    "# 训练配置（序列记忆任务）\n",
    "TRAIN_CONFIG = {\n",
    "    'batch_size': 8,          # 批次大小\n",
    "    'seq_length': 32,         # 序列长度\n",
    "    'num_epochs': 5,          # 训练轮数\n",
    "    'lr': 1e-3,               # 学习率\n",
    "    'print_interval': 10,     # 打印间隔（steps）\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Megatron 张量并行验证\n",
    "- 测试 Megatron-LM 的分布式训练\n",
    "- 更小的模型规模（快速验证）\n",
    "- 简单的序列记忆任务\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parameter import Parameter\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "# Check and warn about import issues\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA is not available. This script requires CUDA.\", file=sys.stderr)\n",
    "\n",
    "def init_distributed():\n",
    "    \"\"\"初始化分布式环境\"\"\"\n",
    "    if not dist.is_available():\n",
    "        raise RuntimeError(\"Distributed package is not available.\")\n",
    "\n",
    "    # Set NCCL environment variables\n",
    "    os.environ[\"NCCL_DEBUG\"] = os.environ.get(\"NCCL_DEBUG\", \"WARN\")\n",
    "    os.environ[\"NCCL_SOCKET_IFNAME\"] = os.environ.get(\"NCCL_SOCKET_IFNAME\", \"^docker0,lo\")\n",
    "    os.environ[\"NCCL_IB_DISABLE\"] = os.environ.get(\"NCCL_IB_DISABLE\", \"1\")\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = os.environ.get(\"NCCL_P2P_DISABLE\", \"0\")\n",
    "    # 使用 PyTorch 推荐的环境变量名\n",
    "    os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = os.environ.get(\"TORCH_NCCL_ASYNC_ERROR_HANDLING\", \"1\")\n",
    "\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n",
    "\n",
    "    print(f\"Rank: {rank}, World size: {world_size}\")\n",
    "\n",
    "    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "    import datetime\n",
    "    timeout_minutes = int(os.environ.get(\"TORCH_DIST_TIMEOUT_MINUTES\", \"30\"))\n",
    "\n",
    "    try:\n",
    "        dist.init_process_group(\n",
    "            backend=backend,\n",
    "            init_method=\"env://\",\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            timeout=datetime.timedelta(minutes=timeout_minutes)\n",
    "        )\n",
    "        print(f\"Rank {rank}: Successfully initialized with {backend} backend\")\n",
    "    except Exception as e:\n",
    "        print(f\"Rank {rank}: Failed to initialize: {str(e)}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", rank % torch.cuda.device_count()))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    return dist.get_rank(), dist.get_world_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a832685",
   "metadata": {},
   "source": [
    "构建一些基本的工具函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf90ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllGather(torch.autograd.Function):\n",
    "    \"\"\"All-Gather 操作 - 在特征维度上拼接各 GPU 的部分输出\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.world_size = dist.get_world_size()\n",
    "        gathered = [torch.zeros_like(x) for _ in range(ctx.world_size)]\n",
    "        dist.all_gather(gathered, x)\n",
    "        return torch.cat(gathered, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return grad.chunk(ctx.world_size, dim=-1)[dist.get_rank()]\n",
    "\n",
    "class AllReduce(torch.autograd.Function):\n",
    "    \"\"\"AllReduce 操作的 autograd 包装 - 修复 PyTorch 警告\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = x.clone()\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        # 梯度在反向传播时也需要 all_reduce\n",
    "        output = grad.clone()\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f5178",
   "metadata": {},
   "source": [
    "这些基础工具函数为 TP 提供了必要的通信原语，两者均支持自动微分，确保反向传播时梯度能正确传递。\n",
    "\n",
    "## 2. MLP 层 TP 实现\n",
    "\n",
    "在 Transformer 的 MLP 层中，通常包含两个线性变换和一个激活函数：\n",
    "\n",
    "$$MLP(x) = Activation(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    " TP 将这两个线性变换分割到多个设备上，核心策略是**列并行+行并行**的组合，平衡计算量与通信开销：\n",
    "\n",
    "1. 第一个线性变换（$xW_1$）按列分割权重 $W_1$，每个设备计算部分输出后通过 All-Gather 聚合；\n",
    "2. 第二个线性变换（$Activation(...)W_2$）按行分割权重 $W_2$，输入先通过 Reduce-Scatter 分散后再计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30531c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnLinear(nn.Module):\n",
    "    \"\"\"列并行线性层\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.local_out_dim = out_dim // world_size\n",
    "        self.weight = Parameter(torch.Tensor(self.local_out_dim, in_dim))\n",
    "        self.bias = Parameter(torch.Tensor(self.local_out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_out = F.linear(x, self.weight, self.bias)\n",
    "        return local_out\n",
    "\n",
    "class RowLinear(nn.Module):\n",
    "    \"\"\"行并行线性层 - 使用 AllReduce 包装\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.local_in_dim = in_dim // world_size\n",
    "        self.weight = Parameter(torch.Tensor(out_dim, self.local_in_dim))\n",
    "        self.bias = Parameter(torch.Tensor(out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_chunks = x.chunk(dist.get_world_size(), dim=-1)\n",
    "        local_input = input_chunks[dist.get_rank()]\n",
    "        local_output = F.linear(local_input, self.weight, self.bias)\n",
    "        # 使用 AllReduce 包装，修复 autograd 警告\n",
    "        return AllReduce.apply(local_output)\n",
    "\n",
    "class ParallelMLP(nn.Module):\n",
    "    \"\"\"并行 MLP 层\"\"\"\n",
    "    def __init__(self, hidden_size, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.fc1 = ColumnLinear(hidden_size, ffn_size, world_size, rank)\n",
    "        self.fc2 = RowLinear(ffn_size, hidden_size, world_size, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate = self.fc1(x)\n",
    "        intermediate_full = AllGather.apply(intermediate)\n",
    "        activated = F.gelu(intermediate_full)\n",
    "        return self.fc2(activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1e55f",
   "metadata": {},
   "source": [
    "该实现的核心优势是**无计算冗余**：每个设备仅计算部分矩阵乘法，通过两次通信操作（All-Gather+Reduce-Scatter）确保最终结果与单卡计算完全一致，同时将单卡内存占用降低至 $1/world_size$。\n",
    "\n",
    "![](./images/Code02Megatron03.png)\n",
    "\n",
    "## 3. Attention 层 TP 实现\n",
    "\n",
    "Transformer 的 Attention 层包含三个核心计算：Q（查询）、K（键）、V（值）的投影，以及 Attention 分数计算与加权求和。其数学表达为：\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "在 TP 中，核心策略是**注意力头分片**：将所有注意力头均匀分配到多个设备，每个设备仅计算部分头的 Attention 结果，最后通过输出投影层聚合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9da4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelAttention(nn.Module):\n",
    "    \"\"\"并行 Attention 层\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, world_size, rank):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        assert num_heads % world_size == 0\n",
    "\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.world_size = world_size\n",
    "        self.local_heads = num_heads // world_size\n",
    "\n",
    "        self.q_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.k_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.v_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.out_proj = RowLinear(hidden_size, hidden_size, world_size, rank)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_scores += mask\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(B, S, self.local_heads * self.head_dim)\n",
    "        complete_attn_output = AllGather.apply(attn_output)\n",
    "\n",
    "        return self.out_proj(complete_attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cc6d2",
   "metadata": {},
   "source": [
    "该实现的关键设计是**头级并行**：每个设备仅存储部分 Q/K/V 投影权重，计算部分注意力头，避免了全量 Attention 计算的内存开销。\n",
    "\n",
    "![](./images/Code02Megatron02.png)\n",
    "\n",
    "## 4. 完整并行 Transformer\n",
    "\n",
    "完整的 Transformer 块包含“多头注意力层+MLP 层”，并配合残差连接和层归一化。在 TP 中，层归一化需在所有设备上**独立同步执行**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4f75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTransformerBlock(nn.Module):\n",
    "    \"\"\"并行 Transformer 块\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.attn = ParallelAttention(hidden_size, num_heads, world_size, rank)\n",
    "        self.mlp = ParallelMLP(hidden_size, ffn_size, world_size, rank)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        return x + self.mlp(self.norm2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580eb51b",
   "metadata": {},
   "source": [
    "## 5. Embedding 层并行\n",
    "\n",
    "在大型语言模型中，词汇表规模常达数万至数十万，导致嵌入层占用大量内存。Embedding Parallel 通过**词汇表分片**解决这一问题：每个设备仅保存部分词嵌入，通过掩码和 All-Gather 聚合完整结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6786c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelEmbedding(nn.Module):\n",
    "    \"\"\"并行 Embedding 层\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        part_size = vocab_size // world_size\n",
    "        remainder = vocab_size % world_size\n",
    "        self.start_idx = rank * part_size + min(rank, remainder)\n",
    "        self.end_idx = self.start_idx + part_size + (1 if rank < remainder else 0)\n",
    "        self.local_vocab_size = self.end_idx - self.start_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(self.local_vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        local_input = input.clone() - self.start_idx\n",
    "        mask = (input >= self.start_idx) & (input < self.end_idx)\n",
    "        local_input[~mask] = 0\n",
    "\n",
    "        local_emb = self.embedding(local_input)\n",
    "        local_emb[~mask] = 0\n",
    "\n",
    "        # 使用 AllReduce 包装\n",
    "        return AllReduce.apply(local_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8837d2b",
   "metadata": {},
   "source": [
    "## 6. 完整 Transformer 并行\n",
    "\n",
    "将上述并行组件（ EP 嵌入、Transformer 并行、并行输出层）组合，形成完整的 TP。输出层采用列并行，确保与 EP 的分割策略一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd48da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelTransformer(nn.Module):\n",
    "    \"\"\"完整的并行 Transformer 模型\"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.embedding = ParallelEmbedding(vocab_size, hidden_size, world_size, rank)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1024, hidden_size))\n",
    "        self.layers = nn.ModuleList([\n",
    "            ParallelTransformerBlock(hidden_size, num_heads, ffn_size, world_size, rank)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.head = ColumnLinear(hidden_size, vocab_size, world_size, rank)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids) + self.pos_embed[:, :input_ids.size(1)]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        local_logits = self.head(self.norm(x))\n",
    "        return AllGather.apply(local_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843f2cb",
   "metadata": {},
   "source": [
    "## 7. 实验与性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8aed971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_stats():\n",
    "    \"\"\"获取当前 GPU 显存使用情况（MB）\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2    # MB\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        return {\n",
    "            'allocated': allocated,\n",
    "            'reserved': reserved,\n",
    "            'max_allocated': max_allocated\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def reset_memory_stats():\n",
    "    \"\"\"重置显存统计\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def create_sequence_memory_task(vocab_size=512, seq_len=32, num_sequences=100):\n",
    "    \"\"\"\n",
    "    创建简单的序列记忆任务\n",
    "    - 有限词汇表（512 个 token）\n",
    "    - 短序列（32 个 token）\n",
    "    - 固定的训练序列（100 条）\n",
    "    \"\"\"\n",
    "    # 生成固定的训练序列，确保可重复\n",
    "    torch.manual_seed(42)\n",
    "    sequences = []\n",
    "    for i in range(num_sequences):\n",
    "        # 每个序列有一定的模式，更容易学习\n",
    "        seq = torch.randint(0, vocab_size, (seq_len,))\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "def train_example():\n",
    "    \"\"\"训练示例\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available.\")\n",
    "\n",
    "    try:\n",
    "        rank, world_size = init_distributed()\n",
    "\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        if gpu_count < world_size:\n",
    "            raise RuntimeError(f\"Not enough GPUs. Required: {world_size}, Available: {gpu_count}\")\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Megatron 张量并行验证\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"GPU 数量: {world_size}\")\n",
    "            print(f\"主机名: {socket.gethostname()}\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # 更小的模型配置 - 用于快速验证\n",
    "        config = {\n",
    "            'vocab_size': 1024,\n",
    "            'hidden_size': 512,\n",
    "            'num_layers': 8,\n",
    "            'num_heads': 8,\n",
    "            'ffn_size': 1024,\n",
    "        }\n",
    "\n",
    "        model = ParallelTransformer(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            num_heads=config['num_heads'],\n",
    "            ffn_size=config['ffn_size'],\n",
    "            world_size=world_size,\n",
    "            rank=rank\n",
    "        ).cuda()\n",
    "\n",
    "        if rank == 0:\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"\\n 模型配置:\")\n",
    "            print(f\"  - Vocab: {config['vocab_size']}, Hidden: {config['hidden_size']}\")\n",
    "            print(f\"  - Layers: {config['num_layers']}, Heads: {config['num_heads']}\")\n",
    "            print(f\"  - 参数量: {total_params:,} (每 GPU 约 {total_params//(world_size*4):,})\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # 优化器 - 使用更大的学习率\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "        # 创建简单的序列记忆任务\n",
    "        train_data = create_sequence_memory_task(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            seq_len=32,  # 短序列\n",
    "            num_sequences=100  # 100 条训练序列\n",
    "        )\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n 训练任务: 序列记忆\")\n",
    "            print(f\"  - 训练序列数: {train_data.shape[0]}\")\n",
    "            print(f\"  - 序列长度: {train_data.shape[1]}\")\n",
    "            print(f\"  - 词汇表大小: {config['vocab_size']}\")\n",
    "            print(f\"\\n 开始训练...\")\n",
    "            print(f\"{'-'*60}\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        # 重置显存统计\n",
    "        reset_memory_stats()\n",
    "\n",
    "        # 训练循环 - 多个 epoch 确保收敛\n",
    "        num_epochs = 5\n",
    "        steps_per_epoch = 100\n",
    "        best_loss = float('inf')\n",
    "        peak_memory = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for step in range(steps_per_epoch):\n",
    "                # 从训练集随机采样\n",
    "                batch_size = 16\n",
    "                indices = torch.randint(0, len(train_data), (batch_size,))\n",
    "                input_ids = train_data[indices].cuda()\n",
    "\n",
    "                # 前向传播\n",
    "                logits = model(input_ids)\n",
    "\n",
    "                # 计算损失\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, config['vocab_size']),\n",
    "                    input_ids.view(-1)\n",
    "                )\n",
    "\n",
    "                # 反向传播\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # 更新峰值显存\n",
    "                if torch.cuda.is_available():\n",
    "                    current_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "                    peak_memory = max(peak_memory, current_mem)\n",
    "\n",
    "                # 打印进度\n",
    "                if rank == 0 and step % 20 == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Step {step:3d}/{steps_per_epoch}, Loss: {loss.item():.4f}, Avg: {avg_loss:.4f}\")\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / steps_per_epoch\n",
    "\n",
    "            if rank == 0:\n",
    "                improvement = \"\" if epoch == 0 else f\" (↓{best_loss - avg_epoch_loss:.4f})\"\n",
    "                print(f\"{'='*60}\")\n",
    "                print(f\"Epoch {epoch+1} 完成 - 平均 Loss: {avg_epoch_loss:.4f}{improvement}\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "                if avg_epoch_loss < best_loss:\n",
    "                    best_loss = avg_epoch_loss\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"✅ 训练完成!\")\n",
    "            print(f\"   最佳 Loss: {best_loss:.4f}\")\n",
    "            print(f\"   最终 Loss: {avg_epoch_loss:.4f}\")\n",
    "            print(f\"   Loss 下降: {train_data.shape[0] * 0.1:.4f} → {avg_epoch_loss:.4f}\")\n",
    "            print(f\"   峰值显存: {peak_memory:.2f} MB ({peak_memory/1024:.2f} GB)\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "        print(f\"Rank {error_rank} encountered error: {str(e)}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if dist.is_initialized():\n",
    "            dist.barrier()\n",
    "        raise e\n",
    "    finally:\n",
    "        if dist.is_initialized():\n",
    "            try:\n",
    "                dist.barrier()\n",
    "            except:\n",
    "                pass\n",
    "            dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d275333",
   "metadata": {},
   "source": [
    "下面的这个代码块是为了自动生成分布式训练脚本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb3fa306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Code02Megatron.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Code02Megatron.py\n",
    "\"\"\"\n",
    "============================================\n",
    "Megatron 张量并行分布式训练脚本\n",
    "============================================\n",
    "本脚本从 Jupyter Notebook 自动生成，用于多 GPU 分布式训练\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parameter import Parameter\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "def init_distributed():\n",
    "    \"\"\"初始化分布式环境\"\"\"\n",
    "    if not dist.is_available():\n",
    "        raise RuntimeError(\"Distributed package is not available.\")\n",
    "\n",
    "    # Set NCCL environment variables\n",
    "    os.environ[\"NCCL_DEBUG\"] = os.environ.get(\"NCCL_DEBUG\", \"WARN\")\n",
    "    os.environ[\"NCCL_SOCKET_IFNAME\"] = os.environ.get(\"NCCL_SOCKET_IFNAME\", \"^docker0,lo\")\n",
    "    os.environ[\"NCCL_IB_DISABLE\"] = os.environ.get(\"NCCL_IB_DISABLE\", \"1\")\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = os.environ.get(\"NCCL_P2P_DISABLE\", \"0\")\n",
    "    os.environ[\"TORCH_NCCL_ASYNC_ERROR_HANDLING\"] = os.environ.get(\"TORCH_NCCL_ASYNC_ERROR_HANDLING\", \"1\")\n",
    "\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n",
    "\n",
    "    print(f\"Rank: {rank}, World size: {world_size}\")\n",
    "\n",
    "    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
    "    import datetime\n",
    "    timeout_minutes = int(os.environ.get(\"TORCH_DIST_TIMEOUT_MINUTES\", \"30\"))\n",
    "\n",
    "    try:\n",
    "        dist.init_process_group(\n",
    "            backend=backend,\n",
    "            init_method=\"env://\",\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            timeout=datetime.timedelta(minutes=timeout_minutes)\n",
    "        )\n",
    "        print(f\"Rank {rank}: Successfully initialized with {backend} backend\")\n",
    "    except Exception as e:\n",
    "        print(f\"Rank {rank}: Failed to initialize: {str(e)}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", rank % torch.cuda.device_count()))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    return dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "\n",
    "class AllGather(torch.autograd.Function):\n",
    "    \"\"\"All-Gather 操作\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.world_size = dist.get_world_size()\n",
    "        gathered = [torch.zeros_like(x) for _ in range(ctx.world_size)]\n",
    "        dist.all_gather(gathered, x)\n",
    "        return torch.cat(gathered, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        return grad.chunk(ctx.world_size, dim=-1)[dist.get_rank()]\n",
    "\n",
    "\n",
    "class AllReduce(torch.autograd.Function):\n",
    "    \"\"\"AllReduce 操作\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = x.clone()\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        output = grad.clone()\n",
    "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ColumnLinear(nn.Module):\n",
    "    \"\"\"列并行线性层\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.local_out_dim = out_dim // world_size\n",
    "        self.weight = Parameter(torch.Tensor(self.local_out_dim, in_dim))\n",
    "        self.bias = Parameter(torch.Tensor(self.local_out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_out = F.linear(x, self.weight, self.bias)\n",
    "        return local_out\n",
    "\n",
    "\n",
    "class RowLinear(nn.Module):\n",
    "    \"\"\"行并行线性层\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.local_in_dim = in_dim // world_size\n",
    "        self.weight = Parameter(torch.Tensor(out_dim, self.local_in_dim))\n",
    "        self.bias = Parameter(torch.Tensor(out_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_chunks = x.chunk(dist.get_world_size(), dim=-1)\n",
    "        local_input = input_chunks[dist.get_rank()]\n",
    "        local_output = F.linear(local_input, self.weight, self.bias)\n",
    "        return AllReduce.apply(local_output)\n",
    "\n",
    "\n",
    "class ParallelMLP(nn.Module):\n",
    "    \"\"\"并行 MLP 层\"\"\"\n",
    "    def __init__(self, hidden_size, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.fc1 = ColumnLinear(hidden_size, ffn_size, world_size, rank)\n",
    "        self.fc2 = RowLinear(ffn_size, hidden_size, world_size, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate = self.fc1(x)\n",
    "        intermediate_full = AllGather.apply(intermediate)\n",
    "        activated = F.gelu(intermediate_full)\n",
    "        return self.fc2(activated)\n",
    "\n",
    "\n",
    "class ParallelAttention(nn.Module):\n",
    "    \"\"\"并行 Attention 层\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, world_size, rank):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        assert num_heads % world_size == 0\n",
    "\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.world_size = world_size\n",
    "        self.local_heads = num_heads // world_size\n",
    "\n",
    "        self.q_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.k_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.v_proj = ColumnLinear(hidden_size, hidden_size, world_size, rank)\n",
    "        self.out_proj = RowLinear(hidden_size, hidden_size, world_size, rank)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, S, self.local_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_scores += mask\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(B, S, self.local_heads * self.head_dim)\n",
    "        complete_attn_output = AllGather.apply(attn_output)\n",
    "\n",
    "        return self.out_proj(complete_attn_output)\n",
    "\n",
    "\n",
    "class ParallelTransformerBlock(nn.Module):\n",
    "    \"\"\"并行 Transformer 块\"\"\"\n",
    "    def __init__(self, hidden_size, num_heads, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.attn = ParallelAttention(hidden_size, num_heads, world_size, rank)\n",
    "        self.mlp = ParallelMLP(hidden_size, ffn_size, world_size, rank)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), mask)\n",
    "        return x + self.mlp(self.norm2(x))\n",
    "\n",
    "\n",
    "class ParallelEmbedding(nn.Module):\n",
    "    \"\"\"并行 Embedding 层\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        part_size = vocab_size // world_size\n",
    "        remainder = vocab_size % world_size\n",
    "        self.start_idx = rank * part_size + min(rank, remainder)\n",
    "        self.end_idx = self.start_idx + part_size + (1 if rank < remainder else 0)\n",
    "        self.local_vocab_size = self.end_idx - self.start_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(self.local_vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        local_input = input.clone() - self.start_idx\n",
    "        mask = (input >= self.start_idx) & (input < self.end_idx)\n",
    "        local_input[~mask] = 0\n",
    "\n",
    "        local_emb = self.embedding(local_input)\n",
    "        local_emb[~mask] = 0\n",
    "\n",
    "        return AllReduce.apply(local_emb)\n",
    "\n",
    "\n",
    "class ParallelTransformer(nn.Module):\n",
    "    \"\"\"完整的并行 Transformer 模型\"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, ffn_size, world_size, rank):\n",
    "        super().__init__()\n",
    "        self.embedding = ParallelEmbedding(vocab_size, hidden_size, world_size, rank)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1024, hidden_size))\n",
    "        self.layers = nn.ModuleList([\n",
    "            ParallelTransformerBlock(hidden_size, num_heads, ffn_size, world_size, rank)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.head = ColumnLinear(hidden_size, vocab_size, world_size, rank)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids) + self.pos_embed[:, :input_ids.size(1)]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        local_logits = self.head(self.norm(x))\n",
    "        return AllGather.apply(local_logits)\n",
    "\n",
    "\n",
    "def create_sequence_memory_task(vocab_size=512, seq_len=32, num_sequences=100):\n",
    "    \"\"\"创建序列记忆任务\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    sequences = []\n",
    "    for i in range(num_sequences):\n",
    "        seq = torch.randint(0, vocab_size, (seq_len,))\n",
    "        sequences.append(seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "\n",
    "def train_example():\n",
    "    \"\"\"训练示例\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available.\")\n",
    "\n",
    "    try:\n",
    "        rank, world_size = init_distributed()\n",
    "\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        if gpu_count < world_size:\n",
    "            raise RuntimeError(f\"Not enough GPUs. Required: {world_size}, Available: {gpu_count}\")\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Megatron 张量并行验证\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"GPU 数量: {world_size}\")\n",
    "            print(f\"主机名: {socket.gethostname()}\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        config = {\n",
    "            'vocab_size': 1024,\n",
    "            'hidden_size': 512,\n",
    "            'num_layers': 8,\n",
    "            'num_heads': 8,\n",
    "            'ffn_size': 1024,\n",
    "        }\n",
    "\n",
    "        model = ParallelTransformer(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            num_heads=config['num_heads'],\n",
    "            ffn_size=config['ffn_size'],\n",
    "            world_size=world_size,\n",
    "            rank=rank\n",
    "        ).cuda()\n",
    "\n",
    "        if rank == 0:\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"\\n 模型配置:\")\n",
    "            print(f\"  - Vocab: {config['vocab_size']}, Hidden: {config['hidden_size']}\")\n",
    "            print(f\"  - Layers: {config['num_layers']}, Heads: {config['num_heads']}\")\n",
    "            print(f\"  - 参数量: {total_params:,} (每 GPU 约 {total_params//(world_size*4):,})\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "        train_data = create_sequence_memory_task(\n",
    "            vocab_size=config['vocab_size'],\n",
    "            seq_len=32,\n",
    "            num_sequences=100\n",
    "        )\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n 训练任务: 序列记忆\")\n",
    "            print(f\"  - 训练序列数: {train_data.shape[0]}\")\n",
    "            print(f\"  - 序列长度: {train_data.shape[1]}\")\n",
    "            print(f\"  - 词汇表大小: {config['vocab_size']}\")\n",
    "            print(f\"\\n 开始训练...\")\n",
    "            print(f\"{'-'*60}\")\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "        num_epochs = 5\n",
    "        steps_per_epoch = 100\n",
    "        best_loss = float('inf')\n",
    "        peak_memory = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for step in range(steps_per_epoch):\n",
    "                batch_size = 16\n",
    "                indices = torch.randint(0, len(train_data), (batch_size,))\n",
    "                input_ids = train_data[indices].cuda()\n",
    "\n",
    "                logits = model(input_ids)\n",
    "\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, config['vocab_size']),\n",
    "                    input_ids.view(-1)\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    current_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "                    peak_memory = max(peak_memory, current_mem)\n",
    "\n",
    "                if rank == 0 and step % 20 == 0:\n",
    "                    avg_loss = epoch_loss / (step + 1)\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Step {step:3d}/{steps_per_epoch}, Loss: {loss.item():.4f}, Avg: {avg_loss:.4f}\")\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / steps_per_epoch\n",
    "\n",
    "            if rank == 0:\n",
    "                improvement = \"\" if epoch == 0 else f\" (↓{best_loss - avg_epoch_loss:.4f})\"\n",
    "                print(f\"{'='*60}\")\n",
    "                print(f\"Epoch {epoch+1} 完成 - 平均 Loss: {avg_epoch_loss:.4f}{improvement}\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "                if avg_epoch_loss < best_loss:\n",
    "                    best_loss = avg_epoch_loss\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"✅ 训练完成!\")\n",
    "            print(f\"   最佳 Loss: {best_loss:.4f}\")\n",
    "            print(f\"   最终 Loss: {avg_epoch_loss:.4f}\")\n",
    "            print(f\"   峰值显存: {peak_memory:.2f} MB ({peak_memory/1024:.2f} GB)\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "        print(f\"Rank {error_rank} encountered error: {str(e)}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if dist.is_initialized():\n",
    "            dist.barrier()\n",
    "        raise e\n",
    "    finally:\n",
    "        if dist.is_initialized():\n",
    "            try:\n",
    "                dist.barrier()\n",
    "            except:\n",
    "                pass\n",
    "            dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_example()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u8g90q3238l",
   "metadata": {},
   "source": [
    "## 8. 在 Jupyter 中执行分布式训练\n",
    "\n",
    "由于 Jupyter Notebook 不支持直接运行多进程代码，我们使用 `%%writefile` 魔法命令将训练代码导出为独立的 Python 文件，然后通过 `torchrun` 在 Jupyter 中启动分布式训练。\n",
    "\n",
    "上面的代码块已经将完整的训练代码保存为 `megatron_distributed_train.py`。现在可以直接在 Jupyter 中执行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9sbrm9fl64l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "启动 4 GPU 分布式训练...\n",
      "Master 地址: localhost\n",
      "Master 端口: 29501\n",
      "============================================================\n",
      "执行命令: torchrun --nproc_per_node=4 --master_addr=localhost --master_port=29501 Code02Megatron.py\n",
      "\n",
      "Rank: 0, World size: 4\n",
      "Rank 0: Successfully initialized with nccl backend\n",
      "Rank: 3, World size: 4\n",
      "Rank: 1, World size: 4\n",
      "Rank: 2, World size: 4\n",
      "Rank 3: Successfully initialized with nccl backend\n",
      "Rank 2: Successfully initialized with nccl backend\n",
      "Rank 1: Successfully initialized with nccl backend\n",
      "\n",
      "============================================================\n",
      "Megatron 张量并行验证\n",
      "============================================================\n",
      "GPU 数量: 4\n",
      "主机名: autodl-container-352c469ce5-0262aef0\n",
      "NCCL version 2.21.5+cuda12.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1107 23:05:28.294109999 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[rank3]:[W1107 23:05:29.461084968 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[rank1]:[W1107 23:05:29.475599499 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[rank2]:[W1107 23:05:29.487187507 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型配置:\n",
      "  - Vocab: 1024, Hidden: 512\n",
      "  - Layers: 8, Heads: 8\n",
      "  - 参数量: 5,011,712 (每 GPU 约 313,232)\n",
      "\n",
      "训练任务: 序列记忆\n",
      "  - 训练序列数: 100\n",
      "  - 序列长度: 32\n",
      "  - 词汇表大小: 1024\n",
      "\n",
      "开始训练...\n",
      "------------------------------------------------------------\n",
      "Epoch 1/5, Step   0/100, Loss: 7.6181, Avg: 7.6181\n",
      "Epoch 1/5, Step  20/100, Loss: 3.1313, Avg: 5.0970\n",
      "Epoch 1/5, Step  40/100, Loss: 0.2469, Avg: 3.0900\n",
      "Epoch 1/5, Step  60/100, Loss: 0.0193, Avg: 2.0979\n",
      "Epoch 1/5, Step  80/100, Loss: 0.0129, Avg: 1.5855\n",
      "============================================================\n",
      "Epoch 1 完成 - 平均 Loss: 1.2859\n",
      "============================================================\n",
      "\n",
      "Epoch 2/5, Step   0/100, Loss: 0.0081, Avg: 0.0081\n",
      "Epoch 2/5, Step  20/100, Loss: 0.0062, Avg: 0.0069\n",
      "Epoch 2/5, Step  40/100, Loss: 0.0045, Avg: 0.0060\n",
      "Epoch 2/5, Step  60/100, Loss: 0.0045, Avg: 0.0055\n",
      "Epoch 2/5, Step  80/100, Loss: 0.0035, Avg: 0.0051\n",
      "============================================================\n",
      "Epoch 2 完成 - 平均 Loss: 0.0047 (↓1.2812)\n",
      "============================================================\n",
      "\n",
      "Epoch 3/5, Step   0/100, Loss: 0.0032, Avg: 0.0032\n",
      "Epoch 3/5, Step  20/100, Loss: 0.0029, Avg: 0.0030\n",
      "Epoch 3/5, Step  40/100, Loss: 0.0025, Avg: 0.0028\n",
      "Epoch 3/5, Step  60/100, Loss: 0.0025, Avg: 0.0027\n",
      "Epoch 3/5, Step  80/100, Loss: 0.0021, Avg: 0.0026\n",
      "============================================================\n",
      "Epoch 3 完成 - 平均 Loss: 0.0025 (↓0.0022)\n",
      "============================================================\n",
      "\n",
      "Epoch 4/5, Step   0/100, Loss: 0.0020, Avg: 0.0020\n",
      "Epoch 4/5, Step  20/100, Loss: 0.0017, Avg: 0.0019\n",
      "Epoch 4/5, Step  40/100, Loss: 0.0018, Avg: 0.0018\n",
      "Epoch 4/5, Step  60/100, Loss: 0.0017, Avg: 0.0018\n",
      "Epoch 4/5, Step  80/100, Loss: 0.0014, Avg: 0.0017\n",
      "============================================================\n",
      "Epoch 4 完成 - 平均 Loss: 0.0017 (↓0.0008)\n",
      "============================================================\n",
      "\n",
      "Epoch 5/5, Step   0/100, Loss: 0.0014, Avg: 0.0014\n",
      "Epoch 5/5, Step  20/100, Loss: 0.0013, Avg: 0.0013\n",
      "Epoch 5/5, Step  40/100, Loss: 0.0013, Avg: 0.0013\n",
      "Epoch 5/5, Step  60/100, Loss: 0.0012, Avg: 0.0013\n",
      "Epoch 5/5, Step  80/100, Loss: 0.0010, Avg: 0.0012\n",
      "============================================================\n",
      "Epoch 5 完成 - 平均 Loss: 0.0012 (↓0.0005)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "✅ 训练完成!\n",
      "   最佳 Loss: 0.0012\n",
      "   最终 Loss: 0.0012\n",
      "   峰值显存: 180.30 MB (0.18 GB)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 配置 GPU 数量和端口\n",
    "NUM_GPUS = 4\n",
    "MASTER_PORT = 29501\n",
    "MASTER_ADDR = \"localhost\"\n",
    "\n",
    "print(f\"启动 {NUM_GPUS} GPU 分布式训练...\")\n",
    "print(f\"Master 地址: {MASTER_ADDR}\")\n",
    "print(f\"Master 端口: {MASTER_PORT}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 检查是否存在训练脚本\n",
    "if not os.path.exists('Code02Megatron.py'):\n",
    "    print(\"错误: Code02Megatron.py 不存在！请先运行上面的 %%writefile 代码块。\")\n",
    "else:\n",
    "    # 使用 os.system 执行 torchrun（确保在 shell 中执行）\n",
    "    cmd = f\"torchrun --nproc_per_node={NUM_GPUS} --master_addr={MASTER_ADDR} --master_port={MASTER_PORT} Code02Megatron.py\"\n",
    "    print(f\"执行命令: {cmd}\\n\")\n",
    "    exit_code = os.system(cmd)\n",
    "\n",
    "    if exit_code != 0:\n",
    "        print(f\"\\n 训练失败，退出码: {exit_code}\")\n",
    "\n",
    "\"\"\"\n",
    "运行训练结束后，自动删除临时脚本 Code02Megatron.py\n",
    "\"\"\"\n",
    "if os.path.exists('Code02Megatron.py'):\n",
    "    print(\"残留 Code02Megatron.py，自动删除。\")\n",
    "    os.remove('Code02Megatron.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe79b8",
   "metadata": {},
   "source": [
    "另外，单 GPU 情况下的训练输出为：\n",
    "\n",
    "```\n",
    "GPU 数量: 1\n",
    "\n",
    "模型配置:\n",
    "  - Vocab: 1024, Hidden: 512\n",
    "  - Layers: 8, Heads: 8\n",
    "  - 参数量: 18,397,184 (每 GPU 约 4,599,296)\n",
    "\n",
    "训练任务: 序列记忆\n",
    "  - 训练序列数: 100\n",
    "  - 序列长度: 32\n",
    "  - 词汇表大小: 1024\n",
    "\n",
    "开始训练...\n",
    "------------------------------------------------------------\n",
    "Epoch 1/5, Step   0/100, Loss: 7.2590, Avg: 7.2590\n",
    "Epoch 1/5, Step  20/100, Loss: 2.8188, Avg: 4.6222\n",
    "...\n",
    "\n",
    "============================================================\n",
    "Epoch 5 完成 - 平均 Loss: 0.0013 (↓0.0005)\n",
    "============================================================\n",
    "\n",
    "\n",
    "============================================================\n",
    "✅ 训练完成!\n",
    "   最佳 Loss: 0.0013\n",
    "   最终 Loss: 0.0013\n",
    "   峰值显存: 407.04 MB (0.40 GB)\n",
    "============================================================\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6985f62",
   "metadata": {},
   "source": [
    "双 GPU 情况下输出为：\n",
    "\n",
    "```\n",
    "GPU 数量: 2\n",
    "\n",
    "模型配置:\n",
    "  - Vocab: 1024, Hidden: 512\n",
    "  - Layers: 8, Heads: 8\n",
    "  - 参数量: 9,473,536 (每 GPU 约 1,184,192)\n",
    "\n",
    "训练任务: 序列记忆\n",
    "  - 训练序列数: 100\n",
    "  - 序列长度: 32\n",
    "  - 词汇表大小: 1024\n",
    "\n",
    "开始训练...\n",
    "------------------------------------------------------------\n",
    "Epoch 1/5, Step   0/100, Loss: 7.4521, Avg: 7.4521\n",
    "Epoch 1/5, Step  20/100, Loss: 3.0226, Avg: 4.8753\n",
    "...\n",
    "============================================================\n",
    "Epoch 5 完成 - 平均 Loss: 0.0012 (↓0.0005)\n",
    "============================================================\n",
    "============================================================\n",
    "✅ 训练完成!\n",
    "   最佳 Loss: 0.0012\n",
    "   最终 Loss: 0.0012\n",
    "   峰值显存: 255.88 MB (0.25 GB)\n",
    "============================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dfb76",
   "metadata": {},
   "source": [
    "四 GPU 情况下的示例输出：\n",
    "```\n",
    "GPU 数量: 4\n",
    "\n",
    "模型配置:\n",
    "  - Vocab: 1024, Hidden: 512\n",
    "  - Layers: 8, Heads: 8\n",
    "  - 参数量: 5,011,712 (每 GPU 约 313,232)\n",
    "\n",
    "训练任务: 序列记忆\n",
    "  - 训练序列数: 100\n",
    "  - 序列长度: 32\n",
    "  - 词汇表大小: 1024\n",
    "\n",
    "开始训练...\n",
    "------------------------------------------------------------\n",
    "Epoch 1/5, Step   0/100, Loss: 7.6181, Avg: 7.6181\n",
    "Epoch 1/5, Step  20/100, Loss: 3.1313, Avg: 5.0970\n",
    "...\n",
    "============================================================\n",
    "Epoch 5 完成 - 平均 Loss: 0.0012 (↓0.0005)\n",
    "============================================================\n",
    "\n",
    "\n",
    "============================================================\n",
    "✅ 训练完成!\n",
    "   最佳 Loss: 0.0012\n",
    "   最终 Loss: 0.0012\n",
    "   峰值显存: 180.30 MB (0.18 GB)\n",
    "============================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3effcb",
   "metadata": {},
   "source": [
    "TP 的核心优势是**降低单卡内存占用**。以下是不同情况下的内存占用情况：\n",
    "\n",
    "| 训练模式       | 内存占用（峰值） | 内存节省比例 |\n",
    "|----------------|----------------|--------------|\n",
    "| 无并行（单卡） | ~407.04MB           | -           |\n",
    "| 2 卡 TP       | ~255.88MB        | ~37.34%       |\n",
    "| 4 卡 TP       | ~180.30MB        | ~55.70%       |\n",
    "\n",
    "## 总结与思考\n",
    "\n",
    "本实验通过可执行的代码深入探讨了 Megatron 风格的 TP 原理与实现，并验证了 TP 在内存节省上的有效性。\n",
    "\n",
    "**核心技术点**：\n",
    "- **列并行线性层**：将权重矩阵按列分割，前向传播需要 All-Gather 操作\n",
    "- **行并行线性层**：将权重矩阵按行分割，前向传播使用 AllReduce 聚合（已优化 autograd）\n",
    "- **并行 Attention**：将注意力头分布到多个设备，每个设备处理部分头\n",
    "- **并行 Embedding**：将大型词汇表分割到多个设备，减少单个设备的内存压力\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

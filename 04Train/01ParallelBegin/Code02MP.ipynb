{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04fef865",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 02: PyTorch 实现模型并行\n",
    "\n",
    "> Author by: 杨涵智\n",
    "\n",
    "随着 AI 模型的规模不断扩大，单个 GPU 的内存容量已经无法容纳整个模型。以 GPT-3 为例，其 1750 亿参数如果使用 FP32 精度存储，就需要 700GB 的内存空间，这远远超过了单个 GPU 的容量限制。模型并行技术通过将模型分割到多个设备上，解决了大模型训练的内存瓶颈问题。\n",
    "\n",
    "模型并行的核心思想是将单个模型的不同部分分布到不同的计算设备上，每个设备只负责计算模型的一个子集。这种方法与数据并行形成鲜明对比——在数据并行中，每个设备都有完整的模型副本，但处理不同的数据批次，我们可以通过这个表格直观地对比二者的区别。\n",
    "\n",
    "|维度|模型并行|数据并行|\n",
    "|---------|---------|---------|\n",
    "|核心逻辑|将模型的不同层/模块分配到不同设备|每个设备保存完整模型，分配不同数据批次|\n",
    "|适用场景|适用于单个设备无法容纳模型的情况|适用于单个设备能够容纳完整模型但是数据量巨大的情况|\n",
    "\n",
    "## 1. 模型并行原理\n",
    "\n",
    "从数学角度看，模型并行可以表示为将模型函数 $f(x)$ 分解为多个子函数的组合：\n",
    "\n",
    "$$f(x) = fₙ(fₙ₋₁(...f₁(x)...))$$\n",
    "\n",
    "其中每个子函数 $fᵢ$ 可以放置在不同的设备上执行。前向传播时，数据从第一个设备流向最后一个设备；反向传播时，梯度则沿着相反的方向传播。\n",
    "\n",
    "### 1.1. 模型并行的主要分类\n",
    "\n",
    "模型并行根据模型分割的粒度，主要分为两大类：**流水线并行**和**张量并行**。\n",
    "\n",
    "流水线并行也称为**层间并行 (Inter-layer Parallelism)**，将模型的**不同层**分配到不同的计算设备上，适用于模型层数过多，整个模型无法装入单个 GPU 内存的情况。主要发生在层与层之间，传输的是**中间激活张量**。\n",
    "\n",
    "张量并行也称为**层内并行 (Intra-layer Parallelism)**，将模型**单个层内部**的权重矩阵（如线性层或注意力矩阵）进行分割，分配到不同的设备上，适用于单个模型层（特别是巨大的 Transformer 块）的参数或激活值过大，无法被单个 GPU 容纳的情况。通信发生在层内部，需要使用分布式原语（如 All-reduce 或 All-gather）来同步计算结果，对通信带宽要求极高。\n",
    "\n",
    "### 1.2. 模型并行中的通信成本\n",
    "\n",
    "模型并行的关键挑战在于设备间的通信效率。当模型被分割到多个设备上时，每个计算步骤完成后都需要将中间结果传输到下一个设备。这种通信开销可能成为性能瓶颈，特别是在使用 PCIe 等相对低速的连接时。\n",
    "\n",
    "通信量可以用以下公式估算：\n",
    "\n",
    "$$C = ∑(sᵢ × b) × 2$$\n",
    "\n",
    "其中 $sᵢ$ 是第 $i$ 层输出的尺寸，$b$ 是批次大小，系数 2 表示前向和反向传播都需要通信。\n",
    "\n",
    "## 2. 环境设置\n",
    "\n",
    "在开始编写代码前，我们需要确保环境正确配置。以下代码检查可用的 GPU 设备数量，这是模型并行的基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 检查可用 GPU 数量\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"可用 GPU 数量: {device_count}\")\n",
    "\n",
    "if device_count < 2:\n",
    "    print(\"警告: 需要至少 2 个 GPU 来进行模型并行实验\")\n",
    "    # 在 CPU 模式下模拟多设备环境（仅用于演示）\n",
    "    dev0 = torch.device(\"cpu\")\n",
    "    dev1 = torch.device(\"cpu\")\n",
    "else:\n",
    "    dev0 = torch.device(\"cuda:0\")\n",
    "    dev1 = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c0fef",
   "metadata": {},
   "source": [
    "在实际应用中，我们通常使用 NCCL 作为分布式训练的后端，它针对 NVIDIA GPU 进行了高度优化，能够提供高效的设备间通信。\n",
    "\n",
    "## 3. 具体实现\n",
    "\n",
    "**流水线并行 (PP)：** 将模型的不同层分配给不同设备，正如我们在 Section 1 中所讨论的，这种实现方式（代码实现见 3.1 节）主要依赖于设备间的**点对点通信**，相对容易实现。\n",
    "**张量并行 (TP)：** 需要将**单层内部**的权重矩阵分割到多个设备上，并在计算过程中使用复杂的**同步集体通信**（如 All-reduce）来汇集结果。这要求进行复杂的分布式环境初始化和自定义模块的编写，较难进行简单的实验进行复现。\n",
    "\n",
    "因此，本节代码将专注于**实现流水线并行**，以演示模型并行跨设备传输激活值的核心概念。\n",
    "\n",
    "### 3.1 流水线并行实现\n",
    "\n",
    "下面我们实现一个简单的流水线并行网络，将网络的不同部分放在不同的设备上。\n",
    "\n",
    "这一部分的逻辑如下图所示，流程从左侧的输入数据 x 开始。数据首先被移动到 dev0。在 dev0 内部，数据流经 part1。而后数据从 dev0 跨设备传输到 dev1。这里是通信开销，是模型并行的性能瓶颈。数据到达 dev1 后，流经 part2 进行计算。最后，在 dev1 上生成最终输出，并流出到右侧。\n",
    "\n",
    "![](./images/Code02MP01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dece1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParallelDemo(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        \"\"\"\n",
    "        初始化模型并行网络\n",
    "        \n",
    "        参数:\n",
    "            dev0: 第一个设备\n",
    "            dev1: 第二个设备\n",
    "        \"\"\"\n",
    "        super(ModelParallelDemo, self).__init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        \n",
    "        # 将网络的第一部分放在第一个设备上\n",
    "        # 这部分包含一个线性层和 ReLU 激活函数\n",
    "        self.part1 = nn.Sequential(\n",
    "            nn.Linear(10, 20),  # 输入维度 10，输出维度 20\n",
    "            nn.ReLU()           # ReLU 激活函数\n",
    "        ).to(dev0)              # 将这部分移动到第一个设备\n",
    "        \n",
    "        # 将网络的第二部分放在第二个设备上\n",
    "        # 这部分包含两个线性层和 ReLU 激活\n",
    "        self.part2 = nn.Sequential(\n",
    "            nn.Linear(20, 10),  # 输入维度 20，输出维度 10\n",
    "            nn.ReLU(),          # ReLU 激活函数\n",
    "            nn.Linear(10, 2)    # 输入维度 10，输出维度 2\n",
    "        ).to(dev1)              # 将这部分移动到第二个设备\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播过程，展示设备间数据传输\n",
    "        \n",
    "        参数:\n",
    "            x: 输入张量\n",
    "            \n",
    "        返回:\n",
    "            输出张量\n",
    "        \"\"\"\n",
    "        # 将输入数据移动到第一个设备\n",
    "        x = x.to(self.dev0)\n",
    "        \n",
    "        # 在第一个设备上执行第一部分计算\n",
    "        x = self.part1(x)\n",
    "        \n",
    "        # 将中间结果从第一个设备传输到第二个设备\n",
    "        # 这是模型并行的关键步骤，会产生通信开销\n",
    "        x = x.to(self.dev1)\n",
    "        \n",
    "        # 在第二个设备上执行第二部分计算\n",
    "        x = self.part2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d781d09",
   "metadata": {},
   "source": [
    "这个实现展示了模型并行的核心概念：**设备分配**和**数据传输**。在前向传播过程中，数据需要在设备间移动，这是模型并行的主要开销来源。\n",
    "\n",
    "### 3.2 张量并行\n",
    "\n",
    "张量并行需要将单个层的权重矩阵 $W$ 拆分到多个设备上，并使用分布式原语同步计算。它比流水线并行更复杂，需要依赖 `torch.distributed` 初始化和 NCCL 后端。感兴趣的读者可以自行研究，如何将 `nn.Linear` 替换为自定义的张量并行线性层，并在训练前设置 `torch.distributed` 环境以实现 TP 效果。\n",
    "\n",
    "## 4. 完整训练\n",
    "\n",
    "下面我们实现一个完整的流水线并行训练循环，并展示如何使用模型并行网络进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_parallel_experiment():\n",
    "    \"\"\"完整的模型并行训练实验\"\"\"\n",
    "    # 创建模型并行实例\n",
    "    model = ModelParallelDemo(dev0, dev1)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    # 注意：优化器需要处理分布在多个设备上的参数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(5):\n",
    "        # 生成模拟数据\n",
    "        # 在实际应用中，这里会从数据加载器读取真实数据\n",
    "        inputs = torch.randn(64, 10)  # 批次大小 64，输入维度 10\n",
    "\n",
    "        # 这一行做了修改\n",
    "        labels = torch.randint(0, 2, (64,)).to(dev1)\n",
    "        # labels = torch.randint(0, 2, (64,))  # 随机生成标签\n",
    "        \n",
    "        # 前向传播\n",
    "        # 模型并行会自动处理设备间数据传输\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        # PyTorch 会自动处理跨设备的梯度计算\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # 显示设备信息，帮助理解模型分布\n",
    "        print(f\"第一部分权重所在设备: {model.part1[0].weight.device}\")\n",
    "        print(f\"第二部分权重所在设备: {model.part2[0].weight.device}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# 运行实验\n",
    "model_parallel_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085042f9",
   "metadata": {},
   "source": [
    "![](./images/Code02MP02.png)\n",
    "\n",
    "这个图概括展示了反向传播过程。  dev1 首先基于损失函数 $\\mathcal{L}$ 计算梯度 $\\nabla \\mathcal{L}$，并对其所包含的参数（如 $\\mathbf{W}_2$）执行反向传播。  随后，计算得到的中间梯度 $\\nabla \\mathcal{L} / \\nabla \\mathbf{a}_1$ 通过跨设备通信传输至 dev0。  dev0 再利用该梯度继续计算其自身参数 $\\mathbf{W}_1$ 的梯度，从而完成整个反向传播流程。\n",
    "\n",
    "## 5. 性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "可用 GPU 数量: 2\n",
    "Epoch 0, Loss: 0.6932\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "Epoch 1, Loss: 0.6920\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "Epoch 2, Loss: 0.6908\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "Epoch 3, Loss: 0.6896\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "Epoch 4, Loss: 0.6885\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "\n",
    "输出：\n",
    "可用 GPU 数量: 2\n",
    "Epoch 0, Loss: 0.7403\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "Epoch 1, Loss: 0.7072\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "Epoch 2, Loss: 0.6836\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "Epoch 3, Loss: 0.7385\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------\n",
    "Epoch 4, Loss: 0.6836\n",
    "第一部分权重所在设备: cuda:0\n",
    "第二部分权重所在设备: cuda:1\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1254a8",
   "metadata": {},
   "source": [
    "模型并行的性能受到多个因素影响，其中最重要的是设备间通信的开销。我们可以通过以下公式估算理论加速比：\n",
    "\n",
    "$$S = 1 / (1 - α + α/n)$$\n",
    "\n",
    "其中 $α$ 是模型中可以并行化的部分比例，$n$ 是设备数量。这个公式基于 Amdahl 定律，揭示了即使增加大量设备，通信开销也会限制最终的性能提升。\n",
    "\n",
    "在实际应用中，我们需要权衡模型分割的策略。过于细粒度的分割会增加通信开销，而过于粗粒度的分割则可能无法充分利用多个设备。\n",
    "\n",
    "## 6. 总结与展望\n",
    "\n",
    "模型并行是训练超大神经网络的关键技术之一。通过将模型分布到多个设备上，我们能够突破单个设备的内存限制，训练之前无法实现的大型模型。\n",
    "\n",
    "然而，模型并行也引入了新的挑战，主要是设备间通信的开销。在实际应用中，需要仔细设计模型分割策略，平衡计算和通信的开销，才能获得最佳的并行效率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

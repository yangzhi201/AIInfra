{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d61ab66",
   "metadata": {},
   "source": [
    "Stabel Diffusion 从推理的角度理解可以分为三个过程，VAE（变分自编码器）、U-NET（解码器）、CLIP Text Encoder（文本编码器）。理解了这三个模块，就入门了 Stabel Siffusion,下面我们逐一看一下这三个模块。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ac86d",
   "metadata": {},
   "source": [
    "![sd 流程图](../images/05SD03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cd603",
   "metadata": {},
   "source": [
    "VAE 模型主要起到图像的压缩和图像重建的作用，比如输入一个 H x W x C 的数据，同时比率为 f, 你会得到一个 H*f、W*f 高度、宽度的图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8c436",
   "metadata": {},
   "source": [
    "![sd2](../images/04SD02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a49a6",
   "metadata": {},
   "source": [
    "下面是 SD VAE 结构的整体 block 框图,主要是 GSC、Downsample、Upsample 三块。\n",
    "![VAE block](../images/06SD04.jpg)\n",
    "下面我们用调库的方式对 VAE 有一个直观的认识，当然想要深入的理解还是要仔细看一下源码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b805b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "VAE = AutoencoderKL.from_pretrained(\"/本地路径/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "VAE.to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "# 用 OpenCV 读取和调整图像大小\n",
    "raw_image = cv2.imread(\"catwoman.png\")\n",
    "raw_image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)\n",
    "raw_image = cv2.resize(raw_image, (1024, 1024))\n",
    "\n",
    "# 将图像数据转换为浮点数并归一化\n",
    "image = raw_image.astype(np.float32) / 127.5 - 1.0\n",
    "\n",
    "# 调整数组维度以匹配 PyTorch 的格式 (N, C, H, W)\n",
    "image = image.transpose(2, 0, 1)\n",
    "image = image[None, :, :, :]\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "image = torch.from_numpy(image).to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "# 压缩图像为 Latent 特征并重建\n",
    "with torch.inference_mode():\n",
    "    # 使用 VAE 进行压缩和重建\n",
    "    latent = VAE.encode(image).latent_dist.sample()\n",
    "    rec_image = VAE.decode(latent).sample\n",
    "\n",
    "    # 后处理\n",
    "    rec_image = (rec_image / 2 + 0.5).clamp(0, 1)\n",
    "    rec_image = rec_image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "    # 反归一化\n",
    "    rec_image = (rec_image * 255).round().astype(\"uint8\")\n",
    "    rec_image = rec_image[0]\n",
    "\n",
    "    # 保存重建后图像\n",
    "    cv2.imwrite(\"reconstructed_catwoman.png\", cv2.cvtColor(rec_image, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9b042",
   "metadata": {},
   "source": [
    "U-Net 模块是 SD 模型的核心模块，能够预测噪声残差，并结合 Sampling method 对输入的特征矩阵进行重构，将随机高斯噪声转换成图片的 Latent Feature，其 block 示意图如下。具体解释参照正文。\n",
    "![Unet](../images/06SD05.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043863e",
   "metadata": {},
   "source": [
    "clip 的部分就比较常见，这里不再赘述，参考前文，演示代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb97b93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# 加载 CLIP Text Encoder 模型和 Tokenizer\n",
    "# SD V1.5 模型权重百度云网盘：关注 Rocky 的公众号 WeThinkIn，后台回复：SDV1.5 模型，即可获得资源链接\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"/本地路径/stable-diffusion-v1-5\", subfolder=\"text_encoder\").to(\"cuda\")\n",
    "text_tokenizer = CLIPTokenizer.from_pretrained(\"/本地路径/stable-diffusion-v1-5\", subfolder=\"tokenizer\")\n",
    "\n",
    "# 将输入 SD 模型的 prompt 进行 tokenize，得到对应的 token ids 特征\n",
    "prompt = \"1girl,beautiful\"\n",
    "text_token_ids = text_tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=text_tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "print(\"text_token_ids' shape:\",text_token_ids.shape)\n",
    "print(\"text_token_ids:\",text_token_ids)\n",
    "\n",
    "# 将 token ids 特征输入 CLIP Text Encoder 模型中输出 77x768 的 Text Embeddings 特征\n",
    "text_embeddings = text_encoder(text_token_ids.to(\"cuda\"))[0] # 由于 CLIP Text Encoder 模型输出的是一个元组，所以需要[0]对 77x768 的 Text Embeddings 特征进行提取\n",
    "print(\"text_embeddings' shape:\",text_embeddings.shape)\n",
    "print(text_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ee8c3",
   "metadata": {},
   "source": [
    "最终我们介绍一下 huggingface 的 diffusions 库。我们可以方便的调用现有的轮子，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c86b8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "  \"Qwen/Qwen-Image\", torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "cinematic film still of a cat sipping a margarita in a pool in Palm Springs, California\n",
    "highly detailed, high budget hollywood movie, cinemascope, moody, epic, gorgeous, film grain\n",
    "\"\"\"\n",
    "pipeline(prompt).images[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bd4710",
   "metadata": {},
   "source": [
    "我们可以自行选择自己喜欢的 VAE 组件来构成 pipeline。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc996ea3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, HeunDiscreteScheduler, AutoencoderKL\n",
    "import torch\n",
    "\n",
    "scheduler = HeunDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "  \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "  scheduler=scheduler,\n",
    "  vae=vae,\n",
    "  torch_dtype=torch.float16,\n",
    "  variant=\"fp16\",\n",
    "  use_safetensors=True\n",
    ").to(\"cuda\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

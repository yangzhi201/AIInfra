{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555e442a5b4886be",
   "metadata": {},
   "source": [
    "# 三种注意力机制详解\n",
    "\n",
    "本文档详细介绍三种重要的注意力机制及其CUDA实现：\n",
    "1. **Multi-Head Self-Attention** (多头自注意力)：标准全局注意力，能够捕捉长程依赖与多子空间特征；典型复杂度为时间/空间 O(N^2)，CUDA 实现关注 QK^T 的并行化、数值稳定 softmax、以及 V 的加权聚合（常配合共享内存/分块提升带宽利用）。\n",
    "2. **Linear Attention** (线性注意力)：通过核方法/特征映射重排计算，将显式 N×N 相关性转化为可累积的 K,V 统计，复杂度降为 O(N·d^2) 或 O(N·d)（依实现而定）；适合超长序列与流式场景，CUDA 核心在 φ 映射的逐元素并行、K^T V 预积累与行级归一的数值稳定处理。\n",
    "3. **Sliding Window Self-Attention** (滑动窗口自注意力)：限制注意范围到长度 w 的局部窗口，复杂度 O(N·w·d)，在保持局部上下文的同时显著降低显存与计算；CUDA 实现侧重窗口裁剪的高效访存、寄存器/共享内存缓存以及边界位置的分支消隐。\n",
    "\n",
    "## 1. Multi-Head Self-Attention\n",
    "\n",
    "### 原理介绍\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./images/multihead_attention.svg\" alt=\"img\" />\n",
    "</div>\n",
    "  \n",
    "\n",
    "多头注意力是Transformer架构的核心组件，其基本思想是**将注意力计算并行化到多个\"头\"（heads）**，每个头学习不同的表示子空间。\n",
    "\n",
    "\n",
    "给定输入矩阵 $Q$（查询）、$K$（键）、$V$（值），大小为 $N \\times d_{\\text{model}}$，多头注意力的计算过程如下：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\ldots,\\text{head}_h)\n",
    "$$\n",
    "\n",
    "其中每个头计算：\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{softmax}\\left(\\frac{Q_iK_i^T}{\\sqrt{d_k}}\\right)V_i\n",
    "$$\n",
    "\n",
    "这里：\n",
    "- $h$ = 注意力头的数量\n",
    "- $d_k = d_{\\text{model}}/h$ = 每个头的维度\n",
    "- $Q_i, K_i, V_i$ 是第 $i$ 个头对应的输入矩阵分区\n",
    "\n",
    "#### 算法步骤\n",
    "\n",
    "1. **分区（Partition）**：将 $Q$, $K$, $V$ 沿特征维度分成 $h$ 个部分\n",
    "2. **注意力计算**：对每个头独立计算：\n",
    "   - 计算注意力分数：$\\text{scores} = Q_iK_i^T / \\sqrt{d_k}$\n",
    "   - 应用softmax归一化\n",
    "   - 加权求和：$\\text{output}_i = \\text{attention} \\cdot V_i$\n",
    "3. **拼接（Concatenate）**：将所有头的输出拼接起来\n",
    "\n",
    "###  CUDA实现详解\n",
    "\n",
    "#### 核心Kernel函数\n",
    "\n",
    "##### Kernel 1: 计算注意力分数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b29596b390ad2",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "__global__ void computeAttentionScores(\n",
    "    const float* Q, const float* K, float* scores,\n",
    "    int N, int d_model, int h, int d_k, int head_idx\n",
    ") {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < N && col < N) {\n",
    "        float sum = 0.0f;\n",
    "        int head_offset = head_idx * d_k;\n",
    "        \n",
    "        // 计算Q[row]和K[col]的点积\n",
    "        for (int i = 0; i < d_k; i++) {\n",
    "            float q_val = Q[row * d_model + head_offset + i];\n",
    "            float k_val = K[col * d_model + head_offset + i];\n",
    "            sum += q_val * k_val;\n",
    "        }\n",
    "        \n",
    "        // 缩放因子 1/sqrt(d_k)\n",
    "        sum /= sqrtf((float)d_k);\n",
    "        \n",
    "        // 存储到scores矩阵\n",
    "        scores[head_idx * N * N + row * N + col] = sum;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a688ba",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- 使用2D线程网格处理 $N \\times N$ 的注意力分数矩阵\n",
    "- `head_offset` 定位当前头在输入矩阵中的起始位置\n",
    "- 缩放因子 $1/\\sqrt{d_k}$ 防止点积过大导致softmax梯度消失\n",
    "\n",
    "##### Kernel 2: Softmax归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00906e1f",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "__global__ void applySoftmax(float* scores, int N, int num_heads) {\n",
    "    int head = blockIdx.y;\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (head < num_heads && row < N) {\n",
    "        float* row_scores = scores + head * N * N + row * N;\n",
    "        \n",
    "        // 数值稳定性：找到最大值\n",
    "        float max_val = -FLT_MAX;\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            max_val = fmaxf(max_val, row_scores[i]);\n",
    "        }\n",
    "        \n",
    "        // 计算exp和sum\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            row_scores[i] = expf(row_scores[i] - max_val);\n",
    "            sum += row_scores[i];\n",
    "        }\n",
    "        \n",
    "        // 归一化\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            row_scores[i] /= sum;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89ca30",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- **数值稳定性**：减去最大值再计算exp，避免溢出\n",
    "- 每个线程处理一行，独立计算softmax\n",
    "- 三遍扫描：找最大值 → 计算exp和sum → 归一化\n",
    "\n",
    "##### Kernel 3: 应用注意力权重到V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f04eaa",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "__global__ void applyAttention(\n",
    "    const float* attention, const float* V, float* output,\n",
    "    int N, int d_model, int h, int d_k, int head_idx\n",
    ") {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < N && col < d_k) {\n",
    "        float sum = 0.0f;\n",
    "        // 指向当前head的第 row 行注意力分布（长度 N）\n",
    "        float* att_row = (float*)attention + head_idx * N * N + row * N;\n",
    "        // V 矩阵中当前 head 的列偏移（把多头拼在 d_model 维中）\n",
    "        int v_col = head_idx * d_k + col;\n",
    "        \n",
    "        // 对所有键位置 j 做加权求和：sum_j att[i,j] * V[j, head_col]\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            sum += att_row[i] * V[i * d_model + v_col];\n",
    "        }\n",
    "        // 写回到输出：直接写入已拼接的模型维度 (d_model) 中对应 head 列\n",
    "        output[row * d_model + v_col] = sum;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ccdac7",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "//按head聚合 + 拼接(concat)”合并到同一kernel中，避免中间临时缓冲与多次kernel launch\n",
    "__global__ void applyAttentionAndConcat(\n",
    "    const float* attention, const float* V, float* output,\n",
    "    int N, int d_model, int h, int d_k\n",
    ") {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int head = blockIdx.x;\n",
    "    int col_in_head = threadIdx.x;\n",
    "    \n",
    "    if (row < N && head < h && col_in_head < d_k) {\n",
    "        float sum = 0.0f;\n",
    "        float* att_row = (float*)attention + head * N * N + row * N;\n",
    "        \n",
    "        // 遍历键位置 j，累积注意力加权的 V 值（同一 head 的第 col_in_head 列）\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            float v_val = V[i * d_model + head * d_k + col_in_head];\n",
    "            sum += att_row[i] * v_val;\n",
    "        }\n",
    "        \n",
    "        // 将每个 head 的输出直接按拼接形式写到输出张量对应位置\n",
    "        output[row * d_model + head * d_k + col_in_head] = sum;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ecf8eb",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- 计算注意力加权的值向量和：$\\text{output}[i] = \\sum_j \\text{attention}[i][j] \\cdot V[j]$\n",
    "- 直接写入最终输出矩阵的对应位置（已拼接）\n",
    "\n",
    "#### 主函数流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4aa08c",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "extern \"C\" void solve(const float* Q, const float* K, const float* V, \n",
    "                     float* output, int N, int d_model, int h) {\n",
    "    int d_k = d_model / h;\n",
    "    \n",
    "    // 分配中间结果内存\n",
    "    float* attention_scores;\n",
    "    cudaMalloc(&attention_scores, sizeof(float) * h * N * N);\n",
    "    \n",
    "    // 步骤1: 计算所有头的注意力分数\n",
    "    for (int head = 0; head < h; head++) {\n",
    "        computeAttentionScores<<<gridDim, blockDim>>>(\n",
    "            Q, K, attention_scores, N, d_model, h, d_k, head\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    // 步骤2: 应用softmax\n",
    "    applySoftmax<<<softmaxGrid, 256>>>(attention_scores, N, h);\n",
    "    \n",
    "    // 步骤3: 应用注意力到V并拼接\n",
    "    if (d_k <= 32) {\n",
    "        // Use single kernel for small d_k\n",
    "        dim3 blockDim3(d_k, 16);\n",
    "        dim3 gridDim3(h, (N + blockDim3.y - 1) / blockDim3.y);\n",
    "        applyAttentionAndConcat<<<gridDim3, blockDim3>>>(\n",
    "            attention_scores, V, output, N, d_model, h, d_k\n",
    "        );\n",
    "    } else {\n",
    "        // Use separate kernels for larger d_k\n",
    "        dim3 blockDim2(16, 16);\n",
    "        dim3 gridDim2((d_k + blockDim2.x - 1) / blockDim2.x,\n",
    "                      (N + blockDim2.y - 1) / blockDim2.y);\n",
    "        \n",
    "        for (int head = 0; head < h; head++) {\n",
    "            applyAttention<<<gridDim2, blockDim2>>>(\n",
    "                attention_scores, V, output, N, d_model, h, d_k, head\n",
    "            );\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Synchronize and clean up\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaFree(attention_scores);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042d618",
   "metadata": {},
   "source": [
    "### 复杂度分析\n",
    "\n",
    "- **时间复杂度**：$O(N^2 \\cdot d_{\\text{model}})$\n",
    "- **空间复杂度**：$O(h \\cdot N^2)$（存储注意力矩阵）\n",
    "- **计算瓶颈**：$N^2$ 的注意力分数计算，对长序列不友好\n",
    "\n",
    "### 示例验证\n",
    "\n",
    "**输入**：\n",
    "- $N = 2, d_{\\text{model}} = 4, h = 2$\n",
    "- $Q = \\begin{bmatrix} 1.0 & 0.0 & 2.0 & 3.0 \\\\ 4.0 & 5.0 & 6.0 & 7.0 \\end{bmatrix}$\n",
    "- $K = \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix}$\n",
    "- $V = \\begin{bmatrix} 0.5 & 1.0 & 1.5 & 2.0 \\\\ 2.5 & 3.0 & 3.5 & 4.0 \\end{bmatrix}$\n",
    "\n",
    "**输出**：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2.39 & 2.89 & 3.50 & 4.00 \\\\\n",
    "2.50 & 3.00 & 3.50 & 4.00\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "这个示例展示了**注意力权重是如何通过$Q·K^T$计算相似度的**，**Softmax**是如何通过分数转为概率分布来实现归一化的，**加权聚合**是如何根据注意力权重提取相关信息V的，**多头并行**是通过不同头学习来不同特征子空间的，在实际应用中，这种机制让模型能够**动态地关注输入序列中的不同部分**，是 Transformer 强大表示能力的核心！\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Linear Attention\n",
    "\n",
    "###  原理介绍\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./images/linear_attention_experiment.jpg\" alt=\"img\" />\n",
    "</div>\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "线性注意力通过**改变计算顺序**来降低复杂度，避免显式计算 $N \\times N$ 的注意力矩阵。核心思想是使用特征映射函数 $\\phi$ 将注意力从softmax形式转换为核函数形式。\n",
    "\n",
    "#### 数学公式\n",
    "\n",
    "标准注意力：\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(QK^T)V\n",
    "$$\n",
    "\n",
    "线性注意力（来自论文 [Transformers are RNNs](https://arxiv.org/pdf/2006.16236)）：\n",
    "$$\n",
    "\\text{LinearAttention}(Q, K, V) = \\frac{\\phi(Q) \\left(\\phi(K)^T V \\right)}{\\phi(Q) \\left(\\sum_j \\phi(K_j) \\right)}\n",
    "$$\n",
    "\n",
    "其中特征映射 $\\phi(x)$ 定义为：\n",
    "$$\n",
    "\\phi(x) = \\text{ELU}(x) + 1 = \n",
    "\\begin{cases} \n",
    "x + 1, & x > 0 \\\\ \n",
    "e^x, & x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### 算法步骤\n",
    "\n",
    "1. **应用特征映射**：计算 $\\phi(Q)$ 和 $\\phi(K)$\n",
    "2. **计算键值乘积**：$\\text{KTV} = \\phi(K)^T V$（$d \\times d$ 矩阵）\n",
    "3. **计算键的和**：$\\text{sumK} = \\sum_{j=1}^M \\phi(K_j)$（$d$ 维向量）\n",
    "4. **计算最终输出**：\n",
    "   - 分子：$\\phi(Q) \\cdot \\text{KTV}$\n",
    "   - 分母：$\\phi(Q) \\cdot \\text{sumK}$\n",
    "\n",
    "### CUDA实现详解\n",
    "\n",
    "#### 核心Kernel函数\n",
    "\n",
    "##### Kernel 1: 特征映射\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed59e8",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Feature map: φ(x) = ELU(x) + 1 = max(0, x) + exp(min(0, x))\n",
    "__device__ inline float phi(float x) {\n",
    "    if (x > 0.0f) {\n",
    "        return x + 1.0f;\n",
    "    } else {\n",
    "        return expf(x);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Kernel to apply feature map φ to a matrix\n",
    "__global__ void applyFeatureMap(const float* input, float* output, int M, int d) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = M * d;\n",
    "    \n",
    "    if (idx < total) {\n",
    "        output[idx] = phi(input[idx]);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6446507",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- ELU激活函数确保输出非负（类似softmax的性质）\n",
    "- 逐元素并行计算，充分利用GPU\n",
    "\n",
    "##### Kernel 2: 计算 $\\phi(K)^T V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffced1d",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Kernel to compute φ(K)^T @ V (result is d×d)\n",
    "__global__ void computeKTV(const float* phiK, const float* V, float* KTV, int M, int d) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < d && col < d) {\n",
    "        float sum = 0.0f;\n",
    "        \n",
    "        // K^T[row, :] @ V[:, col]\n",
    "        for (int i = 0; i < M; i++) {\n",
    "            sum += phiK[i * d + row] * V[i * d + col];\n",
    "        }\n",
    "        \n",
    "        KTV[row * d + col] = sum;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f5fc0",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- 输出是 $d \\times d$ 矩阵，通常 $d \\ll M$\n",
    "- 这是线性注意力的关键：将复杂度从 $O(M^2)$ 降到 $O(M \\cdot d^2)$\n",
    "\n",
    "##### Kernel 3: 计算键的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8c9ec",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Kernel to compute sum of φ(K) rows (result is d-dimensional)\n",
    "__global__ void computeSumPhiK(const float* phiK, float* sumK, int M, int d) {\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (col < d) {\n",
    "        double sum = 0.0;  // 使用double提高精度\n",
    "        for (int i = 0; i < M; i++) {\n",
    "            sum += (double)phiK[i * d + col];\n",
    "        }\n",
    "        sumK[col] = (float)sum;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edcb67",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- 对每个特征维度求和\n",
    "- 使用double精度避免累积误差\n",
    "\n",
    "##### Kernel 4: 计算最终输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549053b",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Main kernel to compute final output\n",
    "__global__ void computeLinearAttentionMain(\n",
    "    const float* phiQ, const float* KTV, const float* sumK, \n",
    "    float* output, int M, int d\n",
    ") {\n",
    "    // Grid-stride loop pattern for better efficiency\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    int total = M * d;\n",
    "    \n",
    "    for (int pos = idx; pos < total; pos += stride) {\n",
    "        int row = pos / d;\n",
    "        int col = pos % d;\n",
    "        \n",
    "        if (row < M && col < d) {\n",
    "            double numerator = 0.0;\n",
    "            double denominator = 0.0;\n",
    "            \n",
    "            // Compute numerator: φ(Q_row) @ KTV[:, col]\n",
    "            for (int k = 0; k < d; k++) {\n",
    "                numerator += (double)phiQ[row * d + k] * (double)KTV[k * d + col];\n",
    "            }\n",
    "            \n",
    "            // Compute denominator: φ(Q_row) @ sumK\n",
    "            for (int k = 0; k < d; k++) {\n",
    "                denominator += (double)phiQ[row * d + k] * (double)sumK[k];\n",
    "            }\n",
    "            \n",
    "            // Avoid division by zero\n",
    "            if (denominator != 0.0) {\n",
    "                output[row * d + col] = (float)(numerator / denominator);\n",
    "            } else {\n",
    "                output[row * d + col] = 0.0f;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed5840",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "// Optimized version with better memory access pattern\n",
    "__global__ void computeLinearAttentionOpt(\n",
    "    const float* phiQ, const float* KTV, const float* sumK, \n",
    "    float* output, int M, int d\n",
    ") {\n",
    "    extern __shared__ float shared_mem[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int row = blockIdx.x;\n",
    "    \n",
    "    if (row >= M) return;\n",
    "    \n",
    "    // 加载sumK到共享内存\n",
    "    for (int i = tid; i < d; i += blockDim.x) {\n",
    "        shared_mem[i] = sumK[i];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // 计算分母（该行只需计算一次）\n",
    "    double denominator = 0.0;\n",
    "    for (int k = 0; k < d; k++) {\n",
    "        denominator += (double)phiQ[row * d + k] * (double)shared_mem[k];\n",
    "    }\n",
    "    \n",
    "    // 每个线程计算多个输出元素\n",
    "    for (int col = tid; col < d; col += blockDim.x) {\n",
    "        double numerator = 0.0;\n",
    "        \n",
    "        // φ(Q_row) @ KTV[:, col]\n",
    "        for (int k = 0; k < d; k++) {\n",
    "            numerator += (double)phiQ[row * d + k] * (double)KTV[k * d + col];\n",
    "        }\n",
    "        \n",
    "        if (denominator != 0.0) {\n",
    "            output[row * d + col] = (float)(numerator / denominator);\n",
    "        } else {\n",
    "            output[row * d + col] = 0.0f;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe6c253",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- 使用共享内存缓存 `sumK`，减少全局内存访问\n",
    "- 分母对每行只计算一次，提高效率\n",
    "- 使用double精度进行中间计算，提高数值稳定性\n",
    "\n",
    "#### 主函数流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774b97f",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "extern \"C\" void solve(const float* Q, const float* K, const float* V, \n",
    "                     float* output, int M, int d) {\n",
    "    float *phiQ, *phiK, *KTV, *sumK;\n",
    "    \n",
    "    // 分配内存\n",
    "    cudaMalloc(&phiQ, M * d * sizeof(float));\n",
    "    cudaMalloc(&phiK, M * d * sizeof(float));\n",
    "    cudaMalloc(&KTV, d * d * sizeof(float));\n",
    "    cudaMalloc(&sumK, d * sizeof(float));\n",
    "    \n",
    "    // 步骤1: 应用特征映射\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocks = (M * d + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    applyFeatureMap<<<blocks, threads>>>(Q, phiQ, M, d);\n",
    "    applyFeatureMap<<<blocks, threads>>>(K, phiK, M, d);\n",
    "    \n",
    "    // 步骤2: 计算 φ(K)^T @ V\n",
    "    dim3 blockDim2D(16, 16);\n",
    "    dim3 gridDim2D((d + blockDim2D.x - 1) / blockDim2D.x,\n",
    "                   (d + blockDim2D.y - 1) / blockDim2D.y);\n",
    "    computeKTV<<<gridDim2D, blockDim2D>>>(phiK, V, KTV, M, d);\n",
    "    \n",
    "    // 步骤3: 计算 sum(φ(K))\n",
    "    int sumBlocks = (d + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    computeSumPhiK<<<sumBlocks, threads>>>(phiK, sumK, M, d);\n",
    "    \n",
    "    // 步骤4: 等待之前的内核完成\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // 步骤5: 计算最终输出\n",
    "    if (M <= 1000) {\n",
    "        // For smaller M, use one block per row\n",
    "        int threads = min(256, d);\n",
    "        size_t shared_size = d * sizeof(float);\n",
    "        computeLinearAttentionOpt<<<M, threads, shared_size>>>(\n",
    "            phiQ, KTV, sumK, output, M, d\n",
    "        );\n",
    "    } else {\n",
    "        // For larger M, use grid-stride approach\n",
    "        int totalElements = M * d;\n",
    "        int threads = 256;\n",
    "        int blocks = min(65535, (totalElements + threads - 1) / threads);\n",
    "        computeLinearAttentionMain<<<blocks, threads>>>(\n",
    "            phiQ, KTV, sumK, output, M, d\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    // 同步并清理\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaFree(phiQ);\n",
    "    cudaFree(phiK);\n",
    "    cudaFree(KTV);\n",
    "    cudaFree(sumK);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f459b1",
   "metadata": {},
   "source": [
    "### 复杂度分析\n",
    "\n",
    "- **时间复杂度**：$O(M \\cdot d^2)$（vs 标准注意力的 $O(M^2 \\cdot d)$）\n",
    "- **空间复杂度**：$O(M \\cdot d + d^2)$（无需存储 $M \\times M$ 注意力矩阵）\n",
    "- **优势**：当 $M \\gg d$ 时（长序列），复杂度降低显著\n",
    "\n",
    "### 示例验证\n",
    "\n",
    "**输入**：\n",
    "- $M = 2, d = 4$\n",
    "- $Q = \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix}$\n",
    "- $K = \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix}$\n",
    "- $V = \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix}$\n",
    "\n",
    "**输出**：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2.8461537 & 3.8461537 & 4.8461537 & 5.8461537 \\\\\n",
    "3.1538463 & 4.1538463 & 5.1538463 & 6.1538463\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "上述示例显示，标准 Attention 的 Softmax 破坏了矩阵乘法的结合律：\n",
    "$$\\text{softmax}(QK^T)V \\neq Q[\\text{某函数}(K^TV)]$$\n",
    "\n",
    "Linear Attention 通过以下方式恢复结合律：\n",
    "\n",
    "1.用简单的归一化替代Softmax\n",
    "\n",
    "2.利用 $\\phi(Q)[\\phi(K)^T V]$来实现矩阵重排计算。\n",
    "\n",
    "Linear Attention 的成功证明**算法创新**可以突破硬件瓶颈（改变计算顺序而非硬件升级）; **复杂度优化**需要在表达能力和效率间权衡\n",
    "; **Transformer 的未来**可能在于混合架构（短距离用标准 Attention，长距离用 Linear Attention）。这正是 **Mamba、RWKV、RetNet** 等新架构的核心思想来源！\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Sliding Window Self-Attention\n",
    "\n",
    "###  原理介绍\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./images/longformer.jpg\" alt=\"img\" />\n",
    "</div>\n",
    "  \n",
    "\n",
    "\n",
    "滑动窗口注意力通过**限制每个位置只关注局部窗口内的其他位置**，在保持局部感受野的同时大幅降低计算复杂度。这在处理长序列（如文档、长文本）时特别有用。\n",
    "\n",
    "#### 数学公式\n",
    "\n",
    "标准注意力：\n",
    "$$\n",
    "\\text{output}_i = \\sum_{j=1}^{M} \\text{softmax}(\\text{score}_{i,*})_j \\cdot V_j\n",
    "$$\n",
    "\n",
    "滑动窗口注意力：\n",
    "$$\n",
    "\\text{output}_i = \\sum_{j \\in [i-w, i+w]} \\text{softmax}(\\text{score}_{i,*})_j \\cdot V_j\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $w$ = `window_size`\n",
    "- 窗口范围：$[\\max(0, i-w), \\min(M-1, i+w)]$\n",
    "- 注意力分数：$\\text{score}_{i,j} = \\frac{Q_i \\cdot K_j}{\\sqrt{d}}$\n",
    "\n",
    "#### 算法步骤\n",
    "\n",
    "1. **确定窗口边界**：对位置 $i$，计算 $[\\text{start}, \\text{end}]$\n",
    "2. **计算局部分数**：只计算窗口内的注意力分数\n",
    "3. **应用softmax**：在窗口内归一化\n",
    "4. **加权求和**：只对窗口内的值向量加权\n",
    "\n",
    "###  CUDA实现详解\n",
    "\n",
    "#### 核心Kernel函数\n",
    "\n",
    "##### Kernel: 滑动窗口注意力（通用版本）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca085b",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "__global__ void slidingWindowAttention(\n",
    "    const float* Q, const float* K, const float* V, float* output,\n",
    "    int M, int d, int window_size\n",
    ") {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i >= M) return;\n",
    "    \n",
    "    // 确定窗口边界\n",
    "    int window_start = max(0, i - window_size);\n",
    "    int window_end = min(M - 1, i + window_size);\n",
    "    int window_len = window_end - window_start + 1;\n",
    "    \n",
    "    // 本地数组存储分数（最大窗口：2*32+1=65）\n",
    "    float scores[65];\n",
    "    \n",
    "    // 步骤1: 计算窗口内的注意力分数\n",
    "    float sqrt_d = sqrtf((float)d);\n",
    "    float max_score = -FLT_MAX;\n",
    "    \n",
    "    for (int j_idx = 0; j_idx < window_len; j_idx++) {\n",
    "        int j = window_start + j_idx;\n",
    "        float score = 0.0f;\n",
    "        \n",
    "        // 计算 Q_i · K_j\n",
    "        for (int k = 0; k < d; k++) {\n",
    "            score += Q[i * d + k] * K[j * d + k];\n",
    "        }\n",
    "        \n",
    "        score /= sqrt_d;\n",
    "        scores[j_idx] = score;\n",
    "        max_score = fmaxf(max_score, score);\n",
    "    }\n",
    "    \n",
    "    // 步骤2: 应用softmax（数值稳定版本）\n",
    "    float sum_exp = 0.0f;\n",
    "    for (int j_idx = 0; j_idx < window_len; j_idx++) {\n",
    "        float exp_val = expf(scores[j_idx] - max_score);\n",
    "        scores[j_idx] = exp_val;\n",
    "        sum_exp += exp_val;\n",
    "    }\n",
    "    \n",
    "    // 归一化\n",
    "    for (int j_idx = 0; j_idx < window_len; j_idx++) {\n",
    "        scores[j_idx] /= sum_exp;\n",
    "    }\n",
    "    \n",
    "    // 步骤3: 计算加权值向量和\n",
    "    for (int k = 0; k < d; k++) {\n",
    "        float result = 0.0f;\n",
    "        for (int j_idx = 0; j_idx < window_len; j_idx++) {\n",
    "            int j = window_start + j_idx;\n",
    "            result += scores[j_idx] * V[j * d + k];\n",
    "        }\n",
    "        output[i * d + k] = result;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baeaa94",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- 每个线程独立处理一个查询位置\n",
    "- 使用**寄存器数组**（`float scores[65]`）存储窗口内的注意力权重，避免共享内存冲突\n",
    "- 动态窗口边界处理边界情况\n",
    "- 三步流程紧凑，减少内存往返\n",
    "\n",
    "##### Kernel: 优化版本（使用共享内存）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c5540",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "__device__ float atomicMaxFloat(float* addr, float value) {\n",
    "    int* addr_as_int = (int*)addr;\n",
    "    int old = *addr_as_int;\n",
    "    int assumed;\n",
    "    \n",
    "    do {\n",
    "        assumed = old;\n",
    "        old = atomicCAS(addr_as_int, assumed,\n",
    "                        __float_as_int(fmaxf(__int_as_float(assumed), value)));\n",
    "    } while (assumed != old);\n",
    "    \n",
    "    return __int_as_float(old);\n",
    "}\n",
    "\n",
    "\n",
    "__global__ void slidingWindowAttentionShared(\n",
    "    const float* Q, const float* K, const float* V, float* output,\n",
    "    int M, int d, int window_size\n",
    ") {\n",
    "    extern __shared__ float shared_mem[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int i = blockIdx.x;  // 每个block处理一个查询\n",
    "    \n",
    "    if (i >= M) return;\n",
    "    \n",
    "    float* scores = shared_mem;\n",
    "    float* query = shared_mem + 65;\n",
    "    \n",
    "    // 加载查询向量到共享内存\n",
    "    for (int k = tid; k < d; k += blockDim.x) {\n",
    "        query[k] = Q[i * d + k];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // 确定窗口边界\n",
    "    int window_start = max(0, i - window_size);\n",
    "    int window_end = min(M - 1, i + window_size);\n",
    "    int window_len = window_end - window_start + 1;\n",
    "    \n",
    "    // 计算注意力分数（并行）\n",
    "    float sqrt_d = sqrtf((float)d);\n",
    "    float max_score = -FLT_MAX;\n",
    "    \n",
    "    for (int j_idx = tid; j_idx < window_len; j_idx += blockDim.x) {\n",
    "        int j = window_start + j_idx;\n",
    "        float score = 0.0f;\n",
    "        \n",
    "        for (int k = 0; k < d; k++) {\n",
    "            score += query[k] * K[j * d + k];\n",
    "        }\n",
    "        \n",
    "        score /= sqrt_d;\n",
    "        scores[j_idx] = score;\n",
    "        max_score = fmaxf(max_score, score);\n",
    "    }\n",
    "    \n",
    "    // 找全局最大值（使用原子操作）\n",
    "    __shared__ float max_score_shared;\n",
    "    if (tid == 0) max_score_shared = -FLT_MAX;\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (max_score > -FLT_MAX) {\n",
    "        atomicMaxFloat(&max_score_shared, max_score);\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // 应用softmax（并行）\n",
    "    float local_sum = 0.0f;\n",
    "    for (int j_idx = tid; j_idx < window_len; j_idx += blockDim.x) {\n",
    "        float exp_val = expf(scores[j_idx] - max_score_shared);\n",
    "        scores[j_idx] = exp_val;\n",
    "        local_sum += exp_val;\n",
    "    }\n",
    "    \n",
    "    __shared__ float sum_exp_shared;\n",
    "    if (tid == 0) sum_exp_shared = 0.0f;\n",
    "    __syncthreads();\n",
    "    \n",
    "    atomicAdd(&sum_exp_shared, local_sum);\n",
    "    __syncthreads();\n",
    "    \n",
    "    // 归一化\n",
    "    for (int j_idx = tid; j_idx < window_len; j_idx += blockDim.x) {\n",
    "        scores[j_idx] /= sum_exp_shared;\n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    // 计算输出（并行）\n",
    "    for (int k = tid; k < d; k += blockDim.x) {\n",
    "        float result = 0.0f;\n",
    "        for (int j_idx = 0; j_idx < window_len; j_idx++) {\n",
    "            int j = window_start + j_idx;\n",
    "            result += scores[j_idx] * V[j * d + k];\n",
    "        }\n",
    "        output[i * d + k] = result;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09bdecc",
   "metadata": {},
   "source": [
    "**关键点**：\n",
    "- **共享内存优化**：缓存查询向量和注意力权重\n",
    "- **Block策略**：每个block处理一个查询位置，block内线程并行计算窗口内的操作\n",
    "- **原子操作**：`atomicMaxFloat` 和 `atomicAdd` 用于规约操作\n",
    "- 适用于较小的 $d$ 和 $w$\n",
    "\n",
    "#### 主函数流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49a02b",
   "metadata": {
    "vscode": {
     "languageId": "cuda-cpp"
    }
   },
   "outputs": [],
   "source": [
    "extern \"C\" void solve(const float* Q, const float* K, const float* V, \n",
    "                     float* output, int M, int d, int window_size) {\n",
    "    if (d <= 64 && window_size <= 32) {\n",
    "        // 使用共享内存优化版本\n",
    "        int threadsPerBlock = 128;\n",
    "        int blocks = M;\n",
    "        size_t shared_mem_size = (65 + d) * sizeof(float);\n",
    "        \n",
    "        slidingWindowAttentionShared<<<blocks, threadsPerBlock, shared_mem_size>>>(\n",
    "            Q, K, V, output, M, d, window_size\n",
    "        );\n",
    "    } else {\n",
    "        // 使用通用版本\n",
    "        int threadsPerBlock = 256;\n",
    "        int blocks = (M + threadsPerBlock - 1) / threadsPerBlock;\n",
    "        \n",
    "        slidingWindowAttention<<<blocks, threadsPerBlock>>>(\n",
    "            Q, K, V, output, M, d, window_size\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80c484",
   "metadata": {},
   "source": [
    "### 复杂度分析\n",
    "\n",
    "- **时间复杂度**：$O(M \\cdot w \\cdot d)$（vs 标准注意力的 $O(M^2 \\cdot d)$）\n",
    "- **空间复杂度**：$O(M \\cdot d)$（无需存储注意力矩阵）\n",
    "- **优势**：\n",
    "  - 当 $w \\ll M$ 时，计算量大幅减少\n",
    "  - 适合长序列处理（如长文档、代码分析）\n",
    "  - 保留局部上下文信息\n",
    "\n",
    "### 示例验证\n",
    "\n",
    "**输入**：\n",
    "- $M = 2, d = 4, w = 1$\n",
    "- $Q = \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix}$\n",
    "- $K = \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 & 0.0 \\end{bmatrix}$\n",
    "- $V = \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 \\\\ 5.0 & 6.0 & 7.0 & 8.0 \\end{bmatrix}$\n",
    "\n",
    "**输出**：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2.5101628 & 3.5101628 & 4.510163 & 5.510163 \\\\\n",
    "3.4898374 & 4.4898376 & 5.4898376 & 6.489837\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  性能对比与总结\n",
    "\n",
    "### 三种注意力机制对比\n",
    "\n",
    "| 特性 | Multi-Head Attention | Linear Attention | Sliding Window Attention |\n",
    "|------|---------------------|------------------|-------------------------|\n",
    "| **时间复杂度** | $O(N^2 \\cdot d)$ | $O(N \\cdot d^2)$ | $O(N \\cdot w \\cdot d)$ |\n",
    "| **空间复杂度** | $O(h \\cdot N^2)$ | $O(N \\cdot d + d^2)$ | $O(N \\cdot d)$ |\n",
    "| **优势场景** | 标准Transformer，全局上下文 | 长序列（$N \\gg d$） | 超长序列，局部依赖 |\n",
    "| **劣势场景** | 长序列（内存/计算爆炸） | 短序列（$d > N$时效率低） | 需要全局上下文的任务 |\n",
    "| **表达能力** | 全局最强 | 近似全局 | 局部最强 |\n",
    "\n",
    "###  CUDA优化要点总结\n",
    "\n",
    "#### 通用优化策略\n",
    "\n",
    "1. **内存访问优化**\n",
    "   - 合并全局内存访问（coalesced access）\n",
    "   - 使用共享内存缓存频繁访问的数据\n",
    "   - 减少全局内存往返次数\n",
    "\n",
    "2. **数值稳定性**\n",
    "   - Softmax计算前减去最大值\n",
    "   - 使用double精度进行累加操作\n",
    "   - 检查除零情况\n",
    "\n",
    "3. **并行策略**\n",
    "   - 根据问题规模选择线程/Block配置\n",
    "   - 平衡线程占用率和寄存器/共享内存使用\n",
    "   - 避免线程束分歧（warp divergence）\n",
    "\n",
    "4. **算法特定优化**\n",
    "   - Multi-Head：跨头并行，注意力矩阵分块计算\n",
    "   - Linear：利用矩阵结合律改变计算顺序\n",
    "   - Sliding Window：寄存器数组存储局部数据\n",
    "\n",
    "#### 性能调优检查清单\n",
    "\n",
    "- [ ] 使用 `nvprof` 或 Nsight Compute 分析瓶颈\n",
    "- [ ] 检查全局内存带宽利用率\n",
    "- [ ] 优化线程块大小（通常256或512）\n",
    "- [ ] 最小化 `__syncthreads()` 调用\n",
    "- [ ] 考虑使用Tensor Core（适用于混合精度）\n",
    "- [ ] 实现多流并行（overlap计算和数据传输）\n",
    "\n",
    "\n",
    "## 结语\n",
    "\n",
    "这三种注意力机制代表了深度学习中不同的设计哲学：\n",
    "\n",
    "- **Multi-Head Attention** 追求表达能力，牺牲计算效率\n",
    "- **Linear Attention** 追求计算效率，近似全局注意力\n",
    "- **Sliding Window Attention** 在局部与全局间取得平衡\n",
    "\n",
    "在实际应用中，选择合适的注意力机制需要根据具体任务、数据特性和资源约束综合考虑。CUDA实现中的优化技巧也可以互相借鉴，例如：\n",
    "\n",
    "- Linear Attention的计算重排思想可以应用于Multi-Head\n",
    "- Sliding Window的局部处理策略可以与Linear Attention结合\n",
    "- 共享内存、寄存器数组等优化手段通用\n",
    "\n",
    "希望这份文档能帮助你深入理解这些注意力机制及其高效CUDA实现！\n",
    "\n",
    "\n",
    "###  扩展阅读\n",
    "\n",
    "- **Multi-Head Attention**: [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "- **Linear Attention**: [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)\n",
    "- **Sliding Window**: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\n",
    "\n",
    "---\n",
    "\n",
    "## 附录：代码文件说明\n",
    "\n",
    "### 编译与运行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294e9a1",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "# 编译Multi-Head Attention\n",
    "nvcc -o multihead multihead.cpp -arch=sm_70\n",
    "\n",
    "# 编译Linear Attention\n",
    "nvcc -o linear_attention linear-self-attention.cpp -arch=sm_70\n",
    "\n",
    "# 编译Sliding Window Attention\n",
    "nvcc -o sliding_window sliding-window-self-attention.cpp -arch=sm_70\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340601c",
   "metadata": {},
   "source": [
    "**注意**：根据你的GPU架构调整 `-arch` 参数：\n",
    "- Tesla V100: `sm_70`\n",
    "- RTX 20 series: `sm_75`\n",
    "- RTX 30 series: `sm_86`\n",
    "- RTX 40 series: `sm_89`\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

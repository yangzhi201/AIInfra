{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e664dee",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# Linear Self-Attention\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "markdown",
   "source": [
    "### 任务要求\n",
    "\n",
    "实现 Linear Attention 机制，给定：\n",
    "- 查询矩阵 **Q**（大小 M×d）\n",
    "- 键矩阵 **K**（大小 M×d）\n",
    "- 值矩阵 **V**（大小 M×d）\n",
    "\n",
    "使用以下公式计算输出矩阵：\n",
    "\n",
    "$$\n",
    "\\text{LinearAttention}(Q, K, V) = \\frac{\\phi(Q) \\left(\\phi(K)^T V\\right)}{\\phi(Q) \\left(\\sum_{j} \\phi(K_j)\\right)}\n",
    "$$\n",
    "\n",
    "其中 $\\phi(\\mathbf{x})$ 是特征映射函数（Feature Map），定义为：\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\text{ELU}(x) + 1 =\n",
    "\\begin{cases}\n",
    "x + 1, & x > 0 \\\\\n",
    "e^x, & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 约束条件\n",
    "\n",
    "- 矩阵 **Q**、**K**、**V** 和 **output** 的数据类型均为 `float32`\n",
    "- M 和 d 的数据类型为 `int32`\n",
    "- 序列长度：1 ≤ M ≤ 10000\n",
    "- 特征维度：1 ≤ d ≤ 128\n",
    "- 矩阵元素范围：[-100.0, 100.0]\n",
    "\n",
    "### 实现要求\n",
    "\n",
    "1. **仅使用原生 CUDA 特性**：不允许使用外部库（如 cuBLAS）\n",
    "2. **函数签名保持不变**：`solve` 函数接口必须按要求实现\n",
    "3. **结果存储**：最终输出必须写入 `output` 矩阵\n",
    "\n",
    "\n",
    "##  Linear Attention 核心原理\n",
    "\n",
    "###  传统 Softmax Attention 的瓶颈\n",
    "\n",
    "标准的 Transformer 自注意力机制的计算公式为：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n",
    "$$\n",
    "\n",
    "**问题分析**：\n",
    "- **计算复杂度**：O(M² × d)\n",
    "  - 计算 $QK^T$ 需要 O(M² × d) 次操作\n",
    "  - 得到的注意力矩阵大小为 M×M\n",
    "- **空间复杂度**：O(M²)\n",
    "  - 需要存储完整的注意力矩阵\n",
    "- **扩展性瓶颈**：\n",
    "  - 当 M=1024 时，注意力矩阵需要 4MB 内存\n",
    "  - 当 M=8192 时，注意力矩阵需要 256MB 内存\n",
    "  - 内存占用随序列长度平方增长\n",
    "\n",
    "###  Linear Attention 的突破\n",
    "\n",
    "Linear Attention 的核心思想是**通过核技巧（Kernel Trick）改变计算顺序**，避免显式计算 M×M 的注意力矩阵。\n",
    "\n",
    "#### 关键洞察\n",
    "\n",
    "观察以下两种计算顺序：\n",
    "\n",
    "**标准 Attention（O(M²)）**：\n",
    "1. 先计算 $A = \\text{softmax}(QK^T)$，得到 M×M 矩阵\n",
    "2. 再计算 $\\text{Output} = AV$\n",
    "\n",
    "**Linear Attention（O(M)）**：\n",
    "1. 先计算 $S = \\phi(K)^T V$，得到 d×d 矩阵\n",
    "2. 再计算 $\\text{Output} = \\phi(Q) S$\n",
    "\n",
    "关键在于：**矩阵乘法满足结合律**，通过改变括号位置，可以避免生成大矩阵。\n",
    "\n",
    "#### 数学推导\n",
    "\n",
    "标准 Softmax Attention 可以近似为：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) \\approx \\frac{\\phi(Q) \\phi(K)^T V}{\\text{normalizer}}\n",
    "$$\n",
    "\n",
    "其中归一化项为：\n",
    "\n",
    "$$\n",
    "\\text{normalizer} = \\phi(Q) \\left(\\sum_{j=1}^{M} \\phi(K_j)\\right)\n",
    "$$\n",
    "\n",
    "利用结合律重排计算顺序：\n",
    "\n",
    "$$\n",
    "\\phi(Q) \\left(\\phi(K)^T V\\right) \\text{ 代替 } \\left(\\phi(Q) \\phi(K)^T\\right) V\n",
    "$$\n",
    "\n",
    "这样就避免了计算 M×M 的 $\\phi(Q) \\phi(K)^T$ 矩阵。\n",
    "\n",
    "###  复杂度对比\n",
    "\n",
    "| 操作 | Softmax Attention | Linear Attention |\n",
    "|------|-------------------|------------------|\n",
    "| **时间复杂度** | O(M² × d) | O(M × d²) |\n",
    "| **空间复杂度** | O(M²) | O(d²) |\n",
    "| **瓶颈操作** | 计算 QK^T（M×M 矩阵） | 计算 K^TV（d×d 矩阵） |\n",
    "| **适用场景** | 短序列（M < 512） | 长序列（M >> d） |\n",
    "\n",
    "**性能分析**（M=1024, d=64）：\n",
    "\n",
    "**Softmax Attention**：\n",
    "- 注意力矩阵：1024×1024 = 1,048,576 个元素\n",
    "- 计算量：≈ 67M 次浮点运算\n",
    "- 内存：4MB\n",
    "\n",
    "**Linear Attention**：\n",
    "- 中间矩阵：64×64 = 4,096 个元素\n",
    "- 计算量：≈ 4M 次浮点运算\n",
    "- 内存：16KB\n",
    "\n",
    "**加速比**：约 **16× 时间复杂度降低**，**256× 空间复杂度降低**\n",
    "\n",
    "###  特征映射函数 φ(x) 的选择\n",
    "\n",
    "本实现采用 **ELU+1** 作为特征映射：\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\text{ELU}(x) + 1 =\n",
    "\\begin{cases}\n",
    "x + 1, & x > 0 \\\\\n",
    "e^x, & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**设计考量**：\n",
    "- **非负性**：$\\phi(x) > 0$，确保注意力权重为正（类似 Softmax）\n",
    "- **平滑性**：可微且连续，有利于梯度传播\n",
    "- **表达能力**：能够捕捉输入的非线性关系\n",
    "- **数值稳定性**：\n",
    "  - 当 x > 0 时，输出为线性增长（避免指数爆炸）\n",
    "  - 当 x ≤ 0 时，输出为指数衰减（保持平滑过渡）\n"
   ],
   "id": "9ed8edeafbbc4e52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "接下来实现 CUDA 版本的 Linear Self-Attention：",
   "id": "b70cbf801428bd65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#include <cuda_runtime.h>\n",
    "#include <cmath>\n",
    "\n",
    "// Feature map: ELU(x) + 1\n",
    "__device__ float phi(float x) {\n",
    "    if (x > 0.0f) {\n",
    "        return x + 1.0f;\n",
    "    } else {\n",
    "        return expf(x);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Apply phi element-wise to a matrix\n",
    "__global__ void apply_phi_kernel(const float* input, float* output, int M, int d) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = M * d;\n",
    "\n",
    "    if (idx < total) {\n",
    "        output[idx] = phi(input[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Compute phi(K)^T * V -> result is (d x d)\n",
    "__global__ void compute_KT_V_kernel(const float* phi_K, const float* V, float* KT_V, int M, int d) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < d && col < d) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < M; i++) {\n",
    "            sum += phi_K[i * d + row] * V[i * d + col];\n",
    "        }\n",
    "        KT_V[row * d + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Compute sum_j phi(K_j) -> result is (d,)\n",
    "__global__ void compute_sum_phi_K_kernel(const float* phi_K, float* sum_phi_K, int M, int d) {\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (col < d) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < M; i++) {\n",
    "            sum += phi_K[i * d + col];\n",
    "        }\n",
    "        sum_phi_K[col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Compute numerator: phi(Q) * (phi(K)^T * V) -> result is (M x d)\n",
    "__global__ void compute_numerator_kernel(const float* phi_Q, const float* KT_V, float* numerator, int M, int d) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < d) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < d; i++) {\n",
    "            sum += phi_Q[row * d + i] * KT_V[i * d + col];\n",
    "        }\n",
    "        numerator[row * d + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Compute denominator: phi(Q) * sum_phi_K -> result is (M,)\n",
    "__global__ void compute_denominator_kernel(const float* phi_Q, const float* sum_phi_K, float* denominator, int M, int d) {\n",
    "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < d; i++) {\n",
    "            sum += phi_Q[row * d + i] * sum_phi_K[i];\n",
    "        }\n",
    "        denominator[row] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Divide numerator by denominator element-wise\n",
    "__global__ void divide_kernel(const float* numerator, const float* denominator, float* output, int M, int d) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < M && col < d) {\n",
    "        output[row * d + col] = numerator[row * d + col] / denominator[row];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Q, K, V, output are device pointers\n",
    "extern \"C\" void solve(const float* Q, const float* K, const float* V, float* output, int M, int d) {\n",
    "    // Allocate temporary buffers\n",
    "    float *phi_Q, *phi_K, *KT_V, *sum_phi_K, *numerator, *denominator;\n",
    "\n",
    "    cudaMalloc(&phi_Q, M * d * sizeof(float));\n",
    "    cudaMalloc(&phi_K, M * d * sizeof(float));\n",
    "    cudaMalloc(&KT_V, d * d * sizeof(float));\n",
    "    cudaMalloc(&sum_phi_K, d * sizeof(float));\n",
    "    cudaMalloc(&numerator, M * d * sizeof(float));\n",
    "    cudaMalloc(&denominator, M * sizeof(float));\n",
    "\n",
    "    // Step 1: Apply phi to Q and K\n",
    "    int total_elements = M * d;\n",
    "    int threads_1d = 256;\n",
    "    int blocks_1d = (total_elements + threads_1d - 1) / threads_1d;\n",
    "\n",
    "    apply_phi_kernel<<<blocks_1d, threads_1d>>>(Q, phi_Q, M, d);\n",
    "    apply_phi_kernel<<<blocks_1d, threads_1d>>>(K, phi_K, M, d);\n",
    "\n",
    "    // Step 2: Compute phi(K)^T * V (d x d)\n",
    "    dim3 threads_2d(16, 16);\n",
    "    dim3 blocks_KT_V((d + 15) / 16, (d + 15) / 16);\n",
    "    compute_KT_V_kernel<<<blocks_KT_V, threads_2d>>>(phi_K, V, KT_V, M, d);\n",
    "\n",
    "    // Step 3: Compute sum_j phi(K_j)\n",
    "    int blocks_sum = (d + threads_1d - 1) / threads_1d;\n",
    "    compute_sum_phi_K_kernel<<<blocks_sum, threads_1d>>>(phi_K, sum_phi_K, M, d);\n",
    "\n",
    "    // Step 4: Compute numerator phi(Q) * KT_V (M x d)\n",
    "    dim3 blocks_num((d + 15) / 16, (M + 15) / 16);\n",
    "    compute_numerator_kernel<<<blocks_num, threads_2d>>>(phi_Q, KT_V, numerator, M, d);\n",
    "\n",
    "    // Step 5: Compute denominator phi(Q) * sum_phi_K (M,)\n",
    "    int blocks_denom = (M + threads_1d - 1) / threads_1d;\n",
    "    compute_denominator_kernel<<<blocks_denom, threads_1d>>>(phi_Q, sum_phi_K, denominator, M, d);\n",
    "\n",
    "    // Step 6: Divide numerator by denominator\n",
    "    dim3 blocks_div((d + 15) / 16, (M + 15) / 16);\n",
    "    divide_kernel<<<blocks_div, threads_2d>>>(numerator, denominator, output, M, d);\n",
    "\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Free temporary buffers\n",
    "    cudaFree(phi_Q);\n",
    "    cudaFree(phi_K);\n",
    "    cudaFree(KT_V);\n",
    "    cudaFree(sum_phi_K);\n",
    "    cudaFree(numerator);\n",
    "    cudaFree(denominator);\n",
    "}\n"
   ],
   "id": "c1a0eab4640713ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "本实现包含 6 个 CUDA 内核：\n",
    "\n",
    "1. **`apply_phi_kernel`**：并行应用特征映射 φ(x)\n",
    "   - 线程分配：每个线程处理一个元素\n",
    "   - 复杂度：O(M×d)\n",
    "\n",
    "2. **`compute_KT_V_kernel`**：计算 φ(K)^T V\n",
    "   - 线程分配：16×16 线程块处理 d×d 输出矩阵\n",
    "   - 复杂度：O(M×d²)\n",
    "\n",
    "3. **`compute_sum_phi_K_kernel`**：计算列和\n",
    "   - 线程分配：每个线程负责一列的求和\n",
    "   - 复杂度：O(M×d)\n",
    "\n",
    "4. **`compute_numerator_kernel`**：计算 φ(Q) · KT_V\n",
    "   - 线程分配：16×16 线程块处理 M×d 输出矩阵\n",
    "   - 复杂度：O(M×d²)\n",
    "\n",
    "5. **`compute_denominator_kernel`**：计算 φ(Q) · sum_phi_K\n",
    "   - 线程分配：每个线程处理一行的点积\n",
    "   - 复杂度：O(M×d)\n",
    "\n",
    "6. **`divide_kernel`**：逐元素相除\n",
    "   - 线程分配：16×16 线程块处理 M×d 输出矩阵\n",
    "   - 复杂度：O(M×d)\n",
    "\n",
    "### 内存管理\n",
    "\n",
    "**中间缓冲区**：\n",
    "- `phi_Q`：M×d（φ(Q) 的结果）\n",
    "- `phi_K`：M×d（φ(K) 的结果）\n",
    "- `KT_V`：d×d（键-值交互矩阵）\n",
    "- `sum_phi_K`：d（键的列和）\n",
    "- `numerator`：M×d（分子）\n",
    "- `denominator`：M（分母）\n",
    "\n",
    "**内存占用估算**（以 M=1024, d=64, float32 为例）：\n",
    "- `phi_Q` 和 `phi_K`：2 × 1024 × 64 × 4 bytes = 512 KB\n",
    "- `KT_V`：64 × 64 × 4 bytes = 16 KB\n",
    "- `sum_phi_K`：64 × 4 bytes = 256 bytes\n",
    "- `numerator`：1024 × 64 × 4 bytes = 256 KB\n",
    "- `denominator`：1024 × 4 bytes = 4 KB\n",
    "- **总计**：≈ 788 KB\n",
    "\n",
    "对比传统 Attention 的注意力矩阵（1024×1024×4 bytes = 4 MB），内存节省约 **80%**。\n"
   ],
   "id": "543c60d46f9d416"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "77e6044ef58c2ea5"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

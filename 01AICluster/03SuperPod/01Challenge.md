<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->

# 01.万卡 AI 集群建设挑战

Author by: staries wang

## 一、超万卡集群背景与趋势

### 1.1 大模型驱动万卡集群建设

自 ChatGPT 面世以来，大模型技术进入迅猛迭代期，模型参数量呈“指数级”跃升，从 2018 年 BERT 模型的 1.1 亿参数，快速突破至 2021 年 GPT-3 的 1750 亿参数，而后随着 Mixture of Experts（MOE）等先进结构迈入万亿规模，OpenAI 于 2025 年发布的 GPT-5 参数规模已经达到了 52 万亿。大模型能力的跃迁，直接带来了对巨量算力与能源的刚性需求，传统算力设施已无法满足需求。从具体消耗来看，GPT-4（含 16 个专家模型、1.8 万亿参数）一次训练需在 25000 个 A100 芯片上运行 90 至 100 天；从设施要求来看，大模型对底层算力提出更高标准，不仅需要更高密度的存算硬件、高性能无阻塞的网络连接，还需支持更高并行度的通信与计算范式，新一代智算中心的技术需求日益严苛。这种“算力缺口”成为关键推力，让超万卡集群逐渐成为大模型基建的标配。

在全球化科技竞争格局下，国内外企业纷纷布局超万卡集群。国际上，Google 推出含 26000 块 Nvidia H100 GPU 的 A3 虚拟机，同时搭建 TPUv5p 8960 卡自研芯片集群；Meta 于 2022 年建成含有 16000 块 A100 芯片的集群 AI Research Super Cluster，2024 年初又公布了 2 个配备 24576 块 H100 芯片的集群，用于生成式 AI 模型训练。国内方面，通信运营商依托机房资源建设智算中心，头部互联网企业（如字节跳动搭建 12288 卡 Ampere 架构集群）加速技术突破，全行业通过布局超万卡集群，支撑大模型快速迭代、应对市场趋势，抢占 AI 技术与产业创新的先机。

### 1.2 万卡集群整体建设挑战

超万卡 GPU/NPU 大规模 AI 集群建设面临多维度复杂挑战。在基础设施层面，超万卡集群对机房配套设施的需求相对于传统云数据中心发生重大变化，对供电、承重、机房洁净度和走线架设计等有极高要求，需要解决高密度供电、高效散热、超大规模网络互联等问题，集群需承载超 5000P 算力、200P 存储资源及近千台交换机，涉及超千机柜、5000 台设备，配套超 10 万个光模块、8 万根数据线缆及 20 万个接头，且需多机房部署，同时参与方涵盖设计院、研发部、设备供应商等多方，工期紧张且机房初始准备度低，协调与推进难度大。其次是硬件资源与能耗约束，单卡能耗达 350W、单节点 3000W、单柜 20kW，千余机柜集中部署使能耗密度突破物理极限，电力供应与散热压力显著；在系统架构层面，需要实现异构算力的统一管理、大规模资源的智能调度、训练任务的高可用保障，模型规模扩大到万亿量级，数据的吞吐量和计算量已远远超过目前最强单机单卡能力，多机多卡互联和并行训练策略至关重要。此外，多层互联架构下近千台交换机易导致网络通信带宽不足与延迟升高，集合通信、点对点通信在超规模场景下回环时间延长。在运维管理层面，系统可靠性随规模扩张呈指数级下降，超万卡集群承载万亿模型训练意味着千万器件的满负荷高速运转，任一部件不可恢复的失效都可能导致训练中断，带来超万卡集群高可用和易运维的关键挑战。在国产化与成本维度方面，万卡集群面临国产化生态与性能的双重差距，同时千亿级的投资规模带来极高的商业运营风险，进一步加剧了集群建设的综合挑战。

## 二 电力供应与散热挑战：能耗密度突破物理极限

### 2.1 万卡集群功耗现状

超大规模 AI 集群建设正面临着前所未有的能耗挑战。首先，AI 芯片的功耗正呈现出前所未有的增长态势。从 NVIDIA 的产品线来看，功耗演进经历了明显的跃升过程。A100 系列的功耗为 250-400 W，而到了 H100 系列，功耗显著提升至 350-800 W。最新发布的 Blackwell 架构更是将功耗推向了新的高度 B200 的功耗已达 1000-1200 W。此外，由两颗 B200 GPU 和一颗 Grace CPU 组成的 GB200 超级芯片，总功耗高达 2700 W。

根据单张 H100 GPU 功耗 525 W，10 万张 GPU 总功耗 52.5 MW 估算，并考虑其他 IT 设备（服务器、存储、网络等），10 万张 H100 GPU 集群总功耗可达 150 MW。在年耗电量方面，10 万卡集群的年耗电量约为 15.9 亿度电，这相当于约 15 万个家庭一年的用电量，按照美国电力标准费率 0.078 美元/千瓦时计算，10 万卡集群每年的电费成本约为 1.24 亿美元。

### 2.2 数据中心供电架构演进：从 12V 到 48V 直连再到垂直供电 VPD

传统 12V 供电架构是早期数据中心和服务器的主流配电方案，其基本原理基于直流电压分配网络（PDN）。在这一架构中，市电通过不间断电源或高压直流系统转换为 12V 直流母线电压，然后通过母线排、电缆和 PCB 走线分配到各个服务器和 IT 设备[^1]。

[^1]: https://www.toutiao.com/article/7561269403564638729/?upstream_biz=doubao&source=m_redirect

12V 供电的核心问题在于其固有的 I²R 损耗机制。根据功率损耗公式 P=I²R，当传输功率一定时，电压越低所需电流越大，而电流的平方增长会导致损耗急剧增加。在典型的 12V 系统中，分配路径的电阻虽然看似微小，但在大电流传输时会产生显著的功率损耗。

随着 AI 芯片性能的指数级增长，服务器功耗呈现爆发式上升。以英伟达为例，其 B200GPU 单芯片功耗达 1000W，GB200 更突破至 2700W，驱动单机柜功率从 20kW 跃升至 120kW 以上。这种功耗激增直接导致了电流需求的急剧上升。现代 AI 处理器的持续电流需求可能达到 1000A 或更高，峰值电流甚至可达 2000A。在如此高的电流需求下，I²R 损耗剧增，散热成本攀升。

在功率损耗和散热管理方面，通常有两种方法可以改善 PDN 对电力系统性能的影响：第一种方式是使用更大尺寸的电缆、连接器和更厚的主板电源板以降低 PDN 电阻；第二种方式是提高电压以降低给定功率传输的电流，这样可以降低电缆、连接器、主板铜平面尺寸及其相关的尺寸、成本和重量。

然而，采用第一种方式会将增加的功率分配给多个服务器处理会造成更大的功率损失，因此，近年来电力设计越来越多地采用第二种方式，即采用更高的电压来降低功率损耗。2016 年，谷歌在 OCP 峰会上提出了 48V 机架电源架构，用以取代当时普遍应用的 12V。

相比 12V 电源架构，采用 48V 直流馈电的优势非常明显。假设分配路径的电阻为 0.1mΩ，12V 的分配损耗为 100W，但在 48V 的情况下，损耗为 6.25W，这里有 16 倍的差值。也就是说，相对 12V 的配电方案，48V 方案可将总功率损耗降低 16 倍，整个系统的转换效率提升 30%[^2]。这一巨大差异在高功率密度场景下变得尤为突出，成为制约数据中心能效提升的关键瓶颈。

[^2]: https://component.eetrend.com/content/2023/100570529.html

![图 1](01.png)

图 1：12V 和 48V 直流供电二者配电损耗比较

尽管 48V 直连方案解决了传输损耗问题，但在高电流（\>1000A/芯片）场景下，电压转换效率成为新的技术瓶颈。这一问题的产生源于现代处理器内核电压的持续下降趋势。现代 AI 处理器的内核电压已经降至 0.7V 甚至更低，而 48V 输入与 0.7V 输出之间的转换比为 68:1。如此高的转换比使得传统的 DC/DC 转换器效率急剧下降，即使采用最先进的同步整流技术，效率也难以超过 88%。大电流下的转换损耗激增成为不可忽视的问题。在 1000A 电流下，即使转换效率达到 90%，仍有 100W 的功率损耗需要处理。而实际情况往往更加严峻，传统的电压调节模块在高转换比和大电流的双重压力下，效率可能降至 80%以下，导致数百瓦的损耗。

此外，高转换损耗产生的热量必须通过有效的散热系统来处理，这不仅增加了系统成本，还带来了设计复杂性。在高密度的服务器环境中，为电压调节模块提供足够的散热空间和气流成为一个巨大挑战。

面对 48V 架构在高电流转换效率方面的挑战，Vicor 公司提出了革命性的垂直供电架构（Vertical Power Delivery, VPD）。利用分比式电源架构，通过横向供电和垂直供电的完美结合，最大限度降低"最后一英寸"阻抗。（随着 AI 技术发展，芯片等设计越来越小，对 PCB 空间限制越来越严苛）[^3]。

[^3]: https://blog.csdn.net/FL63Zv9Zou86950w/article/details/119396129

具体来说，传统的供电架构采用"平面式"设计，即功率转换模块位于处理器的同一平面，通过 PCB 走线将电力传输到处理器。这种设计不可避免地存在较长的电流传输路径，即使在 48V 架构下，仍然会产生显著的传输损耗。而在垂直供电中，电流倍增器直接位于处理器下方、电路板的另一侧，通过缩短电流在主板上的传输距离，大幅降低了电源分配网络的损耗。且为了实现最高效率，电流从 VPD 解决方案流出的实际位置和路径模式和处理器核心电源输入的位置和路径模式完全一致，这能使大电流流动形成真正的“垂直”分布。

VPD 解决方案是一个集成模块，由三层组成：一个 VTM™电流倍增器阵列，其下方设有一个齿轮箱，上方装有一个 PRM™调节器，可为每个处理器（一种 DCM™）提供完整的 48V 至负载稳压解决方案。VTM 阵列的尺寸基于处理器的输出电流要求确定，PRM 的尺寸则基于功率要求确定。如果 GPU 或 ASIC 需要多个电源轨，那么 VTM 和 PRM 层可采用独立的 PRM 和 VTM 来实现，其尺寸需满足每个特定电源轨的电流和功率电压要求。齿轮箱有两个功能：它集成了高频去耦电容，并将来自 VTM 的电流重新分配成与上方处理器相匹配的模式。电流倍增器 GTM 置于处理器下方，最大限度地提高电源传输性能。此外，采用 SM-ChiP 封装将所有无源器件、磁性器件、MOSFET 和控制器集成到一个模块中，降低噪声改善散热性能。这种 VPD 解决方案还能释放处理器顶部外围空间，为更多选择提供可能，包括更高的输入/输出布线、板载内存或更紧密的处理器集群[^4]。

[^4]: https://www.powersystemsdesign.com/articles/powering-clustered-ai-processors/29/18419

![https://www.powersystemsdesign.com/images/articles/1635364477_Figure-2.png](02.png)

图 2 VPD 结构示意图

| 对比维度               | 传统 12V 架构  | 48V 直连架构 | VPD 垂直架构       |
|------------------------|----------------|--------------|--------------------|
| 配电电压               | 12V            | 48V          | 48V                |
| 典型传输电流（12kW）   | 1000A          | 250A         | 250A               |
| 传输损耗（100μΩ路径） | 100W           | 6.25W        | \<1W               |
| 转换效率               | 85-90%         | 88-92%       | 95-97%             |
| 系统总效率             | 75-80%         | 80-85%       | 90-95%             |
| PCB 空间占用           | 高（需宽走线） | 中           | 低（无大电流走线） |
| 散热需求               | 极高           | 高           | 低                 |
| 动态响应时间           | \>10μs         | 5-10μs       | \<1μs              |
| 成本指数（相对值）     | 1.0            | 1.2          | 1.5                |

### 2.3 液冷协同设计

当前液冷技术发展呈现出三大主流技术路线并行的格局。冷板式液冷作为最成熟的方案，通过在 CPU、GPU 等高发热器件上安装冷板实现液冷散热，改造相对简单，兼容性好，适合现有数据中心的升级改造。浸没式液冷则是更为彻底的解决方案，将整个服务器浸没在绝缘冷却液中，在高密度算力场景下优势明显，特别适合 AI 训练等高功率密度应用。喷淋式液冷介于两者之间，通过精确喷淋实现局部液冷。

随着数据中心功率密度的不断提升，液冷系统与配电系统的协同设计已成为提升整体能效的关键路径。英伟达 Kyber 机架架构实现了 800VDC 与液冷技术的融合，这一架构的核心在于对 800VDC 电力输送、液体冷却和机械设计的创新，能显著提升供电效率，减少能量损耗。在 2024 年 OCP 全球峰会上，谷歌讨论了开发多达 1 MW IT 机架的计划[^5]。这一目标的实现离不开液冷与配电系统的深度协同。

[^5]: https://blog.csdn.net/j6UL6lQ4vA97XlM/article/details/147158047

液冷与配电系统协同设计的必要性主要体现在以下几个方面：

首先是空间利用的优化需求。传统的液冷系统和配电系统通常独立设计，占用了大量的机房空间。通过协同设计，可以实现液冷管路与电力电缆的一体化布局，大幅提升空间利用效率。例如，Meta 开发了液冷母线排，包含一个垂直沿机架运行的新型液冷母线，以及将电源机架连接到 IT 机架的母线，通过增加母线排深度和额外的冷板，可以实现 700kW 以上的容量[^6]。

[^6]: https://www.itherm.cn/index/news/news_show/article_id/141.html

其次是能效提升的协同效应。液冷系统的功耗与 IT 设备的功率密度密切相关，而配电系统的效率又直接影响到 IT 设备的供电质量。通过建立液冷与配电系统的动态调节机制，可以实现根据 IT 设备负载实时调整液冷流量和配电容量，从而最大化系统能效。通过液冷与配电系统的协同优化，可以将数据中心 PUE 从 1.3 降至 1.25 以下[^7]。

[^7]: https://www.sohu.com/a/893134962_122081497

而后是可靠性提升的系统级保障。液冷系统的泄漏风险与配电系统的电气安全密切相关，任何冷却液泄漏都可能造成电气短路。通过协同设计，可以建立统一的监控和预警系统，实现对液冷泄漏和电气故障的综合监测和快速响应。例如，采用介电传感器检测冷却液介电常数变化，灵敏度可达±0.5%，能够在泄漏发生的第一时间触发报警。

## 三、网络通信

### 3.1 万卡建设互联现状

在万卡级 AI 集群中，框式交换机和盒式交换机的混合架构已成为主流选择。框式交换机通常作为核心层和汇聚层设备，具备强大的扩展性和高性能处理能力。这类设备拥有较大的机箱，采用模块化设计，内部包含多个插槽，可插入不同功能的模块，如电源模块、交换模块、接口模块等[^8]。框式交换机通过背板交换连接多块线卡，其内部连线采用 CLOS 结构，这种设计使得单个框式交换机能够支持数百个端口的高密度连接（关于框式交换机硬件架构的演进可以参考<https://zhuanlan.zhihu.com/p/712382241>）。

[^8]: https://www.51cto.com/article/808883.html

![](03.png)

图 3 框式交换机

相比之下，盒式交换机则主要应用于接入层，具有固定配置、固定端口数量、固定电源模块和风扇等特点。盒式交换机通常高度在 1-2U 之间，端口数量一般小于 60 个，不具备扩展性[^9]。

[^9]: https://blog.csdn.net/grimmp/article/details/107940282

在实际的万卡集群部署中，“框框、盒盒盒”结构体现为多层次的网络拓扑设计。这种设计充分利用了框式交换机的高性能和盒式交换机的高密度特性，实现了成本与性能的优化平衡。

下图是百度百舸的高性能网络 HPN—AIPod 的架构示意图。AIPod 使用 8 导轨网络架构，以 GPU A800 服务器为例，它配有 8 张网卡，然后每张网卡分别连到一个 TOR 汇聚组的 8 个 TOR 上。在 TOR 和 LEAF 层面，通过 Full Mesh 的方式进行互联。如果是三层 RDMA 网络，则在 LEAF 和 SPINE 层面也是采用 Full Mesh 的互联方式[^10]。

[^10]: https://zhuanlan.zhihu.com/p/706868061

![https://static001.geekbang.org/infoq/50/501abf0a78770eebf3ef32702e1690b7.webp?x-oss-process=image%2Fresize%2Cp_80%2Fformat%2Cpng](04.png)

图 4 百度百舸的高性能网络 HPN—AIPod 架构示意图

### 3.2 万卡集群通信算法

相对于单卡训练，大规模的分布式训练常在训练数据量较大或模型参数规模太大导致单卡不可训练的场景下使用。如当训练数据量较大时，单卡训练耗时过长，需要分布式训练技术提高训练效率；或者当单卡无法支持训练时，即单卡显存无法放下全量模型参数时，可使用分布式训练技术将模型参数等模型信息切分到多张卡或多台设备上，以保证模型可训练。集合通信训练模式和参数服务器训练模式是两种最主要的分布式训练模式。

Ring 算法是分布式训练中最基础的集合通信算法，英伟达的 NCCL 通信库采用了这种算法，其工作原理相对简单：将所有通信节点通过首尾连接形成一个单向环，数据在环上依次传输。在 Ring AllReduce 算法中，整个过程分为两个阶段：Reduce-Scatter 和 AllGather。在 Reduce-Scatter 阶段，每个节点将数据切分成 N 份（N 为节点总数），并将不同的分片发送给环上的邻居节点；在 AllGather 阶段，各节点收集来自其他节点的所有分片数据[^11]。

[^11]: Ring 算法<https://blog.csdn.net/weixin_30814329/article/details/102424508>

Ring 算法的优势在于其带宽效率高，因为每个 GPU 都参与发送和接收，能够充分利用所有节点的带宽资源。在理想情况下，Ring AllReduce 的通信时间与 GPU 节点数无关，只受限于 GPU 间最慢的连接，通信时间计算公式为：T = 2\*(N-1)\*S/(NB)，其中 N 为节点数，S 为数据大小，B 为带宽[^12]。这意味着当网络中存在高速链路时，Ring 算法能够实现接近理论极限的性能。

[^12]: https://blog.csdn.net/weixin_30814329/article/details/102424508

然而，当集群规模扩展到万卡级别时，Ring 算法面临着严重的性能瓶颈。最主要的问题是延迟随 GPU 数量呈线性增长，这是因为 Ring 算法需要 N-1 步才能完成一次全规约操作，每一步都需要等待前一阶段的数据传输完成。在 24,000 块 GPU 的超大规模集群中，树状通信算法在延迟上可优化约 180 倍，此时树状通信的延迟显著优于环状通信[^13]。

[^13]: https://blog.csdn.net/lianghuaju/article/details/144899986

此外，Ring 算法在超大规模集群中的另一个挑战是链路故障的影响。由于所有节点形成一个环，任何一个链路或节点故障都可能导致整个环的中断，进而影响整个训练任务的执行。虽然可以通过双环或多环设计来提高可靠性，但这会进一步增加网络的复杂度和成本。

另一种常见的 AllReduce 实现算法是 Halving-Doubling，该算法的核心机制是每次选择节点距离倍增的节点进行两两通信，且每次通信量会相应地倍减（或倍增）。Halving-Doubling 算法的优点十分突出：一方面，它的通信步骤较少，仅需 2log₂N 次（其中 N 表示参与通信的节点数）即可完成，因此拥有更低的延迟，相比之下 Ring 算法的通信步骤需 2（N-1）次；另一方面，它规避了单节点瓶颈问题，同时能让每个节点充分利用自身的发送与接收带宽，是目前超大规模通信场景中常用的方式。但这种算法的缺点是每一步相互通信的节点均不相同，链接的来回切换会带来额外开销；并且在通信的最后几步中会产生大量数据传递，进而导致速度变慢。

在超大规模 AI 集群中，随着 GPU 数量的增长，无论是 Ring 算法还是 HD 算法，其通信延迟都会显著上升[^14]。因此有很多研究者提出混合通信算法来提升整体性能。根据集群规模和通信模式的不同，动态选择最适合的通信算法。例如，在小规模集群中使用 Ring 算法，在大规模集群中使用树状算法。例如 ACCL 算法，第一步，基于 Ring 算法执行节点内的 Reduce-Scatter；第二步，基于 Halving-Doubling 算法执行节点间的 All-Reduce；第三步，基于 Ring 算法执行节点内的 All-Gather[^15]。

[^14]: https://zhuanlan.zhihu.com/p/469942194

[^15]: <https://blog.csdn.net/u013013023/article/details/137969201> xCCL: A Survey of Industry-Led Collective Communication Libraries for Deep Learning

### 3.3 超大规模组网复杂度

万卡级集群的组网复杂度呈指数级增长，其中光模块成本激增和 InfiniBand 协议的扩展性瓶颈成为制约产业发展的两大核心挑战。

#### 3.3.1 光模块成本激增

基于英伟达 H100 等主流 GPU 的典型配置，并考虑集群内部的多层网络架构和冗余设计，目前单个 10 万卡规模的 AI 训练集群需要 8-12 万个 800G 光模块。从 GPU 与光模块的配比关系来看，不同规模集群呈现出明显的非线性增长特征。在小规模集群（万卡以下）中，光模块与 GPU 的比例约为 1:2 至 1:2.5；中等规模集群（10-20 万卡）的配比提升至 1:5；而在 30 万或 50 万张卡的超大规模集群中，这一比例可能上升至 1:8。以英伟达 GH200 集群为例，在 4SU 集群的全光网络、三层 Fat-Tree 架构下，服务器和 Leaf 层交换机使用 400G 光模块，Leaf-Spine 和 Spine-Core 使用 800G 光模块，GPU 与光模块的比例达到 1:9。

多模光模块使用多模光纤，芯径较粗（通常为 50 或 62.5 微米），允许多条光模式同时传播，适合短距离高速传输，一般传输距离不超过 500 米，常用于数据中心内部或机房内连接。单模光模块则采用单模光纤，芯径细（通常为 9 微米），仅允许单一路径光信号传输，因而在传输过程中光信号衰减和色散较小，支持远距离传输，通常可达到几十公里甚至上百公里，适合城域网和长距离数据中心互联。

单模光模块和多模光模块在价格方面存在明显差异，主要源于其光源、光纤类型及制造工艺的不同。单模光模块通常采用分布反馈激光器（DFB）或外调制激光器（EML），激光器成本较高，且对光纤对准精度要求严格，使得单模模块整体价格较高。相较之下，多模光模块使用的垂直腔面发射激光器（VCSEL）成本较低，且多模光纤芯径较粗，连接和安装更加简便，因此模块和布线成本均较单模方案低廉。以 10G 速率为例，单模光模块价格通常在 120\~2000 元不等，而多模光模块价格多在 100 元左右[^16]。

[^16]: https://whgearlink.com/FAQ/794.html

光模块成本在 AI 集群总体拥有成本中占据重要地位，且这一占比随着集群规模的扩大而显著提升。在十万卡级 AI 集群中，光模块成本占比通常为 15%-20%。Meta 的 LLaMA3 训练集群采用 H100+800G 方案时，光模块投资占比约 18%[^17]。以 8-12 万个 800G 光模块的需求量计算，假设平均单价为 5000 元人民币，则光模块总成本约为 4-6 亿元人民币。在十万卡集群中，不同互联方案的光模块成本差异显著。英伟达 InfiniBand 方案的物料成本最高，约为 3.9 亿美元，光模块与 GPU 的比例约为 3.6 倍；英伟达以太网方案成本略低，约为 3.7 亿美元，比例约为 2.6 倍；博通以太网方案成本最低，约为 3.5 亿美元，比例同样为 2.6 倍，相比 InfiniBand 可节省约 4 亿美元。

[^17]: https://xueqiu.com/1251124410/351701042?_ugc_source=ugctoutiao

功耗成本是另一个重要的考虑因素。对于十万卡的 AI 集群，其光模块部分的功耗约为 40MW[^18]。按照美国电力标准费率 0.078 美元/千瓦时计算，每年的电力成本约为 2500 万美元。

[^18]: https://cloud.tencent.com.cn/developer/article/2529698

#### 3.3.2 InfiniBand 协议在超大规模组网中的技术瓶颈与成本挑战

InfiniBand 技术是一种用于高性能计算的计算机网络通信标准，具有极高的吞吐量和极低的延迟，专为计算机与计算机之间的数据互连而设计[^19]。InfiniBand 协议在 AI 训练低延迟网络领域拥有霸主地位，基于英伟达自己的一套协议，配合 GPU 运算特点自成一套体系。该协议在设计之初就支持 RDMA（远程直接内存访问）技术，从硬件层面确保数据的可靠传输，提供了更高的带宽和更低的延迟。

[^19]: https://blog.csdn.net/njbaige/article/details/142031711

在技术架构方面，InfiniBand 采用分层设计，包括物理层、链路层、网络层、传输层和应用层。其核心优势体现在以下几个方面：

1.  极低的通信延迟：InfiniBand 网络的端到端延迟通常在微秒级别，远低于以太网的毫秒级别延迟，这对于 AI 训练中的梯度同步等集体通信操作至关重要。
2.  高带宽利用率：InfiniBand 支持 QoS（服务质量）控制和拥塞管理，能够确保关键流量的带宽保证，避免网络拥塞导致的性能下降。
3.  硬件级可靠性：InfiniBand 在硬件层面实现了数据的可靠传输，包括错误检测、重传机制和路径冗余等功能，提高了系统的整体可靠性。
4.  与 GPU 的深度集成：InfiniBand 与英伟达 GPU 具有天然的兼容性，能够充分利用 GPU Direct 技术，实现 GPU 到 GPU 的直接数据传输，避免 CPU 和内存的参与，显著提升性能。

尽管 InfiniBand 在低延迟方面具有显著优势，但其在超大规模组网中面临严重的扩展性瓶颈。最核心的限制来自于交换机端口容量的约束。NVIDIA Quantum-2 系列交换机由 64 个每秒 400Gb 的连接埠或 128 个每秒 200Gb 连接埠组成，使用 32 个实体八进位小型插入式（OSFP）连接器。这一端口容量限制直接影响了集群的扩展能力。由于 Quantum-2 交换机的端口容量较低，在一个拥有十万节点的集群中，完全互联的 GPU 数量最多只能达到 65,536 个 H100[^20]。这意味着十万卡集群无法通过传统的 3 层 Clos 网络架构实现完全互联，必须采用更为复杂的 4 层架构。这种架构设计虽然解决了连接数量的问题，但带来了成本的显著增加。这一成本增加主要来自以下几个方面：交换机数量的增加：4 层架构需要更多的各级交换机，不仅增加了设备采购成本，还增加了机房空间占用和电力消耗。光模块数量的增加：4 层架构比 3 层架构需要更多的收发器，这直接导致光模块成本增加约 33%。布线复杂度的提升：4 层架构的网络拓扑更为复杂，需要更多的光纤连接和更复杂的布线设计，增加了部署和维护成本。

[^20]: https://m.sohu.com/a/787863726_473283/

为了应对 InfiniBand 在超大规模组网中的局限性，业界提出了多种替代方案，主要包括以太网 RoCE（RDMA over Converged Ethernet）和英伟达 Spectrum-X 以太网方案。以太网 RoCE 方案的优势在于其开放性和成本效益。RoCE 利用标准的以太网硬件实现 RDMA 功能，避免了 InfiniBand 的专用硬件需求，显著降低了成本。Meta 等公司已成功部署基于 RoCEv2 的大规模 AI 网络，其 24K H100 GPU 集群采用单平面 Spine-Leaf 架构，实现了良好的性能和成本平衡。英伟达 Spectrum-X 以太网方案则提供了另一种选择。Spectrum-X 以太网的每个 SN5600 交换机有 128 个 400G 端口，而 InfiniBand NDR Quantum-2 交换机只有 64 个 400G 端口。这意味着采用 Spectrum-X 可以使用 3 层交换机网络而不是 4 层，4 层交换机网络比 3 层交换机网络要多出 1.33 倍的光模块。

## 四、计算效率

### 4.1 万卡建设计算效率现状与提升策略

#### 4.1.1 全球万卡建设规模

华为昇腾集群作为业界首个万卡级 AI 集群，于 2023 年 7 月从 4000 卡扩展至 16000 卡，实现了历史性突破。Meta 自 2022 年起公开其 AI 基础设施，率先推出 Research SuperCluster (RSC)，该集群由 16,000 个 A100 GPU 组成，而后发布第二代 GPU 集群，每个集群 24,576 张 H100 GPU[^21]。Google 全新 A3 虚拟机，搭载 26000 块 Nvidia H100 GPU，并构建 TPUv5p 8960 卡集群[^22]。xAI Colossus 超级计算机由马斯克的 xAI 公司建设，位于田纳西州孟菲斯市，达到了 10 万颗 NVIDIA Hopper GPU 的规模，该集群采用超威电脑（Supermicro）的服务器，基于 NVIDIA HGX H100 方案，每个服务器配备 8 个 H100 GPU，封装在 4U 通用 GPU 液冷系统内[^23]。字节跳动提出了万卡集群大模型训练架构 MegaScale，并在 12288 个 GPU 上训练一个 175B LLM 模型，实现了 55.2%的 MFU[^24]。

[^21]: [打破 AI 算力天花板，Meta 超大规模 AI 基础设施架构解读 - 知乎](https://zhuanlan.zhihu.com/p/696241154#:~:text=%E8%AF%A5%E8%AE%BE%E8%AE%A1%E4%B8%93%E6%B3%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E3%80%81%E7%BD%91%E7%BB%9C%E5%92%8C%E5%AD%98%E5%82%A8%E7%9A%84%E6%97%A0%E7%BC%9D%E9%9B%86%E6%88%90%EF%BC%8C%E6%97%A8%E5%9C%A8%E6%8E%A8%E5%8A%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E3%80%82%20Meta%20%E8%87%AA%202022%20%E5%B9%B4%E8%B5%B7%E5%85%AC%E5%BC%80%E5%85%B6%E5%BC%BA%E5%A4%A7%E7%9A%84%20AI%20%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%EF%BC%8C%E7%8E%87%E5%85%88%E6%8E%A8%E5%87%BA%20Research,GPU%20%E7%BB%84%E6%88%90%E3%80%82%20RSC%20%E4%B8%BA%20Meta%20%E7%9A%84%20AI%20%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BC%80%E5%8F%91%E6%8F%90%E4%BE%9B%E4%BA%86%E6%97%A0%E4%B8%8E%E4%BC%A6%E6%AF%94%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B%E3%80%82)

[^22]: https://blog.csdn.net/njbaige/article/details/143028415

[^23]: https://news.mydrivers.com/1/1011/1011073.htm

[^24]: MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs（https://arxiv.org/pdf/2402.15627.pdf）

#### 4.1.2 MFU 指标定义与当前水平

模型 FLOPS 利用率（MFU）定义为训练时获得的有效 GPU 计算 FLOPS（training FLOPS per GPU）与该 GPU 的峰值 FLOPS（peak FLOPS）的比值，是衡量算力集群有效吞吐量和利用率的核心指标[^25]。

[^25]: https://zhuanlan.zhihu.com/p/688248925

当前万卡集群的 MFU 水平呈现出明显的规模效应递减特征。在 H100 集群的万亿参数训练中，8 位浮点运算实现了 35%的 MFU，16 位浮点运算实现了 40%的 MFU[^26]。这意味着即使在最先进的硬件平台上，仍有 60-65%的算力资源未能被有效利用。

[^26]: https://blog.csdn.net/sinat_37574187/article/details/140636241

不同规模集群的 MFU 表现存在显著差异。以 OPT-175B 模型为例，在 992 个 80GB A100 GPU 上训练时，每个 GPU 达到 147 TFLOP/s，对应的 MFU 约为 47%。国产 NPU 集群通过自动分布式策略推荐、流水线并行优化、overlap 优化和全链路 profiling 等技术，在 16384 个加速卡上实现了 405B 参数大模型的预训练，MFU 达到 45.13%，较基准性能提升 10%以上[^27]。百度百舸 4.0 在训练主流开源模型时，集群 MFU 可提升至 58%，有效训练率达到 98%[^28]。这表明通过系统性的优化，MFU 仍有较大的提升空间。

[^27]: 基于国产 NPU 的超万卡智算集群大模型训练调优实践

[^28]: https://www.infoobs.com/article/20250208/68414.html

#### **4.1.3 万卡集群效率损失的根本原因**

万卡集群效率损失的根本原因是多方面的，涉及通信瓶颈、调度开销、负载不均衡、内存管理等多个维度的复杂交互。

通信延迟是最主要的效率瓶颈。在万卡集群中，GPU 之间需要频繁进行数据交换，包括梯度同步、参数更新、中间结果传递等。由于网络拓扑的复杂性和距离的增加，通信延迟呈指数级增长。特别是在使用 InfiniBand 网络时，虽然理论带宽很高，但实际延迟仍然是一个严重问题。在十万卡集群中，即使每个网卡到叶交换机链路的平均故障时间为 5 年，由于光模块数量众多，在一个全新的工作集群上发生第一次作业故障也只需要 26.28 分钟[^29]。

[^29]: 10 万卡 H100 集群的尽头……

MoE 模型的特殊挑战进一步加剧了效率损失。MoE 架构的"小专家"设计虽然理论上可以节省计算资源，但前提是能灵活高效地选择和调度，这对通信带宽提出了极高要求。在传统 AI 集群中，GPU 之间通过 PCIe 或 InfiniBand 通信，激活路由涉及大量跨卡、跨节点访问，导致在 All-to-All 通信模式下，MoE 架构的通信压力远大于 Dense 架构，训练速度甚至比 Dense 更慢[^30]。

[^30]: https://developer.aliyun.com/article/1663723

调度开销在万卡规模下变得不容忽视。随着集群规模的增大，调度决策的复杂度呈指数级增长。Kubernetes 默认调度器需要遍历所有节点，筛选出剩余 GPU 数量满足请求的候选节点，然后根据节点负载、GPU 型号匹配度等因素计算得分，最后选择最优节点[^31]。这个过程在万卡集群中可能需要数秒甚至更长时间，严重影响了作业的启动速度。

[^31]: https://g.pconline.com.cn/x/1983/19837509.html

负载不均衡是另一个重要因素。在 MoE 模型中，不同专家的负载可能存在巨大差异，导致部分 GPU 满载运行，而其他 GPU 处于空闲状态。这种不均衡不仅降低了整体效率，还可能导致梯度同步时的等待时间增加。

内存管理效率低下也是重要原因。在大规模模型训练中，内存管理的复杂性急剧增加。MoE 模型不仅需要保存所有专家的参数（显存占用是稠密模型的 4-8 倍），还面临激活值膨胀问题，显存需求增加 30%以上。此外，频繁的 checkpoint 操作会暂停训练进程，将当前权重保存到持久内存或 CPU 内存中，进一步降低了训练效率。

#### 4.1.4 万卡集群软件适配

在软件框架层面，PyTorch+Megatron 已成为万卡集群训练大模型的主流选择。NVIDIA Megatron-Core 是一个基于 PyTorch 的开源库，专门用于在数千个 GPU 上进行大规模模型训练，提供了先进的模型并行技术，包括张量并行、序列并行、工作流并行、上下文并行和 MoE 专家并行[^32]。Megatron 框架在实际应用中展现出显著的性能优势。即使在单机八卡环境下，Dense 模型训练使用 Megatron 比 transformers 同模型代码的速度提升约 20%，GPU 利用率更高。而在 MoE 模型上，该优势更加明显，加速比可达 1000%或更多[^33]。这种巨大的性能差异源于 Megatron 对 MoE 结构的专门优化，特别是在专家路由、负载均衡和通信优化方面的技术突破。

[^32]: https://developer.nvidia.cn/megatron-core

[^33]: https://developer.aliyun.com/article/1662411

#### 4.1.5 万卡集群调度策略

在万卡规模集群中，传统的 Kubernetes 调度系统面临着前所未有的挑战[^34]。资源碎片化是最突出的问题之一，在千卡级集群中，由于物理卡级分配导致 GPU 利用率不足 30%，碎片率超过 40%[^35]。这是因为显存请求通常小于整卡容量，剩余空间无法被有效利用，造成了严重的资源浪费。

[^34]: K8S <https://kubernetes.io/zh-cn/docs/concepts/architecture/>

[^35]: https://blog.51cto.com/u_17450104/14039631

为应对万卡规模的调度挑战，业界提出了多种创新方案。DeepSeek 创新性地将 Kubernetes 的容器编排能力与 Slurm 的高性能计算作业调度相结合，构建了兼顾灵活性与效率的混合调度系统。该系统采用分层调度架构，K8s 管控在线服务，Slurm 调度离线训练，通过统一资源池实现动态分配[^36]。这种混合架构在实际应用中取得了显著效果：作业排队时间从平均 23 分钟缩短至 5 分钟以内，降幅达 78%；GPU 碎片率从 15%-20%降至 5%以下，降幅达 75%；大规模作业成功率从 83%提升至 99.6%，提升 16%。

[^36]: https://blog.csdn.net/zhibaijiang/article/details/146550288

在调度策略方面，万卡集群需要支持丰富的调度算法，包括 Gang scheduling（组调度）、Capacity scheduling（容量调度）、Topology aware scheduling（拓扑感知调度）和优先级队列等[^37]。这些策略的综合运用能够确保资源在多个任务之间合理分配，避免资源浪费，提高整体效率。

[^37]: https://juejin.cn/post/7326268968959311899

#### 4.1.6 MoE 模型的 All-to-All 通信压力

MoE 模型的 All-to-All 通信是造成万卡集群效率损失的关键技术挑战。与传统大模型集群网络通讯的 All Reduce 模式不同，All-to-All 模式不仅带来更庞大的数据通讯总量（如果将 All Reduce 的网络通讯量视作 N，那 All-to-All 模式的总体通讯量则为 N2），更会让通讯量在集群内的分布变得无法准确预测。MoE 模型单次前向传播的通信量与专家数量呈线性关系，与序列长度和批大小呈乘积关系，这使得在处理长序列和大批量数据时，通信开销急剧增加。

动态路由的挑战进一步加剧了通信压力。在 MoE 模型中，每个 token 需要根据其特征动态路由到相应的专家。这种动态选择机制导致两个关键问题：一是数据分发不均匀，不同专家接收的 token 长度存在差异，需要依赖低效的 AllToAllV 通信；二是元数据同步开销，获取收发信息需要调用前置 AllGather 算子收集路由表，并在 Host 侧完成同步，引入额外通信开销和 Stream 同步延迟[^38]。

[^38]: [【昇腾热点算子大解密】CANN 创新通算融合算子破局 MoE 通信瓶颈，实现推理吞吐 50%提升-技术干货-昇腾社区](https://www.hiascend.com/developer/techArticles/20250726-1)

为应对这些挑战，业界提出了多种优化方案。DeepSeek-V3 通过专家分组路由，将 256 个专家划分为 16 个通信组，有效降低了跨节点数据传输。同时，其双向管道并行技术实现了计算与通信在时间维度上的重叠率达 98%，自适应梯度压缩技术平均节省 40%通信带宽。NetMoE 则通过改变数据的放置模式来减小 AlltoAll 过程中的 inter-connect 开销，尽可能将 AlltoAll 传输过程最大程度限制在 intra-connect 中，从而减小 MoE Layer 的整体延迟[^39]。

[^39]: NETMOE: ACCELERATING MOE TRAINING THROUGH DYNAMIC SAMPLE PLACEMENT

### 4.2 分布式训练策略

#### 4.2.1 字节跳动 MegaScale 的 3D 并行优化技术

字节跳动的 MegaScale 系统代表了万卡集群训练效率优化的最新突破。该系统在 12288 个 GPU 上训练 175B 参数 transformer 模型时，实现了 55.2%的 MFU，比英伟达 Megatron-LM 提高了 1.34 倍。这一成就的取得，得益于 MegaScale 在 3D 并行优化、通信重叠、算子优化等多个维度的系统性创新。

3D 并行架构的深度优化是 MegaScale 的核心技术突破。3D 并行指的是张量并行（Tensor Parallelism）、流水线并行（Pipeline Parallelism）和数据并行（Data Parallelism）的有机结合。

张量并行。它将单个操作符分布在多个设备上，每个设备并行执行计算的一部分。根据特定的分区策略及其与模型中先前和后续操作符的关系，分区可能需要参与的 GPU 之间进行通信，以分割输入然后合并输出。例如，在多个 GPU 之间分割 MLP 和自注意力块中的 GEMM，以利用更多的计算单元。

在数据并行中，MegaScale 优化了两个主要通信操作：All-gather 操作在前向传递期间从其他数据并行等级的工作者获取最新的模型参数；reduce-scatter 操作在后向传递期间收集梯度。

在流水线并行方面，MegaScale 采用了交错 1F1B 调度方法，以实现通信的重叠。它将模型层分布在多个设备上，每个设备拥有模型的一部分。同时，每个训练批次被细分为多个微批次进行流水线执行。为了减少流水线气泡，每个工作器上的每个流水线阶段被细分为多个虚拟阶段，这代表了模型的一个子集，称为模型块。最初，工作器进入热身阶段，执行有限数量的飞行微批次的前向传递。热身之后，每个工作器进入稳定阶段，工作器执行一次前向传递，然后执行一次后向传递，通常缩写为 1F1B。在完成一个批次后，工作器在冷却阶段完成剩余的飞行微批次的后向传递。

#### 4.2.2 摩尔线程夸娥集群的自适应混合并行策略

夸娥（KUAE）智算集群是摩尔线程为应对大模型训练与推理的规模化算力需求所打造的系统级算力解决方案，支持从千卡到万卡级别的灵活部署，单集群可部署超过 1000 个计算节点，每个节点集成 8 颗摩尔线程自研 OAM 模组化形态的 GPU，并通过优化的 3D 全互联拓扑实现了极低的通信延迟，为像 DeepSeek 这样的千亿参数大模型预训练提供稳定且高效的算力支撑[^40]。

[^40]: [国产 GPU 公司摩尔线程“夸娥智算集群”，轻松构建高能效 AI 基础设施 - 知乎](https://zhuanlan.zhihu.com/p/1931112786806673504)

## 五、系统可靠性

### 5.1 硬件故障常态化

当前，全球科技巨头正竞相部署超大规模 AI 集群。英伟达的 DGX SuperPOD 可扩展至 64 个扩展单元，包含 2000 多个 DGX B300 节点[^41]。构建万卡集群并非上万张 GPU 卡简单堆叠那么简单，对于算力集群而言，规模越大，故障率往往越高，Meta 官方就曾透露，在 Meta 的大模型 Llama3.1 训练过程中，其运行的 1.6 万张 GPU 训练集群每 3 小时就会出现一次故障[^42]。

[^41]: https://docs.nvidia.com/dgx-superpod/reference-architecture/scalable-infrastructure-b300/latest/dgx-superpod-architecture.html

[^42]: https://www.pconline.com.cn/focus/1877/18772662.html

由于大规模分布式训练的高度同步性要求，在分布式训练系统中，众多加速器在同步环境中协同工作，任何一个 GPU 服务器组件故障都可能中断或停止训练过程。因此，万卡集群的硬件故障率呈现出显著的规模效应，即随着集群规模的扩大，故障发生的概率呈指数级增长。根据概率学计算，假设单张 GPU 故障率仅为十万分之一，在一张卡出现故障整体都要停止的情况下，万卡集群的总体故障率为 9.5%，而十万卡集群的故障率则飙升至 63.2%。

更为严峻的是实际运行数据。Meta 在训练 Llama 3.1 时使用的包含 16384 个 GPU 的集群，在 54 天训练期间遇到了 419 次意外组件故障，平均每 3 小时发生一次故障。其中 148 次中断由 H100 GPU 故障引起（占 30.1%），72 次由 HBM3 内存故障导致（占 17.2%），GPU 相关故障合计占所有意外中断的 58.7%[^43]。

[^43]: https://hub.baai.ac.cn/view/38942

![](05.png)

图 5 万卡集群硬件故障

万卡集群的硬件故障主要集中在以下几个类型：

GPU 故障是最主要的故障源，占意外中断的 58.7%。常见的故障原因有 Xid、ECC、NVLINK error 和 NCCL error 故障等。对于一个千卡训练作业来说，卡故障导致一天内训练失败的概率高达到 93%。作业失败后，用户需要手动重启作业，运维成本很高。如果用户重启不及时，中间间隔的时间就会导致 GPU 卡空闲，浪费昂贵的算力资源[^44]。

[^44]: https://blog.csdn.net/SOFAStack/article/details/132843619

内存故障，特别是 HBM3 内存故障，是第二大故障源。Meta 的统计显示，HBM3 内存故障占所有意外中断的 17.2%。这类故障通常与 GPU 的高功耗和热应力有关，H100 GPU 高达 700W 的功耗使其在运行过程中承受大量热应力。

网络故障占意外中断的 8.4%。不同于其他由软件引发的故障，网络设备故障大多为硬件故障，例如网络线缆或是网卡光模块等硬件设备出现故障。相较于软件故障，硬件故障的处理难度大、恢复时间长，可能造成更大的算力损失。

其他故障包括 CPU 故障、存储故障、电源故障等。值得注意的是，在 Meta 的 54 天训练期间，只有两个 CPU 发生故障，说明 CPU 的可靠性远高于 GPU。

H100 的算力建设费用约为 10 元/卡/小时，若每次中断恢复需耗时 1 小时，那么万卡集群每次训练因网络故障造成的算力损失约为 350 万元[^45]。

[^45]: https://www.ruijie.com.cn/jszl/929847/

### 5.2 快速恢复机制

Kubernetes 作为容器编排的事实标准，在万卡集群的故障自动恢复中发挥着关键作用，其核心机制是通过节点控制器（node controller）周期性检查所有节点状态，当节点处于 NotReady 状态超过一段时间后，自动驱逐该节点上的所有 Pod。

kube-controller-manager 主要由多个控制器构成，而 eviction 的功能主要由 node controller 这个控制器实现。kube-controller-manager 提供了以下启动参数控制驱逐策略[^46]：

[^46]: https://blog.csdn.net/ygq13572549874/article/details/136310115

pod-eviction-timeout：即当节点宕机该事件间隔后，开始驱逐机制，驱赶宕机节点上的 Pod，默认为 5min；

node-eviction-rate:驱赶速率，由令牌桶流控算法实现，默认为 0.1，即每秒驱赶 0.1 个节点。相当于每隔 10s，清空一个节点；

secondary-node-eviction-rate: 二级驱赶速率，当集群中宕机节点过多时，相应的驱赶速率也降低，默认为 0.01；

unhealthy-zone-threshold：不健康 zone 阈值，会影响什么时候开启二级驱赶速率，默认为 0.55，即当该 zone 中节点宕机数目超过 55%，而认为该 zone 不健康；

large-cluster-size-threshold:大集群法制，当该 zone 的节点多余该阈值时，则认为该 zone 是一个大集群。大集群节点宕机数目超过 55%时，则将驱赶速率降为 0.0.1，假如是小集群，则将速率直接降为 0。

全栈运行时打点技术是一种革命性的故障诊断和性能优化技术，通过在系统的各个层面插入轻量级的监控探针，实现对训练过程的全方位、实时观测。基于全栈打点技术，百度百舸将隐式故障的平均检测时间从分钟级缩短至秒级，诊断准确率提升 40%以上[^47]。

[^47]: https://www.51cto.com/article/810383.html

![](06.png)

图 6 百度百舸集群训练稳定性全景图

秒级切换备份节点是快速恢复机制的重要组成部分，其目标是在故障发生后最短时间内启用备份资源，确保训练任务的连续性。华为的秒级恢复机制采用了分级策略：进程级重调度恢复通过参数面网络将临终 Checkpoint 传递到备用节点，完成参数状态恢复后继续训练，能够将训练恢复时间缩短到 3 分钟以内；进程级在线恢复针对硬件 UCE 故障，通过业务面昇腾 CANN 软件、框架软件、MindCluster 软件配合实现故障地址在线修复，进一步将训练恢复时间缩短到 30 秒以内[^48]。

[^48]: https://www.toutiao.com/article/7513927925666824719/?upstream_biz=doubao&source=m_redirect

## 六、国产化与运营成本

### 6.1 国产化挑战：生态与性能的双重差距

#### 6.1.1 GPU 供应受限

自 2022 年起，美国政府以"国家安全"为名，四次升级对华出口管制，从禁止 A100、H100 等高端芯片出售，到掐断 14 纳米以下技术的零部件供应，再到 2025 年加码限制 AI 芯片用于中国模型训练。2025 年 4 月，美国政府进一步收紧对华 AI 芯片出口限制，直接将英伟达 H100、A100 等旗舰型号纳入禁止出口清单，彻底切断了其高端芯片的对华供应。2025 年 5 月 12 日，美国商务部工业与安全局（BIS）发布《AI 芯片出口管制指南》，不仅细化了算力限制标准，还要求向中国出口特供版芯片需抽取 15%的“技术税”，且必须保留远程监控功能[^49]。

[^49]: https://www.163.com/dy/article/KC5IFQ0105566N7W.html

面对 GPU 供应受限，中国加快了国产 AI 芯片的研发和产业化进程。目前，华为昇腾、寒武纪等多家企业已推出多款国产 NPU 产品，但与英伟达 GPU 相比在算力上仍存在代差。例如，英伟达 B300（Blackwell 架构）在 FP16 标准下的算力约为 3840 TFLOPS（FP4 约 15 PFLOPS），配备 288GB HBM3e 内存（带宽 8 TB/s）；华为昇腾 910C 在 FP16 标准下的算力为 800 TFLOPS。英伟达 GPU 的 HBM 内存容量与带宽领先。例如，H100 配备 80GB HBM3 内存（带宽 3 TB/s），B300 升级至 288GB HBM3e（带宽 8 TB/s）。华为 NPU 通过自研 HBM（如昇腾 950 的 HiBL 1.0）弥补制程差距。例如，昇腾 950PR 配备 128GB HBM（带宽 1.6 TB/s），昇腾 950DT 升级至 144GB HBM（带宽 4 TB/s），但整体容量与带宽仍落后于英伟达。DeepSeek 团队的实测数据显示，华为昇腾 910C 在 AI 推理中的表现达到英伟达 H100 芯片的 60%左右。在训练场景下，昇腾 910B 的训练效率接近 A100 的 90%。

在 AI 集群系统中，互联带宽和显存容量是决定系统性能的关键因素。国产 NPU 在这两个核心指标上与国际先进产品存在明显的代际差距，严重影响了万卡集群的整体性能。在互联技术方面，英伟达凭借其成熟的 NVLink 技术占据领先地位。英伟达 GB200 采用第五代 NVLink，带宽达到 1800GB/s，相较于第四代 NVLink 的 900GB/s 实现翻倍。在集群层面，英伟达 NVL72 系统通过 NVLink 4.0（单 GPU 900GB/s）结合 InfiniBand（HDR 200Gbps），总带宽约 14.4TB/s。相比之下，国产 NPU 在互联技术上采用了不同的技术路线。华为昇腾 384 超节点采用 MatrixLink 全光互联技术，总带宽达 1229TB/s，芯片间时延 150 纳秒，而英伟达 NVL72 采用 NVLink 铜缆互联，总带宽 130TB/s，GPU 间时延微秒级。在显存容量方面，差距更为明显。英伟达 H100 配备 80GB HBM3 内存，带宽 3TB/s；最新的 B200 更是升级至 288GB HBM3e，带宽高达 8TB/s。这种大容量高带宽的显存配置对于训练大规模模型至关重要，GPT-4 级别的模型仅权重和优化器就需要 10.8TB 的内存。国产 NPU 在显存技术上虽有进步，但仍落后一代以上。华为昇腾 950 系列中，950PR 配备 128GB HBM（带宽 1.6TB/s），950DT 升级至 144GB HBM（带宽 4TB/s）。寒武纪思元系列的显存配置更低，如 MLU370-X8 配备 48GB GDDR6 显存。这种显存容量的差距直接限制了国产 NPU 在训练超大模型时的能力。显存技术的差距不仅体现在容量上，还体现在技术架构上。英伟达的 GPU 支持统一内存架构，可以实现 CPU 和 GPU 内存的统一管理，而国产 NPU 多采用分离式内存设计，在数据传输效率上存在劣势。此外，在 HBM 技术方面，国产厂商虽有突破，但在产能、良率和成本控制方面仍面临挑战。

在 MFU 指标上，国产万卡集群与国际先进水平仍存在差距。2025 年 5 月发布的华为盘古 Ultra MoE 模型，参数规模达 7180 亿，具备 256 个路由专家，每个任务激活 8 个专家协同工作，在昇腾 CloudMatrix 384 超节点集群上，依据优化算子执行序和内存管理策略，算力利用率（MFU）从 30%提升至 41%。然而，这一水平与国际先进水平仍有差距。国际领先的 H100 集群在万亿次参数训练中，8 位浮点运算可实现 35%的 MFU，16 位浮点运算可达 40%的 MFU。部分优化较好的集群，MFU 目标可达 60%，周均训练有效率最高 99%。

#### 6.1.2 软件生态割裂

CUDA 生态系统在 AI 计算领域的垄断地位是国产替代面临的最大挑战之一。英伟达通过数十年的持续投入，构建了一个涵盖硬件、软件、工具链和开发者社区的完整生态系统，形成了极高的技术壁垒。CUDA 生态的垄断地位体现在多个方面。首先是市场份额的绝对优势。全球 90%的 AI 框架（包括 PyTorch、TensorFlow 等主流框架）依赖 CUDA 接口。全球超 500 万开发者依赖 CUDA 开发的代码库，涵盖科学计算、深度学习、自动驾驶等领域，迁移成本高达数亿美元量级。软件和生态成本是国产替代面临的最大隐性成本。虽然国产 NPU 在硬件性能上已接近国际水平，但在软件生态方面的差距导致了高昂的迁移成本。72%尝试用国产算力卡替换英伟达的机构，在 3 个月内仍受困于效率问题，平均需额外投入约每人每天 120 元的调试成本[^50]。

[^50]: https://m.163.com/dy/article/K9OP29TO05118E4U.html

### 6.2 成本与运营：千亿级投资的商业风险

#### 6.2.1 天量资本支出

软件生态割裂导致高昂迁移成本。CUDA 生态的垄断地位难以撼动，全球 90%的 AI 框架依赖 CUDA 接口，500 万开发者构建了强大的生态壁垒。国产 NPU 虽推出兼容层技术，但迁移成本依然高昂：华为昇腾需重写 40%代码，迁移周期 3 个月，成本约 2000 万元；壁仞 BR100 核心算法重构比例达 35%。算子适配成本普遍增加 5-10%，实际项目中可能更高。

天量投资与低利用率形成尖锐矛盾。10 万 H100 集群总投资 40-60 亿美元，其中硬件成本约 25 亿美元，年电力成本 1.24 亿美元。国产替代方案因单卡算力低、需更多芯片，相同算力下总成本并不占优势。更为严峻的是，全国智算中心整体利用率仅 32%，部分国产算力闲置率高达 70-80%，河南万卡集群利用率仅 40%。

#### 6.2.2 利用率与空转风险

政策支持力度空前但效果有待观察。政府推出系列支持政策，2025 年新型工业化专项债超三成投向算力基础设施，中央财政专项扶持资金 860 亿元。但政策效果的显现需要时间，特别是在生态建设、人才培养等方面，短期内难以弥补与国际先进水平的差距。
# test


## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=114988018571687&bvid=BV1DktBzLEvb&cid=31551195779&p=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>

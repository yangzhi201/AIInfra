<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->

# 02.AI 万卡集群建设方案

Author by: staries wang

## 一、万卡集群整体建设方案

（1）规划设计：明确业务需求，设计网络、计算、存储架构，并制定LO/L1基建部署方案

（2）硬件安装:按规划安装 IT 服务器和网络设备，进行物理部署与线缆连接联调

（3）软件安装：部署通算操作系统和AI基础软件，包括集群管理软件，配置调度系统及作业平台

（4）验收运维：验证性能功能稳定性，正式交付后转移运维主题，持续监控维护更新优化

## 二、万卡集群机房布局

### 2.1 整体布局

单层集中式布局在万卡AI集群中具有显著优势，主要体现在通信延迟控制和运维效率提升方面。在万卡甚至更大规模的场景中，必须将所有的算力节点到网络核心节点的通信延迟控制在一个稳定的波动范围之内，这要求从传统并行布局改成以网络为核心的向心布局[^1]。此外，单层集中式布局还能够简化基础设施的复杂性。通过将所有核心设备集中在同一层，可以避免垂直布线的复杂性，减少电缆长度和信号衰减，从而提升整体系统的可靠性和性能。同时，单层布局便于实施统一的制冷和供电策略，有利于提升能效比。

多层数据中心通过在相同土地面积上容纳更多服务器机房空间来节省土地成本，带来明显的经济效益。此外，多层设施通过降低服务器机房空间与暴露在直射阳光下的建筑物表面之间的比率来提高冷却系统的效率[^2]。在超大规模集群中，由于机房空间、供电等基础设施限制以及单栋建筑体量限制，会在单层集中式布局的基础上采用同园区跨楼宇部署的多栋布局策略。

### 2.2 功能分区

万卡AI集群在功能分区上采用核心设备居中部署、辅助区域环绕四周的策略。核心设备区部署服务器集群与存储阵列，这一区域是整个集群的计算核心，需要确保设备的高密度部署同时满足散热和维护需求。网络通信区设置双层网络机柜，下层部署交换机，上层安装光纤终端盒，配置理线架与光纤槽道。网络设备的部署遵循\"核心-汇聚-接入\"层级，核心交换机安装于机柜中部1/3高度处，避免重心偏移[^3]。监控管理区集成KVM控制台、环境监控主机与安防系统，这一区域负责整个集群的运行监控和管理，需要具备良好的可视性和操作便利性。辅助设备区包含UPS间、精密空调机房、消防气瓶间，这些设备为核心计算设备提供电力保障、环境控制和安全防护[^4]。

### 2.3 机柜扩容

万卡AI集群的扩展空间预留是确保系统长期可持续发展的关键设计要素，通常需要预留20%-30%的电力、机柜空间与网络带宽[^5]。模块化设计是实现灵活扩展的重要技术手段。通过将机房基础设施分解为多个独立的模块化单元，每个单元具有标准化的接口和规范，能够通过简单的插拔或配置调整实现功能扩展。

### 2.4 机柜部署密度与空间规划

万卡AI集群的机柜部署密度设计需要在空间利用效率和运维便利性之间取得平衡。根据标准42U机柜的配置要求，每台GPU服务器高度约4U（8卡机型通常为2U或4U），单机柜可放置10台（4U×10=40U，预留2U空间用于PDU、交换机等）。机柜布局建议按\"列\"部署，如4列×5柜的方式，列间距≥1.2米便于运维[^6]。同时，机柜与墙面距离应大于0.8
米，避免影响空气流通。这种布局方式既保证了设备的高密度部署，又为日常维护和故障处理提供了充足的操作空间。

百度昆仑芯超节点的实践提供了超高密度部署的创新方案。相比传统的8卡服务器算力交付方式，昆仑芯超节点将64张昆仑芯XPU放到同一个机柜，卡间互联带宽提升8倍，单整机柜训练性能提升了10倍，单卡推理性能提升了13倍[^7]。以典型64卡场景为例，通常需要8台8U的风冷AI服务器，占用64U空间，而昆仑芯超节点通过整机柜方案仅需28U，即16个1U
Compute Tray + 8个1U Switch Tray + 2个2U Power
shelf，机柜空间利用效率提升一倍以上。

### 2.5 供电策略

**左右分区供电设计**

万卡AI集群的供电系统采用左右分区就近供电的架构设计，这一策略能够有效避免跨层布线，提高供电效率和可靠性。列头柜形式的配电系统是典型的放射式供配电系统，总配电柜直接供电给列头柜和负载。

目前常见的机房末端配电方案为"列头柜+电缆"，即在机房内每一列机架的始端设置两个列头柜（互为主备）置于每列机架的始端，背靠背安装。在列头柜内设置固定安装的断路器输出分路，从列头柜到机架PDU采用双回路供电[^8]。这种设计确保了供电的冗余性和可靠性，任何一路电源故障都不会影响设备的正常运行。

**高压直流供电技术**

随着单机柜功率密度的不断提升，传统的交流供电方案面临着电流承载能力的瓶颈。为缓解配电干线与列间配电损耗、支撑\>100kW/机架的高密度需求，业界逐渐采用380--800V直流电源干线进入机房，至机架侧再降压到48V/12V[^9]的方案。

英伟达提出了适配300kW以上单机柜容量的800VDC直流架构，采用800V母线槽并将415V交流电转为800V直流电，相同导体尺寸下传输功率可提升85%[^10]，而后将800V直流电分配至IT机架，采用外置DC/DC转化为12V直流电给GPU供电。这种架构不仅提高了电力传输效率，还能够显著减少配电系统的占地面积，为高密度机房的布局设计提供了更大的灵活性。全球科技巨头如META及Google也在计划部署±400V直流供电的Sidecar解决方案[^11]，采用480V
AC输入边柜，通过大功率PSU转化为±400V
DC，直接为IT机柜供电，边柜方案配备BBU[^12]。

### 2.6 制冷管理

**冷热通道隔离技术**

万卡AI集群的散热系统设计采用冷热通道隔离技术，这是确保高密GPU集群稳定运行的关键技术措施。通过采用背靠背机柜布局，构建冷热通道隔离，能够有效提高制冷效率，降低能耗[^13]。

冷热通道隔离的实施需要配合一系列优化措施。首先是风道优化，在采用液冷前必须优化风道以最大化自然对流与强制风冷的协同效应，实施盲板封堵消除机柜内无效空间漏风，部署垂直导流罩引导气流精准覆盖DCU散热鳍片。

服务器的布局设计采用前进后出风流结构，整齐排列服务器朝向，让冷气集中吹入进气侧、热气集中排出，确保GPU风道通畅，形成高效的冷热循环系统。使用高静压轴承风扇，减少噪音并提高气流穿透力。鳍片材质与间距设计采用铝与铜混合鳍片可提升导热效率，导热材料通常选择相变材料或液态金属，提升热传导效果。部分设计加入背板铜导热块，有效导出主板背面热量[^14]。

**混合散热架构**

为了充分发挥液冷和风冷各自的优势，万卡AI集群越来越多地采用混合散热架构。这种架构针对不同的应用场景选择适合的散热方式，CPU及XPU采用液冷散热，因为这些核心器件的功耗密度最高，对散热要求最为严格。而网卡、内存、SSD等相对低功耗的器件采用风冷散热，既能满足散热需求，又能降低系统复杂度和成本[^15]。类似这种多级散热技术的组合（如风冷-液冷混合）可兼顾成本与散热效率，风冷适用于低功耗节点，液冷（直接或间接）则用于高功率密度场景，如HPC集群中的GPU节点。通过优化散热片、热管、相变材料等散热元件的结构与材料，提升其导热系数与散热效率，能够进一步提高整个散热系统的性能。

## 三、万卡集群机房布线

### 3.1 综合布线设计

万卡AI集群的综合布线系统设计采用按楼层划分光纤区域的策略，这一设计能够有效避免跨区布线混乱，提升维护效率。光缆穿层路径需要实现最短化，避免绕线，降低延迟和损耗。此外，万卡AI集群利用柜间空间设置上下两层桥架，满足高密度布线与冗余部署需求，实现线缆的分层管理和路由优化。

### 3.2 布线方式

在万卡集群中，布线方式至关重要。不同的布线方式会影响集群的能效、可靠性和可维护性。常见的布线方式有EOR（End
of Row）、TOR（Top of Rack）和MOR（Middle of
Row）等。下面将对这几种布线方式进行详细介绍[^16][^17]。

**EOR（End of Row）布线方式**

EOR是最传统的数据中心接入交换机集成方法。在EOR架构中，接入交换机集中安装在一排机柜末端的机柜（开关柜）中，设备柜内的主机、服务器和小型机通过水平电缆通过永久链路连接。通常，从服务器机柜到网络机柜的布线施工在服务器和接入交换机安装之前就已经完成，在服务器机柜和网络机柜内进行设备（服务器/交换机）安装和电缆连接。这种方式的优点是简单、可靠，适用于中小规模的数据中心。但是，由于线缆长度较长，能效和可维护性可能会受到影响。

**MOR（Middle of Row）布线方式**

MOR布线方式是对EOR布线方式的改进。主要区别在于排头柜的位置。在MOR架构中，排头柜放置在每排柜子的中间。MOR网络机柜部署在POD的两排机柜中间，可以减少服务器机柜到网络机柜的线缆距离，简化线缆管理和维护。

**TOR（Top of Rack）布线方式**

TOR也称为架顶式接线方式，是对EOR/MOR方式的扩展，适用于大规模的数据中心。在这种方式中，每个服务器机柜的上端部署1-2台接入交换机，服务器通过跳线接入到机柜内的交换机上。交换机上行端口通过铜缆或光纤接入到EOR/MOR的网络机柜中的汇聚交换机上。这种方式的优点是简化了服务器机柜与网络机柜间的布线，提高了能效和可维护性。同时，机柜中服务器的密度较高。

**MPT/MPO布线**

MPO（Multi-fiber
Push--On）布线指采用MPO多芯光纤连接器的布线方式，单根线缆可集成12芯、24芯甚至48芯光纤，能一次性完成多芯光纤的连接。它的核心是通过高密度的光纤集成，解决海量高速传输场景下的布线空间和效率问题[^18]。

MPT（Multi-fiber
Termination）布线，是MPO的兼容型替代方案，连接器机械结构、芯数规格与MPO一致，核心差异是端接工艺更简化，成本更易控制，可与MPO连接器直接互配使用。MTP布线可用于40G-40G、100G-100G、200G-200G、400G-400G的直连，也能用于升级与上行连接[^19]。

MTP布线系列产品种类繁多，可满足不同应用需要，其中包括MTP跳线、MTP配线盒、MTP-LC分支线缆。MTP主干跳线由一根光缆与两端的连接器组成，可以用来连接光模块形成一条完整的链路，通常可容纳8、12、16、24、32、48甚至72芯光纤，充分满足了高密度布线的要求。MTP分支跳线有单模与多模之分，传输距离可从几米延伸到更长距离。MTP转换跳线与MTP分支跳线一样采用扇出设计，但两端都接MTP连接器。MTP光纤配线盒为封闭的盒式结构，一般里面有12或24根光纤，可以将主干线缆的光纤分成双工跳线。

MTP结构化布线为网络提供了分层结构，将通过汇聚层提供多条连接方案，可减少线缆杂乱的问题。随着40G/100G/200G/400G数据中心网络的部署，高速率与高密度已成趋势，MTP布线方案则迎合了这一趋势，以其省时、省空间、省成本的优势，以及优越的稳定性与高密度的特性，为搭建高性能数据中心网络铺平了道路。

在选择布线方式时，需要考虑集群的规模、结构、能效要求、可靠性要求、可维护性要求等多方面因素。此外，还需要遵循一系列的标准和规范。例如，需要遵循国际标准和行业规范，如TIA-942、ISO
21147等；需要采用合适的线缆管理和维护工具，如线缆管理架、标签等，以便对布线进行维护。

### 3.3 线缆类型

AI服务器集群常见的线缆有三种：网线（特指双绞线）、光纤和直连铜缆（Direct
Attach Copper，简称DAC）[^20]。

根据频率和信噪比的不同，常见的网线包括五类线（CAT5）、超五类线（CAT5e）、六类线（CAT6），它们都是两端为RJ45连接器的双绞线，最大传输距离为100米。此外，网线还包括一类线（CAT1）、二类线（CAT2）、三类线（CAT3）、四类线（CAT4）、超六类线（CAT6A）、七类线（CAT7）等。一般来说，类型数字越大、版本越新，技术越先进、带宽也越宽，当然价格也越贵。根据有无屏蔽层，网线又可分为屏蔽双绞线（Shielded
Twisted Pair，STP）和非屏蔽双绞线(Unshielded Twisted
Pair，UTP）。屏蔽双绞线可减少辐射，防止信息被窃听，也可阻止外部电磁干扰的进入。与同类的非屏蔽双绞线相比具有更高的传输速率，但是价格也相对更高，且安装时也更困难。非屏蔽双绞线的优点在于：成本低、重量轻、易弯曲等，且其性能对于一般网络来说影响不大，所以应用相对更为广泛。不过七类双绞线除外，因为要实现全双工10Gbps的速率传输，所以只能采用屏蔽双绞线，而没有非屏蔽的七类双绞线。

按光传输模式的不同，光纤可以分为多模光纤（MMF，Multi Mode
Fiber）和单模光纤（SMF,Single Mode
Fiber)。多模光纤的纤芯较粗，能传输多种模式的光，但其模式色散较大，并且随着传输距离的增大模式色散会逐渐加重，因此常和多模光模块配合用于短距离低成本的通信传输。根据光纤直径和模式带宽的不同，多模光纤可分为OM1、OM2、OM3、OM4几个等级。常用的多模光纤为G.651标准的光纤，可传输800～900nm、1200～1350nm波长的光。单模光纤的纤芯较细，只能传输一种模式的光，因此模式色散很小，适用于远距离通信。常用的单模光纤为G.652标准的光纤，可传输1260～1360nm、1530～1565nm波长的光。

直连铜缆，或称高速线缆，是一种固定长度、两端有固定连接器的线缆组件。DAC铜缆包含有源（active）和无源（passive）两种，有源DAC铜缆内置了放大器和均衡器，可以提升信号质量，但相对成本较高。大多数情况下，当传输距离小于5米时，可以选择使用无源DAC铜缆，而当传输距离大于5米时，选择有源DAC铜缆。DAC铜缆上的连接器与光模块相比，接口类型相同，但缺少了昂贵的光学激光器和其他电子元件，因此可以大大节省成本和功耗，广泛应用于数据中心网络中的短距离连接。在TOR场景下，DAC铜缆是进行机柜内短距离布线的最佳选择。在EOR场景下，如果传输距离小于10米，也可以选择使用DAC铜缆。

## 四、万卡机房配电制冷架构

### 4.1 传统双路冗余供电方案的局限性

传统数据中心普遍采用双路冗余供电架构，通常采用2N配置，即两条市电输入来自不同的变压器，两个独立的UPS系统分别接在两路市电上，同时给负载供电[^21]。这种架构虽然可靠性极高，能够实现99.99%以上的可用性，但其在万卡集群场景下暴露出诸多局限性。

首先是初期投资巨大。以2000个机柜规模的数据中心为例，单柜功率为8kW时，采用2N架构需要的UPS容量为38.4MW，按照当前国内普遍使用的600kW
UPS计算，需要960台UPS设备[^22]。这不仅意味着巨额的设备采购成本，还需要大量的机房空间来部署这些设备。

其次是运维复杂度高。双路冗余系统涉及大量的开关设备、UPS模块、配电线路等，系统架构复杂，故障点多。据统计，采用集中式供电架构的数据中心，其供电系统故障率约为分布式架构的1.5倍。同时，复杂的系统也对运维人员的技术水平提出了更高要求，增加了人力成本[^23]。

第三是空间占用严重。传统UPS系统通常需要独立的UPS室和电池室，这些辅助设施占据了大量的机房面积。根据规范要求，辅助区面积宜为主机房面积的0.2-1倍，这造成了土地资源浪费。

### **4.2单路供电技术原理与架构设计**

单路供电方案的核心思想是采用一路高质量市电直供，配合UPS或储能系统，省去冗余供电线路，从而降低初期投资和运维复杂度。这种方案特别适用于对可靠性要求不是极端苛刻的AI训练场景，如非关键业务的模型训练、推理服务等。

单路供电系统通过采用高质量的市电输入，配合先进的电源质量监测和保护装置，确保供电的稳定性和可靠性。系统架构相对简单，主要包括市电引入、变压器、UPS系统、配电系统等几个部分。与传统双路冗余系统相比，单路供电系统减少了一路市电引入、一套UPS系统以及相应的配电线路和开关设备。

在具体设计上，单路供电系统需要特别注意以下几个方面：首先是市电质量的保证，应选择电力供应稳定、电压波动小、停电频率低的优质市电接入点。其次是UPS系统的配置，应根据负载容量和后备时间要求，选择合适容量和类型的UPS，通常建议UPS容量为IT负载的1.2-1.5倍[^24]。第三是配电系统的设计，应采用放射式与树干式相结合的配电方式，确保配电的可靠性和灵活性。

### 4.3 UPS与储能系统配置方案

在单路供电架构中，UPS和储能系统是保证供电连续性的关键设备。根据不同的应用需求和投资预算，可以选择不同的配置方案。对于标准配置，建议采用高频模块化UPS系统，容量按照IT负载的1.2倍配置。以10万卡H100集群为例，假设单卡功耗700W，10万卡总功耗为70MW，考虑到其他设备的功耗，总IT负载约为100MW，因此需要配置120MW的UPS容量。可以选择200台600kW的模块化UPS，采用N+1冗余配置，即199台工作，1台备用。

在电池配置方面，建议采用高压直流锂电池组，电压等级为384V，后备时间不少于15分钟。电池容量的计算公式为：电池容量（Ah）=（UPS额定输出电流
× 后备时间）/（电池放电效率 × 电池串联组数 ×
电池单体平均放电电压）。以120MW
UPS系统为例，假设后备时间15分钟，需要配置约4000kWh的电池容量[^25]。

对于更高要求的场景，可以考虑配置储能系统作为UPS的补充或替代。例如，特斯拉Megapack可存储高达3.9MWh的电能，能够为超算中心提供强大的电力缓冲[^26]。储能系统不仅可以提供后备电力，还可以参与电网调峰、需求响应等，获得额外的收益。

在系统集成方面，可以采用智能化的能源管理系统，实现对UPS、储能系统、市电等多种电源的统一管理和调度。系统应具备实时监测、故障诊断、自动切换等功能，确保在各种情况下都能保证供电的连续性和稳定性。

### 4.4 选址策略与配电架构适配性

选址是影响万卡机房配电架构设计的重要因素，特别是在采用单路供电方案时，选址的重要性更加突出。理想的选址需要考虑以下几个因素：首先是电网容量，应选择电网容量充足、能够满足万卡集群电力需求的地区。其次是电力可靠性，虽然采用单路供电，但仍需要保证市电的基本可靠性，可以选择年停电时间少于2小时的地区。第三是电力接入条件，应选择便于高压接入、有充足变电站容量的地区。第四是未来扩展性，应预留充足的土地和电力容量，满足未来扩容需求。

在电力资源方面，西北地区具有显著优势。截至2024年12月底，新疆、青海、甘肃、宁夏、陕西5省电力总装机规模分别为19207万千瓦、6982万千瓦、9993万千瓦、7511万千瓦、11679万千瓦，其中青海新能源装机占比最高，达70.61%[^27]。这些地区不仅电力资源丰富，而且电价低廉，新疆2024年新能源市场化交易均价仅为190.98
元/兆瓦时（约0.19元/度）。西南地区同样具有良好的电力资源优势。贵州省作为\"中国数谷\"核心区，拥有丰富的水电资源，工商业电价约0.35-0.45
元/度，远低于东部地区的0.6-1
元/度[^28]。四川、云南等省份的水电资源也非常丰富，四川水电均价约0.113-0.133
元/千瓦时，云南光伏、风电机制电价约0.33 元/千瓦时[^29]。

气候条件是影响制冷成本的关键因素。贵州属于亚热带湿润季风气候，年均气温15℃左右，一年中有8个月可以用自然风冷却机房，空调使用率降低60%。内蒙古呼和浩特冬季寒冷，可通过热交换器把室外冷空气引入机房，零下20℃时甚至能关闭空调，每年空调电费能省30%。这些气候优势可以显著降低制冷成本，提高数据中心的整体能效。

### 4.5 传统风冷方案的散热瓶颈

传统数据中心普遍采用风冷散热方案，通过精密空调系统向机房内送入冷风，带走IT设备产生的热量。然而，这种方案在面对万卡集群的高功耗密度时已经达到了技术极限。风冷系统的功率密度限制是其最大的瓶颈。传统风冷系统的单机柜功率密度通常限制在20kW以下，部分优化设计可以达到30kW，但难以满足AI服务器日益增长的散热需求。当单机柜功率超过20kW时，风冷系统会出现明显的散热不足，导致局部热点和设备过热。其次是能效低下。传统风冷数据中心的PUE（电源使用效率）普遍在1.5-2.0之间，部分甚至更高。这意味着制冷系统消耗了大量的电力，增加了运营成本。特别是在炎热地区，空调系统的能耗占比可高达40%-50%[^30]。第三是空间占用大。风冷系统需要复杂的风道设计，包括架空地板、送风管道、回风系统等，这些设施占据了大量的机房空间。同时，为了保证送风效果，机房净高通常要求在3.5米以上，进一步增加了建筑成本。此外，风冷系统的室外机也需要占用大量的室外空间。

### 4.6 液冷技术原理与分类

主动式液冷技术是利用液体作为冷却介质，通过直接接触发热部件来实现高效散热的技术。与传统风冷相比，液体的导热系数约为空气的25倍，比热容约为空气的4000倍，因此具有极高的散热效率。在万卡集群场景下，液冷技术能够直接接触GPU等高热密度部件，有效解决传统风冷的散热瓶颈问题。

液冷技术主要分为两大类：冷板式液冷和浸没式液冷。冷板式液冷是将金属冷板（通常为铜或铝）直接贴附在CPU、GPU等高热密度芯片上，冷却液在冷板内部流道循环吸热。这种方式结构相对简单，只需要对现有服务器进行部分改造，兼容性好，但散热效率相对较低，通常只能支持40-50kW/柜的功率密度。

浸没式液冷则是将整台服务器或核心部件浸没在绝缘冷却液中，通过液体的对流、沸腾等方式进行散热。根据冷却方式的不同，浸没式液冷又可分为单相浸没和两相浸没。单相浸没是指冷却液在液态下吸收热量，温度升高后通过外部冷却设备降温循环。两相浸没则是利用冷却液的相变原理，液体吸收热量后沸腾变成气体，气体上升到顶部遇到冷凝器后重新液化，形成自然循环[^31]。浸没式液冷的散热效率更高，可支持100kW/柜以上的超高密度。

在冷却液的选择上，主要有去离子水、水-乙二醇混合物、氟化液、矿物油等。去离子水和水-乙二醇混合物主要用于冷板式液冷，成本低、导热性能好，但需要严格的防泄漏措施。氟化液主要用于浸没式液冷，具有绝缘、不燃、化学稳定性好等优点，但成本较高，3M氟化液价格约1.5-3万元/升。矿物油也可用于浸没式液冷，成本相对较低，但需要考虑环保和维护问题。

### 4.7 液冷系统架构设计与关键组件

液冷系统的架构设计需要综合考虑散热需求、可靠性、维护性等多个因素。典型的液冷系统包括冷量分配单元（Cooling
Distribution
Units，CDU）、液冷服务器、管路系统、控制系统等几个主要部分。

CDU是一种基于液冷技术的热管理设备，其核心任务是将热量转移到冷却介质中，然后将冷却介质通过冷却系统散热出去。液冷CDU设备通过一系列管道和泵将冷却介质输送到需要冷却的设备，如服务器、存储器等IT发热器件。当冷却介质流经这些设备时，它会与热源直接接触，吸收并带走产生的热量。

液冷CDU以液体（水、氟化液、矿物油等）为冷却介质，通过循环管路直接或间接接触热源散热。其核心组主要包括泵、热交换器、流量控制阀、温度传感器、液体分配管路[^32]。

根据部署方式的不同，CDU有三种主流部署方式：In-rack、In-row、Side
cooling（Sidecar），它们的核心区别在于CDU的安装位置和散热路径设计不同[^33]，如下表所示。

在系统架构设计上，建议采用模块化设计理念，每个模块包含2个CDU。这种设计具有以下优势：首先是可靠性高，每个模块独立运行，单个CDU故障不会影响整个系统；其次是维护方便，可以在线更换故障部件，无需停机；第三是扩展性好，可以根据需求逐步增加模块，实现平滑扩容。在管路系统设计上，应采用双路冗余设计，确保在单路故障时系统仍能正常运行。控制系统是液冷系统的大脑，负责整个系统的运行监控和自动调节。系统应具备以下功能：实时监测冷却液的温度、压力、流量等参数；根据负载变化自动调节制冷量和流量；具备故障诊断和报警功能；支持远程监控和管理。通过智能化控制，可以实现系统的高效运行和节能。

## **五、光链路脏污问题**

### 5.1 万卡集群光链路污染问题背景

在典型的万卡集群中，每张GPU卡配备200Gbps带宽，极端情况下流量瞬时突发达上千Tbps。以256个GPU组成的GH200集群为例，每颗GPU配备9个800Gbps光模块，通过NVLink
4.0实现900GB/s的双向聚合带宽，整个集群需要2304个800G光模块，GPU与光模块比例达到1:9。在更大规模的10万卡集群中，仅光模块部分的功耗就高达40MW[^34]。

然而，高速光链路的污染问题已成为制约万卡集群可靠性的关键瓶颈。根据海思光电子的数据，AI集群中29%的光链路故障中，64.7%由端面污染引起，仅9.3%为模块本体失效[^35]。更为严重的是，传统光纤链路故障率是铜缆的100倍以上，10万GPU规模的AI集群甚至每6-12小时就会出现链路故障，导致同步性极强的AI训练任务中断[^36]。以行业平均年失效率4‰计算，万卡级集群在训练过程中，平均每3.6天就可能因光模块故障导致中断一次，每次平均恢复需约2小时，仅算力浪费一项，便可能造成每天140万元的经济损失[^37]。

### 5.2 万卡集群光链路污染问题核心难点

#### **5.2.1 脏污难以避免，高速光通信对清洁度要求严苛**

**200GE/400GE技术对噪声容忍度的显著降低**

200GE/400GE高速光通信技术相比传统速率在噪声容忍度方面呈现出指数级下降的特征。根据光通信技术原理，随着传输速率的提升，系统对光信噪比（OSNR）的要求急剧增加。具体而言，100G系统比10G系统的OSNR要求高10dB，比40G系统高4dB[^38]。这种容忍度的降低直接体现在误码率控制上。光模块的纠前误码率阈值通常要求优于10\^-10，而实际应用中，即使是微小的污染也可能导致误码率急剧恶化。此外，光纤的传输容量已经逼近信道的香农极限，频谱效率越高，信号无误码传输需要的信噪比就越高，过高的信噪比需求会导致光传输距离急剧减少。在200GE/400GE系统中，这一效应被进一步放大，使得系统对任何形式的信号干扰都极为敏感。

**微米级断面清洁精度的技术标准**

200GE/400GE光模块对光纤端面清洁度的要求达到了前所未有的严格程度。行业标准要求光接口与光纤对接的同轴偏差需控制在1微米内，仅为人类头发丝直径的1/50[^39]。这一精度要求在实际操作中几乎达到了人工操作的极限。

在端面加工质量方面，技术标准更加严苛。光纤端面粗糙度需控制在Ra0.05μm以内，纤芯同心度偏差不超过±1μm[^40]。对于多芯光纤阵列，要求更为严格，纤芯间距误差需控制在0.1μm量级，端面角度公差需压缩至±0.1°以内[^41]。这些参数的控制精度已经进入了纳米级别，任何微小的污染都可能对光信号传输产生决定性影响。

清洁度的量化标准同样严格。根据IEC61300-2-48清洁度测试标准，端面颗粒污染物需控制在0.5μm以下[^42]。这意味着即使是肉眼无法看见的微小颗粒，也可能对高速光传输造成严重影响。

**机房洁净度与污染概率的量化关系**

机房环境的洁净度直接决定了光链路污染的概率和严重程度。根据国际标准ISO
14644-1，洁净度等级与空气中悬浮粒子浓度存在严格的对应关系。ISO
4级洁净室每立方米空气中0.1μm以上颗粒物不超过10,000个，ISO
8级高达100,000,000个[^43]。

在实际的AI集群中，机房洁净度通常要求达到国标规定的大于等于0.5微米灰尘每升不大于18,000个，相当于英制50万级标准。然而，即使在这样的洁净环境下，光模块污染仍然难以避免[^44]。

**施工过程中的污染风险因素**

在万卡集群的建设和维护过程中，施工环节是光链路污染的主要风险来源。施工过程中的污染风险主要来自以下几个方面：

粉尘污染是最常见的污染源。在机房建设和设备安装过程中，不可避免地会产生各种粉尘，包括建筑材料粉尘、设备包装材料碎屑、人员活动产生的皮屑等。这些粉尘一旦接触到光模块端面，就可能造成永久性的污染。

油污污染同样严重。施工人员的指纹、设备润滑油、清洁剂残留等都可能在光模块端面上形成油污，并在对接时形成污染。

操作污染是另一个重要风险。在光模块的插拔过程中，由于操作不当可能导致端面与其他物体接触，从而造成污染。研究表明，在插拔过程中会加大光模块光口污染的可能性。

**暴露时间与污染概率的关系**

光模块在施工过程中的暴露时间与污染概率呈现出明显的正相关关系。研究表明，光纤切割完成后不宜在空气中暴露过长时间，否则会沾染灰尘或造成二次碰伤影响端面质量。特别是在多尘潮湿的环境中，暴露时间的影响更为显著，切割后的光纤在多尘潮湿环境中暴露后需要重新进行清洁和切割。实际测试数据显示，连接器接头在室内空气中暴露48小时后就会布满微小灰尘颗粒。这意味着，即使是短期的暴露也可能导致严重的污染问题。在万卡集群的大规模部署中，由于涉及大量的光模块和光纤连接，累计的暴露时间可能达到数小时甚至数十小时，这使得污染概率大大增加。

#### 5.2.2 脏污难以识别，监测技术局限

**缺乏有效的在线监测手段**

当前万卡集群光链路系统面临的一个重大技术挑战是缺乏有效的在线监测手段来实时感知污染状态。传统的光链路监测主要依赖光模块的数字诊断功能（DDM），通过监测发射功率、接收功率、温度、电压等参数来评估链路状态。然而，这些参数与光模块端面的实际污染状态之间缺乏直接的线性关系。这是由于污染对光信号的影响是复杂的光学现象，包括光的散射、吸收、反射等多种效应的综合作用。不同类型、不同位置、不同程度的污染对光信号的影响机制各不相同，难以用单一的参数来准确表征。

更为关键的是，现有的光模块监测系统主要关注电层参数，而对光层的物理状态缺乏有效的感知能力。光模块的DOM（数字光学监控）数据虽然可以提供一些参考信息，但这些数据主要反映的是光模块内部器件的工作状态，而非光接口端面的清洁状态。当光链路出现问题时，运维人员往往只能通过间接的方式推断可能的原因，如通过测量接收光功率的变化来推测是否存在污染，但这种方法的准确率很低。

**传统监测参数的局限性**

传统光链路监测系统所依赖的参数存在多重局限性，无法准确反映光模块端面的污染状态，接收光功率监测的局限性尤为明显。虽然接收光功率是最常用的监测参数，但它受到多种因素的影响，包括传输距离、光纤损耗、连接器损耗、温度变化等。当光链路出现污染时，接收光功率的变化可能并不明显，特别是在污染初期或污染程度较轻时。因此，仅通过接收光功率无法准确判断污染的存在和程度。

信噪比（OSNR）监测同样存在问题。虽然OSNR是衡量光信号质量的重要指标，一般要求不低于20dB，但OSNR的测量需要使用专门的光谱分析仪或光信噪比测试仪，且测量过程复杂，无法实现实时在线监测。更为重要的是，OSNR反映的是整个光链路的综合性能，无法定位具体的故障点。

误码率监测虽然可以反映链路的实际传输质量，但也存在明显的滞后性。只有当污染达到一定程度，导致误码率超过FEC纠错能力时，才能被检测到。而在误码率超标之前，污染可能已经存在了相当长的时间，这期间可能已经对AI训练任务造成了潜在的影响。此外，误码率的变化还受到网络流量、温度变化等多种因素的影响，很难将其归因于光模块污染。

**断面污染分布的非均匀性特征**

光模块端面污染的一个重要特征是分布的非均匀性，这使得污染的识别和评估变得更加困难。研究发现，断面中心区比边缘对脏污更敏感，这是因为光信号主要集中在纤芯中心区域传输，中心区域的污染对光信号的影响更为直接和严重。

污染物在光模块端面上的分布具有随机性和复杂性，且污染物类型极为繁杂，包括皮屑、矿物质碎屑、油脂等。这些污染物的大小、形状、光学特性各不相同。例如，皮屑通常呈不规则形状，可能完全遮挡光信号，其不规则边缘还会导致光线偏转；而微小的球形颗粒在连接时可能被压碎，其强度足以损坏光纤内部的玻璃成分。

更为复杂的是，不同类型的污染物对光信号的影响机制存在显著差异。有机污染物（如皮屑、油脂）主要通过吸收和散射影响光信号，而无机污染物（如灰尘、金属颗粒）则可能通过反射、折射等方式影响光传输。这种影响机制的多样性使得单一的监测方法难以全面准确地评估污染状态。

**光学成像检测技术的挑战**

虽然光学成像技术可以直接观察光模块端面的污染情况，但在实际应用中面临诸多挑战。首先，光模块常用的陶瓷或金属材料可能存在微小划痕或凹坑，这些表面缺陷与实际污染物在图像中的表现相似，增加了识别的难度。其次，一些静电吸附的超细纤维或薄膜状污染物可能覆盖在端口表面，但由于其极薄的特性，难以在二维图像中显现出明显边界。这些污染物虽然厚度很薄，但可能对光传输产生严重影响，而传统的光学成像方法却无法有效检测。此外，光学成像检测需要将光模块从系统中取出，这在万卡集群的运行过程中是不现实的。即使采用内窥镜等技术进行在线检测，也会受到光路阻挡、照明条件、成像角度等多种因素的限制，难以获得清晰完整的端面图像。

#### 清洁效果难保证

**传统清洁工具的效果局限性**

在万卡集群光链路的维护过程中，传统清洁工具面临着严重的效果局限性。清洁笔作为一种常用的清洁工具，虽然操作简单，号称可以替代传统的棉签加酒精，只需一按就能去除油污和灰尘，但在实际应用中，其对顽固油污的清洁效果十分有限。酒精棉签的使用同样存在诸多问题。首先是操作风险，酒精浓度的偏差（如使用75%医用酒精而非专用的无水酒精）会导致水分残留，在传感器表面形成结晶盐，造成永久性损伤。其次是材料风险，普通棉签的棉纤维直径约100-200微米，而光模块的纳米级镀膜厚度仅0.1-0.3微米，用棉签擦拭几次后表面就可能出现肉眼可见划痕。更为严重的是，传统清洁方法可能导致二次污染。特别是在清洁过程中，如果操作不当，可能将污染物从一个端面转移到另一个端面，或者在清洁过程中产生新的污染物。

**清洁操作的时间成本分析**

在万卡集群的大规模部署中，光链路清洁的时间成本是一个不可忽视的问题。按照单链路故障清洁4个端面耗时15分钟计算，在一个包含数万个光模块的万卡集群中，如果每个光模块平均每年需要清洁2-3次，仅清洁操作就需要耗费数千小时的人工时间。而在AI训练过程中，光链路的任何中断都可能导致训练任务的回滚，造成巨大的经济损失。一次万卡集群的训练回滚，经济损失可达300万元。因此，在AI训练期间进行光链路清洁维护的风险极高，通常只能在训练任务间隙或系统维护窗口进行，这进一步加剧了清洁工作的时间压力。清洁操作的复杂性还体现在需要严格遵循操作规范。例如，使用酒精清洁时，需要先用无尘布蘸取适量异丙醇溶液，从中心向边缘单方向擦拭，去除表面灰尘和油污；如果初步清洁后仍有污染物残留，需要使用专用的光纤清洁棉棒进行深度清洁[^45]。这些复杂的操作步骤不仅增加了时间成本，也提高了操作失误的风险。

**二次污染问题的严重性**

二次污染是光链路清洁过程中面临的最严重挑战之一，清洁后二次污染率超过40%。二次污染的来源是多方面的。首先是清洁环境的影响。即使在相对洁净的机房环境中，空气中仍然存在大量的悬浮颗粒。清洁后的光模块端面处于高度洁净状态，极易吸附空气中的灰尘。其次是清洁工具和材料的污染。例如，使用质量不合格的无尘布或清洁液，可能在清洁过程中释放纤维或化学物质，造成新的污染。再次是操作过程的污染。在清洁过程中，如果操作人员没有严格遵守操作规程，如没有佩戴专用手套、清洁动作不当、清洁后没有及时盖上防尘帽等，都可能导致二次污染。特别是在大规模的清洁作业中，由于操作人员的疲劳和疏忽，二次污染的概率会大大增加。

**污染传播机制的复杂性**

光链路污染的一个重要特征是污染在光模块与光纤间相互传染。这种传染机制使得污染问题变得更加复杂和难以控制。当一个光模块的端面被污染后，在与其他光模块或光纤连接时，污染物可能被转移到清洁的端面上，造成交叉污染。

污染传播的机制主要包括以下几种：

直接接触传播是最常见的传播方式。当污染的端面与清洁的端面接触时，污染物可能通过物理接触转移到清洁端面上。特别是在插拔过程中，由于接触压力的作用，污染物更容易发生转移。

气溶胶传播也不容忽视。在清洁或操作过程中，污染物可能形成气溶胶悬浮在空气中，然后沉降到其他清洁的端面上。特别是在机房的空调系统运行时，空气的流动会加速这种传播过程。

工具传播是另一个重要途径。如果使用同一清洁工具清洁多个光模块，而工具本身没有得到有效清洁，就可能成为污染物的载体，将污染从一个端面传播到另一个端面。

**清洁效果评估的困难性**

即使完成了光链路的清洁操作，如何评估清洁效果也是一个难题。传统的评估方法主要依赖于人工观察和经验判断，但这种方法存在很大的主观性和不确定性。特别是对于微小的污染物或分布不均匀的污染，人工观察很难准确判断清洁效果。一些先进的清洁设备声称可以达到很高的清洁率，但这种数据通常是在实验室条件下获得的，在实际的现场应用中，由于环境条件、操作水平、污染物类型等因素的影响，实际的清洁效果可能大打折扣。此外，即使表面看起来清洁的端面，也可能存在微观层面的污染或损伤。这些微观污染可能不会立即影响光传输性能，但可能在长期使用过程中逐渐恶化，最终导致光链路故障。因此，如何建立科学、准确、可操作的清洁效果评估标准，是光链路维护技术面临的重要挑战。

### 5.3 万卡集群光链路污染问题解决方案

#### 5.3.1 物理隔离层技术方案

**光模块真空封装技术**

光模块真空封装技术是从根本上解决污染问题的一种革命性方案。该技术通过在光模块制造过程中采用真空封装工艺，将光模块的核心光学器件密封在一个与外界环境完全隔离的空间内，从而彻底避免了外界污染物的侵入。

真空封装的工艺过程极为严格。根据光通信器件封装技术标准，在进行封盖密封前，混合电路需要在真空中烘烤至少24小时，然后转移到与封盖密封机集成的真空烘烤室中进行处理。这种严格的真空处理过程不仅可以去除器件内部的水分和气体，还可以确保封装空间的高真空度，从而防止后续使用过程中产生凝露或气体污染。

光模块真空封装技术从根本上解决了污染问题，使得光模块在整个使用寿命内都无需担心外界污染物的侵入。其次，真空环境可以有效防止光学器件的氧化和老化，延长器件的使用寿命。此外，真空封装还能提供优异的热传导性能，有助于光模块的散热管理。

然而，真空封装技术也面临一些挑战。首先是成本问题，真空封装工艺的设备投资巨大，工艺要求严格，导致光模块的制造成本大幅增加。其次是散热设计的复杂性，虽然真空环境有利于热传导，但如何在保持真空密封的同时实现有效的散热，需要精心的结构设计。再次是可靠性验证的困难性，由于真空封装是一个不可逆的过程，一旦封装完成就无法打开检查，因此对封装过程的质量控制要求极高。

**光纤端面防尘帽技术**

光纤端面防尘帽是一种简单而有效的物理隔离方案，其设计和材料选择直接影响到防护效果。现代防尘帽技术已经从简单的橡胶帽发展为多材料、多结构的复合防护系统。

在材料选择方面，现代防尘帽采用多种高性能材料以满足不同的应用需求。常见的材料包括
HDPE、PVC、不锈钢和TPE等。硅胶材料在防尘帽中的应用尤为重要。优质的硅橡胶具有耐高低温（-50℃\~200℃）、弹性持久的特性，长期密封可靠性强，密封寿命可达20年以上。一些产品还采用特殊配方的硅树脂，包括甲基硅树脂、低苯基甲基硅树脂、氟硅树脂等，以及含氟填料和含氢硅油等添加剂，进一步提升防尘帽的性能。

然而，传统的防尘帽设计仍存在一些问题。市面上的防尘帽多使用软橡胶材质，这种材质容易吸附灰尘，保存不当的情况下，二次使用反而会对光纤口造成污染。此外，标准防尘帽的设计在加盖和开盖过程中可能会引入污染物，这是因为在操作过程中，防尘帽内部可能会积累灰尘，当再次盖上时就会将这些灰尘转移到光纤端面上。

为了解决这些问题，业界正在开发新一代的优化防尘帽。康宁公司提出CleanAdvantage技术，通过优化防尘帽的结构设计和制造工艺，确保安装人员在打开连接器包装后可以立即使用连接器连接设备，而不需要先逐个清洁连接器或进行检查。这种优化的防尘帽可以减少安装时间多达17%，并减少清洁耗材成本多达95%[^46]。

**无尘操作舱技术**

无尘操作舱是一种在光链路施工和维护过程中提供局部洁净环境的技术方案。该技术通过在操作区域建立一个与外界环境隔离的洁净空间，确保光模块和光纤在安装和维护过程中免受污染。

无尘操作舱的设计原理基于正压洁净室技术。操作舱内部保持正压状态，使洁净空气从舱内向外流出，防止外界污染物进入。舱内的空气经过多级过滤系统处理，包括初效、中效和高效过滤器，确保舱内空气达到所需的洁净度等级。

在万卡集群的应用场景中，无尘操作舱需要具备以下特点：

模块化设计：考虑到万卡集群的大规模部署需求，无尘操作舱应采用模块化设计，可以根据实际需要灵活组合，覆盖不同规模的操作区域。

快速部署能力：在AI训练过程中，系统维护窗口通常很短，因此无尘操作舱需要具备快速部署和拆除的能力，确保在最短时间内建立洁净的操作环境。

人员友好性：操作舱内部需要提供良好的工作环境，包括适宜的温度、湿度、照明条件等，确保操作人员可以在舱内长时间工作而不会感到不适。

成本效益：考虑到万卡集群的规模，无尘操作舱的成本必须控制在合理范围内，同时要保证足够的使用寿命和可靠性。

无尘操作舱技术的优势在于可以在不改变整个机房环境的情况下，为关键的光链路操作提供局部的洁净环境。这不仅可以大大降低机房整体洁净度的要求，从而降低机房建设和运营成本，还能确保光链路操作的质量。然而，无尘操作舱技术也面临一些挑战。首先是空间限制，操作舱的内部空间有限，可能会影响操作人员的活动范围，增加操作难度。其次是环境适应性，操作舱需要能够适应不同的机房环境条件，包括温度、湿度、地面条件等。再次是运维复杂性，操作舱本身也需要定期清洁和维护，确保其内部环境的洁净度。

#### 5.3.2 主动防御层技术方案

**机房正压通风系统**

机房正压通风系统是从整体环境控制角度解决光链路污染问题的重要方案。该系统通过在机房内建立持续的正压环境，使洁净空气从机房内部向外流出，从而防止外界污染物进入机房。正压通风系统的工作原理基于压差控制。系统通过精确控制进风量和排风量，使机房内的气压始终高于外界环境气压，通常压差控制在5-10Pa。这样，当机房的门、窗等开口部位打开时，空气会从机房内部向外流出，而不会有外界的污染空气进入。

在万卡集群的应用中，正压通风系统需要与其他环境控制系统协同工作。SPF级实验动物屏障环境采用的\"三级过滤 +
正压屏障\"系统可以为集群提供设计依据，即通过初效（G4）→中效（F8）→高效（H14）的三级过滤链路，配合正压屏障技术，确保进入机房的空气达到所需的洁净度等级。

正压通风系统的设计需要考虑以下关键因素：

风量计算：需要根据机房的体积、换气次数要求、压差控制要求等因素，精确计算所需的送风量和排风量。对于万卡级别的大型机房，送风量可能需要达到每小时数万立方米。

气流组织：合理的气流组织设计可以确保机房内的洁净度分布均匀，避免出现气流死角。通常采用上送下回的气流组织方式，使洁净空气从天花板送风口进入，从地板回风口排出。

过滤系统配置：根据机房洁净度要求，配置相应等级的过滤器。对于万卡集群，建议采用H13或H14级别的高效过滤器，确保对0.3μm颗粒的过滤效率达到99.95%以上。

控制系统设计：正压通风系统需要配备先进的控制系统，实时监测机房内的压力、温度、湿度、洁净度等参数，并根据监测结果自动调节风机转速、阀门开度等，确保系统始终处于最佳运行状态。

**H14级空气过滤系统**

H14级空气过滤系统代表了当前空气净化技术的最高水平。根据欧洲EN
1822标准，H14级过滤器对0.3μm颗粒的过滤效率≥99.995%，穿透率≤0.005%。这种极高的过滤效率使得H14级过滤器成为万卡集群机房空气净化的理想选择。H14级过滤器采用了先进的极光粒子技术，其MPPS（最易穿透粒径）效率可达99.95%或99.995%。这种技术通过优化过滤器的纤维结构和排列方式，使过滤器在最易穿透粒径范围内仍能保持极高的过滤效率。

在万卡集群的应用中，H14级空气过滤系统通常采用多级过滤配置：

初效过滤：采用G4级初效过滤器，主要过滤5μm以上的大颗粒污染物，如灰尘、纸屑、昆虫等。初效过滤器的作用是保护后续的中效和高效过滤器，延长其使用寿命。

中效过滤：采用F8级中效过滤器，主要过滤1-5μm的颗粒污染物。中效过滤器是整个过滤系统的重要组成部分，承担了大部分的过滤负荷。

高效过滤：采用H14级高效过滤器，主要过滤0.3μm以上的微小颗粒，包括细菌、病毒、烟雾颗粒等。H14级过滤器是整个系统的核心，其过滤效率直接决定了机房内的洁净度水平。

H14级空气过滤系统可以有效去除空气中的各种污染物，包括对光通信系统危害最大的微小颗粒。其次，它可以显著降低机房内的污染物浓度，从而减少光模块和光纤的污染风险。此外，它还可以改善机房内的空气质量，为工作人员提供健康的工作环境。

然而，H14级空气过滤系统也存在一些挑战。首先是成本问题，H14级过滤器的价格相对较高，而且需要定期更换，这会增加机房的运营成本。其次是压降问题，H14级过滤器的过滤效率极高，相应地其压降也较大，这会增加风机的能耗。另外是维护要求，H14级过滤器对安装和维护的要求很高，需要专业人员进行操作，否则可能影响过滤效果。

**智能环境监测与控制系统**

智能环境监测与控制系统是主动防御层技术的重要组成部分，它通过实时监测机房环境参数并自动调节相关设备，确保机房始终处于最佳的运行状态。海思的StarSensor星云智检技术已经在部分万卡集群中进行试点应用，星云400G光模块具备光口、电口健康度诊断、自动脏污检测等功能。通过增强型光模块级压测，星云光模块可以更容易识别出光链路脏污引起的突发误码，从而有效降低闪断风险。星云400G光模块的脏污检测算法，可以实现光链路端口检测准确率达到90+%，实现分钟级检测[^47]。星云智检技术相较传统压测方案效率提升60倍，检出率和准确率提高200%以上。

[^1]: 超大规模 AI 基础设施建设实践，极致释放算力效能-CSDN博客
    <https://blog.csdn.net/2301_82040283/article/details/151360308>

[^2]: 释放效率:多层数据中心的优势-51CTO.COM
    <https://server.51cto.com/article/785703.html>

[^3]: 人工智能服务器机柜安装方案.docx-原创力文档
    <https://m.book118.com/html/2025/1028/5314344213013003.shtm>

[^4]: 机房可视化施工方案.doc - 人人文库
    <https://m.renrendoc.com/paper/483814361.html>

[^5]: 数据中心设备部署管理指南.doc - 人人文库
    <https://m.renrendoc.com/paper/480872800.html>

[^6]: GPU集群如何规划_gpu服务器布局-CSDN博客
    <https://blog.csdn.net/weixin_42795092/article/details/149462180>

[^7]: 昆仑芯超节点创新设计:1U 4
    卡高密算力，无缝适配各类机房环境在大模型参数爆炸、训练推理并重的趋势下，「超节点」成为下 -
    掘金 <https://juejin.cn/post/7507311921604821046>

[^8]: 机架母线配电方案 提高供电方式稳定性
    数据中心适用_机架配电系统-CSDN博客
    <https://blog.csdn.net/m0_69597706/article/details/127648195>

[^9]: AI 机架功耗飙升，驱动数据中心电源与散热方案全面迭代
    <https://cj.sina.cn/article/norm_detail?froms=ttmp&url=https%3A%2F%2Ffinance.sina.com.cn%2Ftech%2Froll%2F2025-10-23%2Fdoc-infuvzsu8061290.shtml%3Ffinpagefr=ttzz>

[^10]: 800V高压直流架构破局!英伟达掀起 AI 数据中心电力革命_AI电堂
    <http://m.toutiao.com/group/7571355305146122752/?upstream_biz=doubao>

[^11]: https://zhuanlan.zhihu.com/p/1899150705090068719

[^12]: https://xueqiu.com/7613731234/349014823

[^13]: 8\*DCU K100-AI集群部署时如何实现高效散热?\_编程语言-CSDN问答
    <https://ask.csdn.net/questions/8770342>

[^14]: AI Server 散热设计全攻略:高效散热方案与机架优化一次搞懂
    <https://www.pinda.com.tw/cn/news-detail/ai-server-cooling-guide/>

[^15]: 昆仑芯超节点创新设计:1U 4 卡高密算力，无缝适配各类机房环境 \|
    极客公园 <https://www.geekpark.net/news/349926>

[^16]: [数据中心服务器接入部署的布线方式-百度开发者中心](https://developer.baidu.com/article/detail.html?id=3060918)

[^17]: https://zhuanlan.zhihu.com/p/464731105

[^18]: https://zhuanlan.zhihu.com/p/683427820

[^19]: [数据中心布线解决方案------MTP/MPO布线系统 -
    知乎](https://zhuanlan.zhihu.com/p/683427820)

[^20]: https://support.huawei.com/enterprise/zh/doc/EDOC1100023543/8e54cf0b

[^21]: 科华UPS电源系统1+1冗余与2N冗余有什么区别?-科华 UPS
    电源\|不间断电源・精密空调・蓄电池\|厦门科华全系列产品
    http://www.kehua-ups.net.cn/gongsi/399.html

[^22]: 智算，驱动UPS技术与应用------向高密、高容量飞奔 -
    数据中心供配电------传统突破与未来重构 - 文章资料 数智元网
    <http://www.dc-se.com/articledetail.asp?ID=12542>

[^23]: 机房供电布局优化-洞察及研究.docx -
    人人文库<https://www.renrendoc.com/paper/432112534.html>

[^24]: https://www.kstps.com.cn/hangye/459.html

[^25]: 数据中心UPS容量与线缆配置详细计算.docx-原创力文档
    <https://m.book118.com/html/2025/0829/8130101143007125.shtm>

[^26]: 深入探秘全球最大AI超级集群xAI Colossus
    <https://finance.sina.com.cn/roll/2024-11-02/doc-incurtue5588970.shtml>

[^27]: 韦伯咨询:收藏:一文梳理各省新能源政策规划、交易电价、方案、电源结构_搜狐网
    <https://www.sohu.com/a/864540518_100158378>

[^28]: 如何计算出使用贵州云计算服务的成本?虚拟主机 \| 南数网络
    <https://www.nanshucloud.com/news/news412.html>

[^29]: 云南成为国内首批新能源机制电价落地省份
    <https://www.yn.gov.cn/ywdt/bmdt/202511/t20251108_319676.html>

[^30]: 机房能耗大户竟是它?精密空调节能设计如何平衡制冷与成本_结论精密空调节能的核心在于智能控制、自然冷却、液冷技术及ai优化,未来将向更低p-CSDN博客
    <https://blog.csdn.net/br360/article/details/148400311>

[^31]: Ai服务器 GPU液冷方案_挚爱
    <http://m.toutiao.com/group/7570225115313537571/?upstream_biz=doubao>

[^32]: https://zhuanlan.zhihu.com/p/1904668800965652663

[^33]: https://xueqiu.com/4604547476/351358297

[^34]: OFC 2025: AI时代的光互连需求-腾讯云开发者社区-腾讯云
    <https://cloud.tencent.com.cn/developer/article/2529698>

[^35]: 海思专家谈万卡集群:澄清两大技术误区 光互联成为必选项_手机新浪网
    <http://finance.sina.cn/tech/2025-09-17/detail-infqufep2904713.d.html>

[^36]: 故障率降低100倍，微软突破性技术破解AI算力浪费大难题-51CTO.COM
    <https://www.51cto.com/article/825399.html>

[^37]: 什么是星联光模块?为什么需要星联光模块? - 华为
    <https://info.support.huawei.com/info-finder/encyclopedia/zh/%E6%98%9F%E8%81%94%E5%85%89%E6%A8%A1%E5%9D%97.html>

[^38]: 100G光模块的技术与应用-AET-电子技术应用
    <https://m.chinaaet.com/article/142274>

[^39]: 光模块与光纤的"精准对接":小接口核心技术剖析_优雅圆月WWc2E
    <http://m.toutiao.com/group/7564528174319550995/?upstream_biz=doubao>

[^40]: 多芯MT-FA光组件批量生产 值得信赖「上海光织科技供应」 - 商名企业
    <http://qiye.b2bname.com/r7aq2i47tl-26770612.html>

[^41]: 上海MT-FA多芯光组件精密制造 信息推荐 上海光织科技供应-宝商在线
    <https://m.trueland.net/tradeinfo/6492218.html>

[^42]: 端面光滑塑料光纤工艺要求 - 腾讯云开发者社区-腾讯云
    <https://cloud.tencent.cn/developer/news/2516910>

[^43]: 半导体制造环境控制系统(ECS)系列:洁净度控制系统\_(10).洁净度等级与标准.docx-原创力文档
    <https://m.book118.com/html/2024/1125/7056003035010002.shtm>

[^44]: 揭秘数据中心的环境参数对机房空调的几种要求 -
    腾讯云开发者社区-腾讯云
    <https://cloud.tencent.com/developer/news/65863>

[^45]: 提升信号完整性:Mellanox线缆端面清洁终极指南\|道通存储
    <https://mellanox.dt-stor.com/Industry-News/401.html>

[^46]: 为什么洁净度对数据中心如此重要 \| 光通信\| 康宁
    <https://www.corning.com/data-center/cn/zh/home/knowledge-center/fiber-cleanliness-in-the-data-center.html>

[^47]: https://www.c114.com.cn/4app/3543/a1297122.html


## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=114988018571687&bvid=BV1DktBzLEvb&cid=31551195779&p=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>

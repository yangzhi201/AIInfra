<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->

# 03.AI 万卡集群交付与测试

Author by: staries wang

# 一、测试步骤

## 1. 测试前准备

### 1.1 大模型镜像准备（Step1）

万卡集群环境下，确保所有节点的镜像环境一致性至关重要。通过容器化技术，可以实现环境的标准化部署和版本管理。将开发机中的运行环境保存为镜像，不仅能够在日后快速复用该开发环境，还可以将其应用到多节点训练任务中，确保训练环境的一致性与稳定性。

镜像构建的核心组件包括：

（1）模型文件与权重：需要将预训练模型权重文件存储在共享存储系统中，支持多节点并发访问。分布式文件系统如Lustre可以提供每秒TB级的IO带宽，避免数据读取成为瓶颈。

（2）训练脚本与依赖：将模型训练所需的所有脚本、配置文件、依赖库打包进镜像。通过采用分层构建策略，将不常变动的依赖安装置于Dockerfile前部，可以最大化缓存命中率。使用多阶段构建分离编译环境与运行环境，显著减小最终镜像大小。

（3）CUDA/CANN运行时环境：根据硬件平台选择相应的运行时环境。对于NVIDIA平台，需要安装完整的CUDA Toolkit和cuDNN。对于华为昇腾平台，则需要部署CANN异构计算架构。

（4）分布式训练框架：NVIDIA Megatron-LM是专为超大规模Transformer模型量身打造的开源分布式训练框架。在MoE模型支持方面，新版本针对DeepSeek-V3模型的微调进行了优化，引入了无辅助损失的负载均衡策略、灵活的路由策略等创新特性。

常用的分布式训练框架对比如下：

|  |  |  |  |  |
| --- | --- | --- | --- | --- |
| **框架** | **开发者** | **核心能力** | **硬件支持** | **适用场景** |
| DeepSpeed | 微软 | 1. ZeRO优化器（内存压缩核心）；2. 支持数据 / 模型 / 流水线混合并行；3. 推理一体化（Infinity Engine） | NVIDIA GPU | 千亿级LLM（语言模型）、低成本训练（内存占用低） |
| Megatron-LM | NVIDIA | 1. 极致张量并行优化（Transformer 层深度定制）；2. 支持混合并行；3. 适配 NVIDIA GPU生态 | NVIDIA GPU | 超大规模模型（万亿级）、追求极致训练速度 |
| Colossal-AI | 华为 / 社区 | 1. 模块化设计（支持 AutoParallel 自动选并行策略）；2. 轻量易集成；3. 多模态模型支持 | NVIDIA GPU/CPU | 灵活适配不同规模模型（从亿级到万亿级） |

镜像优化策略：

（1）体积压缩：采用8-bit量化技术将浮点权重转为int8，可将模型大小压缩至原大小的1/4；通过结构化剪枝移除不重要的神经元连接，减少参数量；通过.gitignore排除冗余文件，并使用轻量级基础镜像，可以显著减少镜像体积。同时，为提高镜像拉取速度，可以将镜像同步至容器镜像服务（如TCR），并在YAML文件中替换相应的镜像地址[[1]](#endnote-1)。

（2）版本管理：为不同模型和训练任务创建独立的镜像版本，通过标签系统进行管理，便于版本回滚与灰度发布。

（3）安全加固：在镜像中集成安全扫描工具，定期检测漏洞和恶意软件，确保训练环境的安全性。

### 1.2 数据集准备（Step2）

#### 1.2.1 适合集群测试的开源数据集选择

针对万卡集群的性能测试，选择合适的开源数据集至关重要。大规模集群测试通常需要TB级甚至PB级的数据集，以充分发挥集群的计算能力。以下是一些适合集群测试的主流开源数据集：

**文本类数据集**：

C4（Common Crawl Cleaned）：一个经过清洗的大规模文本数据集，包含大约156GB的文本数据，是预训练语言模型的标准数据集之一。

WikiText：包含来自维基百科的高质量文本，分为WikiText-103和WikiText-2两个版本，适合语言模型评估。

BookCorpus：包含超过11,000本未出版书籍的文本，是训练语言模型的重要数据集。

**图像类数据集**：

ImageNet：包含1000个类别的120万张高分辨率图像，是计算机视觉领域的标准基准数据集。

COCO：包含超过33万张图像，其中有20万张标注图像，涵盖80个物体类别，常用于目标检测、分割和字幕生成任务。

**代码类数据集**：

CodeSearchNet：包含来自GitHub的数百万个代码片段，涵盖多种编程语言，适合代码生成和理解任务。

在实际测试中，建议根据目标模型类型选择相应的数据集。例如，对于语言模型训练，C4和WikiText是理想选择；对于计算机视觉任务，ImageNet是标准选择。

#### 1.2.2 数据集存储架构与分片策略

万卡集群环境下，数据集的存储架构设计直接影响训练性能。传统的集中式存储难以满足万卡规模的高并发访问需求，因此需要采用分布式存储架构。

**分布式文件系统**：

Lustre：专为大规模并行计算设计的分布式文件系统，支持PB级数据存储和高并发访问。

GPFS（IBM Spectrum Scale）：提供全局命名空间和并行I/O能力，适合大规模AI训练场景。

BeeGFS：具有优异的聚合带宽和低延迟特性，支持横向扩展至数千个节点。

**对象存储**：

阿里云OSS、腾讯云COS、华为云OBS等主流云存储服务，提供高可靠、高扩展的对象存储能力。

通过与缓存加速系统（如RapidFS）结合，可以实现接近本地存储的访问性能。

**数据集分片策略需要考虑以下因素**：

分片大小：通常建议分片大小为128MB到1GB，过小会增加元数据开销，过大则影响并行度。

分片数量：分片数量应大于GPU数量，通常为GPU数量的2-4倍，以确保负载均衡。

存储位置：分片应分布在多个存储节点上，避免单点故障和访问热点。

以百度智能云的RapidFS存储加速系统为例，该系统为昆仑芯3万卡集群提供了数百台国产CPU服务器，集群设计总吞吐接近10TiB/s。测试结果显示，20个RapidFS存储节点稳定提供了302 GiB/s吞吐，70个RapidFS存储节点稳定提供了1.03 TiB/s吞吐，单台RapidFS存储节点可提供15 GiB/s吞吐。

#### 1.2.3 数据加载与预处理优化

在万卡集群环境下，数据加载和预处理的效率直接影响整体训练性能。以下是一些关键的优化策略：

异步数据加载：使用多线程或多进程异步加载数据，避免GPU等待数据的时间。在PyTorch中，可以通过设置DataLoader的num\_workers参数来实现多进程数据加载，通常将num\_workers设置为GPU数量的2-4倍。

数据预处理优化：预处理操作应尽量在GPU上完成，利用GPU的并行计算能力。使用NVIDIA DALI（Data Loading Library）等专门的数据加载库，支持GPU加速的数据解码和预处理。对于图像数据，使用混合精度加载和预处理，减少内存占用和计算开销。

缓存策略：在每个计算节点上设置本地缓存，存储经常访问的数据分片。使用分布式缓存系统，如Redis或Memcached，在集群范围内共享热点数据。实现智能预加载机制，根据训练进度预测下一步需要的数据。

数据增强并行化：在多个GPU上并行执行数据增强操作，提高数据处理效率。使用NVIDIA的DALI或类似工具，支持GPU加速的数据增强操作。对于文本数据，实现并行的文本清洗、分词和向量化处理。

### 1.3 模型训练脚本准备（Step3）

#### 1.3.1 关键超参数配置详解

模型训练脚本的超参数配置直接决定了训练的收敛速度、模型性能和资源利用率。万卡集群环境下的关键超参数配置如下：

**学习率配置**：学习率的选择需要根据模型规模进行调整。对于8B参数的模型，可以使用1.5e-4的学习率。预热策略方面，小模型（<10B参数）预热样本数设为总样本的0.03%，大模型（>100B参数）可提高至0.1%。当验证损失不再下降时，可通过min\_lr参数保留基础学习率10%-20%的最小值，防止学习停滞。

**批次大小配置**：全局批次大小的计算公式为：全局批次大小 = 微批次大小 × 数据并行数 × 梯度累积步数。在实际配置中，需要根据GPU显存大小和模型规模调整微批次大小。

**优化器配置：**常用的优化器包括AdamW、SGD等。AdamW是Adam优化器的改进版本，在权重衰减方面有更好的表现。梯度裁剪（clip-grad）参数可以防止梯度爆炸。

**混合精度训练配置：**混合精度训练可以显著减少内存占用和计算时间，同时保持模型精度。PyTorch的自动混合精度（AMP）功能可以自动选择合适的精度进行计算。

#### 1.3.2 集群环境适配与调试工具

万卡集群环境下的训练脚本需要特别适配大规模分布式环境的特点，并集成相应的调试工具，常用的调试工具有：

（1）性能分析工具：

NVIDIA Nsight Systems：用于CUDA应用的性能分析和优化

PyTorch Profiler：集成在PyTorch中的性能分析工具

torch.autograd.profiler：用于分析计算图和内存使用

（2）通信调试工具：

NCCL-tests：官方提供的NCCL通信性能测试工具

nccl-netmon：用于监控NCCL通信性能和错误

ibmon：用于监控InfiniBand网络性能

（3）可视化工具：

TensorBoard：用于可视化训练过程、损失曲线、模型结构等

WandB（Weights & Biases）：提供更高级的实验跟踪和可视化功能

Grafana：结合 Prometheus 实现集群级别的监控数据可视化

此外，在万卡规模下，故障是常态，训练脚本需要集成完善的容错机制，定期保存检查点，发现异常数据并处理，监控系统和日志。

## 2. 测试策略与原则

万卡集群测试遵循"由小到大、由易到难、先功能再性能、先测峰值再测长稳"的核心原则，通过系统性的递进验证，确保每个规模节点的稳定性和可靠性，为最终的万卡规模训练奠定坚实基础。

测试规模梯度设计采用指数级增长策略，例如可以分为以下阶段：

|  |  |  |
| --- | --- | --- |
| 测试阶段 | 集群规模 | 测试重点 |
| 单机测试 | 1节点（8卡） | 验证单节点内多卡通信和负载均衡 |
| 小规模集群 | 16-64卡 | 验证分跨节点通信和任务分发 |
| 中规模集群 | 256-2048卡 | 验证分布式训练功能、并行训练性能以及中等规模压力测试 |
| 大规模集群 | 4096-万卡 | 验证系统在大规模下的稳定性以及集群的性能极限 |

在节点扩展过程中，集群线性加速比和GPU利用率两个指标十分重要。集群线性加速比反映了系统在增加计算节点时的性能提升效率，理想情况下应接近理论加速比。GPU利用率则直接影响训练效率，通过优化资源调度和负载均衡策略，确保GPU计算核心得到充分利用。

## 3. 故障冗余与备份体系

### 3.1 2K+X冗余策略设计

2K+X冗余策略是万卡集群测试的核心保障机制，其中X代表备用节点数量，通常配置为总节点数的5-10%。以2000个节点的测试集群为例，建议配置100-200个备用节点，形成2100-2200个节点的完整测试环境。这一策略的设计理念在于确保在出现节点故障时能够快速替换，最大限度减少训练中断时间。

备用节点采用"热备 + 冷备"的混合策略，其中10%的节点作为热备节点（已启动并加入集群管理），5%的节点作为冷备节点（关机状态，需要时快速启动）。且备用节点与主节点具有完全相同的硬件配置，包括GPU型号、内存容量、网络接口等，确保故障替换时不会因硬件差异导致兼容性问题；以及一致的操作系统、驱动程序、容器环境和训练镜像，通过自动化配置管理工具确保环境的实时同步。这种设计确保在主节点出现故障时，备用节点能够立即接管工作负载，实现无缝切换。

故障切换机制采用自动化流程设计，当检测到主节点出现故障时，系统自动触发切换流程。首先将故障节点标记为不可用，然后从备用节点池中选择一个健康节点，通过Kubernetes等容器编排平台重新调度训练任务。整个切换过程通常在30秒内完成，对训练任务的影响可以忽略不计。

### 3.2 故障检测与自动替换机制

故障检测机制采用多层次监控体系，包括硬件级监控、系统级监控和应用级监控三个层面。硬件级监控通过IPMI、BMC等管理接口实时监测服务器的电源状态、温度、风扇转速、电压等关键指标，当出现异常时立即触发告警。

系统级监控重点关注操作系统、驱动程序、运行时环境的健康状态。通过部署轻量级监控代理，实时采集CPU使用率、内存占用、磁盘I/O、网络流量等系统指标。当检测到系统资源耗尽、进程异常退出、网络连接中断等问题时，立即启动故障处理流程。

应用级监控聚焦于训练任务的运行状态和性能表现。通过心跳机制检测训练进程的活跃度，当在预定时间窗口内未收到心跳消息时，判定为进程故障。同时监控训练日志输出、计算速度、通信延迟等关键指标，通过机器学习算法识别异常模式。

自动替换机制的核心在于快速定位故障节点并启动替换流程。当检测到故障时，系统首先通过诊断测试确认故障类型和严重程度。对于硬件故障和系统级故障，立即触发节点替换流程；对于应用级故障，尝试重启进程，如重启失败则替换节点。替换流程包括以下关键步骤：首先将故障节点从集群中隔离，确保不再分配新的训练任务，并标记为不可用；然后从备用节点池中选择一个健康节点加入集群，该节点应具备相同的硬件配置和软件环境；接下来通过快速数据同步机制，将故障节点的最新状态信息同步到备用节点；最后重新调度训练任务，从最近的检查点恢复训练。

## 2.1 环境检查与验证

在启动万卡集群测试之前，必须进行全面的硬件健康状态检测，确保所有硬件组件处于正常工作状态。以下是详细的检测项目和方法：

GPU健康检查：GPU温度、使用率、显存使用、GPU风扇转速、ECC错误；

CPU和内存检查：CPU信息和使用率、内存使用情况、内存带宽

网络连接验证：InfiniBand网络状态、以太网连接、NCCL通信

存储系统检查：本地存储、分布式存储的吞吐量、延迟、并发性能

除了硬件组件外，软件栈的版本兼容性直接影响训练的稳定性和性能，通常需要检查的一些关键软件包括CUDA和驱动版本兼容性，cuDNN、PyTorch或相关库版本、Megatron版本、NCCL版本等。

集群连通性是确保分布式训练能够正常进行的关键步骤，因此还需要进行集群连通性测试，例如节点间通信测试、分布式训练环境验证、数据访问验证（验证所有节点能够访问训练数据、测试数据读取速度）、GPU Direct RDMA测试等。

此外，还需要测试万卡集群在极限负载下持续稳定运行能力。

（1） 稳定性测试：将学习率设置为最小值，批处理大小设置为最大值，使用标准模型测试集群在长时间训练下的稳定性。
（2） 混合负载测试：同时执行推理和训练任务，模拟真实生产环境的多任务场景。
（3） 压力测试：将资源使用设置为达到硬件极限，测试系统在接近崩溃边缘的稳定性。
（4） 容错能力测试：注入硬件故障（模拟GPU掉电、内存错误、网络故障、磁盘损坏、节点宕机等），软件故障（进程崩溃、死锁、网络延迟）或模拟资源耗尽，来验证系统的容错能力。

衡量系统性能的关键指标包括：

（1）计算性能指标：

线性度=实际加速比/理论加速比；

模型FLOPS利用率（MFU）=实际FLOPS/理论峰值FLOPS；

训练效率=有效训练时间/总时间等。

（2）通信性能指标：

聚合带宽（Aggregate Bandwidth）：所有GPU间通信的总带宽；

通信效率（Communication Efficiency）：实际通信速度与理论速度的比值；

延迟（Latency）：数据从源到目的地的传输时间

（3）资源利用率指标：

GPU利用率（GPU Utilization）：GPU实际计算时间占总时间的比例

显存利用率（Memory Utilization）：实际使用显存占总显存的比例

CPU利用率（CPU Utilization）：CPU实际使用时间占总时间的比例

（4）能效指标：

算力功耗比（FLOPS/Watt）：总FLOPS/总功耗（瓦特）

PUE（Power Usage Effectiveness）：总能耗/IT设备能耗

（5）可靠性指标：

平均无故障时间（MTBF）：两次故障之间的平均时间，总运行时间/故障次数

可用性（Availability）：系统正常运行时间占总时间的比例，(总时间 - 故障时间)/总时间

故障恢复时间（MTTR）：从故障发生到系统恢复正常的平均时间

（6）容错能力评估：

故障检测覆盖率：系统能够自动检测到的故障类型占总故障类型的比例

故障自动恢复率：系统能够自动恢复的故障占检测到故障的比例

数据一致性保障：故障恢复后数据完整性的保证程度

## 4 测试环境具体核心步骤

### 4.1 单核压测

单核压测是验证单个GPU核心计算能力的基础测试，主要用于评估GPU在极限负载下的稳定性和性能表现。gpu\_burn是基于NVIDIA CUDA框架开发的轻量级GPU压力测试工具，专为验证GPU核心（CUDA Core）和显存（VRAM）稳定性设计，通过"饱和式计算"让GPU达到满负载，是检测GPU硬件故障（如显存坏块、核心算力衰减）的核心工具[[2]](#endnote-2)。

gpu\_burn的核心原理包括三个方面：（1）算力拉满，调用CUDA内核函数执行密集型浮点运算（支持单精度float、双精度double），使CUDA Core利用率接近100%；（2）显存压榨，分配大尺寸显存缓冲区循环读写数据，占用90%以上显存空间；（3）多卡适配，自动识别服务器中所有NVIDIA GPU，支持单卡、多卡并行压测。

除了gpu\_burn，常见的压测工具还有DCGMI，通过dcgmi stress子命令执行GPU压力测试，该命令启动默认的GPU压力测试模式，通常包括计算和内存操作的混合负载，默认执行60秒后自动停止[[3]](#endnote-3)。FurMark是由Geeks3D开发的GPU压力测试工具，专门针对NVIDIA GeForce、AMD Radeon和 Intel HD Graphics等显卡，在满负载下进行全面测试，包括CPU频率、功耗和温度表现，支持单显卡和多显卡烤机模式[[4]](#endnote-4)。

此外，还可以通过CPU基准测试工具进行长时间计算，测试不同类型的计算任务来验证CPU的计算能力，检测CPU的温度、功耗和频率变化。使用STREAM benchmark等工具测试内存读写性能，验证L1、L2、L3缓存的性能表现，测量内存访问的延迟时间。使用fio等工具测试磁盘读写性能；测试网络接口的带宽和延迟；验证分布式文件系统的访问性能。

### 4.2 单机压测

单机压测的评估需要建立全面的指标体系来衡量GPU和系统的综合性能。主要评估指标如下：

计算性能指标方面，主要测试GPU的FP32/FP16和INT8计算能力。使用GEMM（矩阵乘法）函数测试实际浮点计算能力，可通过CUBLAS测试极限性能。测试要求在8张卡系统上分别测试1、2、4、8卡数量下的FP32/FP16/INT8性能，实际计算效率需达到SPEC基准的80%以上。

多GPU并行性能指标包括三个核心参数：加速比（Speedup）=单GPU运行时间/多GPU运行时间，这是最直观的性能提升指标；效率（Efficiency）=加速比/GPU数量，反映资源利用率；计算能力利用率（Compute Utilization），衡量GPU计算单元的实际使用效率[[5]](#endnote-5)。

硬件健康指标监控是确保测试安全的重要环节，包括：GPU利用率、温度、功耗、显存带宽与占用、错误率与稳定性。这些指标需要在测试过程中实时监控，确保GPU工作在安全范围内[[6]](#endnote-6)。

系统级性能指标通过NVIDIA Nsight Systems等工具进行深入分析。Nsight Systems能够采集GPU硬件指标（如SM利用率、显存带宽），从硬件层面定位性能瓶颈。该工具支持CPU相关采集项（CPU指令指针/回溯采样、CPU上下文切换追踪）、异构计算相关采集（CUDA trace、GPU metrics、NVTX trace）以及图形API追踪等全方位监控能力。

### 4.3 通信压测

在万卡集群中，集体通信算法的性能直接决定了分布式训练的效率。NCCL（NVIDIA Collective Communications Library）是最常用的集合通信库，用于加速多GPU的分布式深度学习训练和推理。

万卡集群的通信压测需要特别关注网络拓扑的影响。当前主流的网络架构是借鉴NVIDIA DGX-SuperPod-A100的设计，每个基本单元SU（Scalable Unit）包含200台A800主机和4台Leaf交换机，20台节点间的同号网卡通过单台Leaf直接通信。这种架构支持四种通信路径：同一个Node内的GPU通过NVLink通信；同一个SU内不同Node的同号网卡GPU通过Leaf交换机直接通信（同轨通信）；不同网卡的GPU需要跨Spine通信（跨轨通信）；不同SU间的GPU同样需要跨Spine通信。

在集体通信算法的选择上，不同算法在不同场景下表现各异。Ring AllReduce算法将每个节点的数据切分成N份，充分利用所有节点的发送和接收带宽，适合大数据块传输，能够改善时延抖动问题，但延迟相对较高（2N-2 步长）。相比之下，MultiShot算法只需要2个通信步骤，速度几乎是Ring AllReduce的3倍，特别适合小数据量的快速同步[[7]](#endnote-7)。

万卡集群的通信压测工具需支持大规模节点扩展、多通信模式，且能适配RDMA等高速协议，常用工具按场景分为以下几类：

|  |  |  |  |
| --- | --- | --- | --- |
| **通信场景** | **推荐工具** | **核心能力** | **适用场景** |
| 点对点通信 | iperf3/ib\_send\_bw | - iperf3：测试TCP/UDP带宽、延迟（支持多线程）；  - ib\_send\_bw：Infiniband专用，测试RDMA点对点带宽/延迟 | 验证单链路通信性能（如网卡、交换机端口） |
| 集合通信 | osu-micro-benchmarks | AllReduce性能测试：使用NCCL的AllReduce操作测试通信效率；  Broadcast测试：验证广播操作的性能和正确性；  ReduceScatter测试：测试数据分发的性能 | 验证分布式训练、大数据任务的多节点协同性能 |
| GPU集群通信 | NCCL-tests | NVIDIA专属，基于NCCL库，测试GPU间直接通信（如GPU-GPU RDMA）的集合通信性能 | AI训练场景（如GPT、ResNet训练的梯度同步） |
| 网络稳定性压测 | pktgen-dpdk/ib\_traffic\_gen | 生成高并发数据包，测试长时间（如24小时）通信的丢包率、误码率 | 验证集群在满负载下的稳定性（如7x24小时任务） |

万卡集群直接全量压测风险高（易因局部故障导致整体失败），通常按照“单节点→小集群→全集群”的梯度推进，逐步定位问题：

步骤1：单节点与点对点基础测试（排除单点故障），验证单节点网卡、驱动、通信库的基础功能，排除单点硬件/软件问题。

网卡基础检测：通过ibstat（Infiniband）或ethtool（以太网）查看网卡状态，确认RDMA功能启用（ibv\_devinfo查看RDMA设备）；

点对点带宽/延迟测试：选2台节点（同交换机接入、跨交换机、跨汇聚层），使用ib\_send\_bw -a（Infiniband）测试RDMA点对点带宽（需接近网卡理论带宽，如200G网卡需≥180Gbps）；

使用ib\_send\_lat -a测试延迟（RDMA场景通常≤5μs，TCP场景≤50μs）；

多线程测试：模拟多并发流，验证带宽是否线性叠加。

步骤2：小集群集合通信测试（定位局部瓶颈），验证小规模节点（如32卡、128卡、512卡）的集合通信性能，定位交换机层级、路由配置的瓶颈。

基于osu-micro-benchmarks的集合通信测试：通过Slurm调度分配N个节点（如N=32/128），运行命令：srun -N 32 -n 32 osu\_allreduce -m 1024:16384（测试AllReduce操作，消息大小从1KB到16KB，覆盖梯度同步常用大小），可以测试得到延迟（随节点数增长是否平缓）、吞吐量（是否接近“节点数 × 单链路带宽”的理论值，如32节点×100Gbps=3.2Tbps，实际需≥80%理论值）等关键指标。

GPU集群专项测试：用NCCL-tests的all\_reduce\_perf测试GPU间AllReduce性能：

评估“总线带宽”（如8卡GPU服务器内的NVLink带宽）和“跨节点带宽”是否匹配，避免“内部快、外部慢”的瓶颈。

步骤3：全集群（万卡）压测（模拟真实业务），验证万卡规模下的通信扩展性、稳定性，模拟真实业务负载。

节点分配策略：通过调度工具（如Slurm）分配连续拓扑的节点（如按机架、交换机分组），避免跨区域调度导致的网络延迟激增；

AI训练场景：用Megatron-LM（大模型训练框架）的“空跑测试”（不计算仅通信），测试万卡下AllReduce的梯度同步延迟（如1024维梯度的同步延迟≤1ms）；

大数据场景：用Spark的SortBenchmark测试Shuffle阶段的通信吞吐量（如1TB数据Shuffle的总通信耗时≤30分钟）；

长时间稳定性测试：运行压测任务24-72小时（如ib\_traffic\_gen -t 86400），监控丢包率（需≤10⁻⁹）、节点离线率（需为0）、交换机端口错误数（如CRC错误、丢包计数需为0）。

### 4.4 集群健康检查

万卡集群的健康状态评估需要建立全面、多层次的指标体系。腾讯云的GPU实例监控系统提供了完整的指标框架，涵盖GPU基础性能、内存使用、功耗、温度等核心维度[[8]](#endnote-8)。

GPU基础性能指标包括：GPU使用率（评估负载所消耗的计算能力，非空闲状态百分比）、GPU显存使用量（评估负载对显存占用，单位MB）、GPU显存使用率（评估负载对显存占用百分比）、GPU功耗使用量（评估GPU耗电情况，单位W）、GPU温度（评估GPU散热状态，单位摄氏度）、GPU编码器使用率和GPU解码器使用率。

GPU健康状态综合指标（gpu\_status）是一个关键的综合评估指标，当出现以下情况时会判定为故障状态：ECC错误超过阈值、显存地址重映射失败、GPU卡rev ff、infoROM错误、存在待隔离页、remapped rows错误。这些指标的监控对于及时发现硬件故障至关重要。

RDMA网络健康指标是万卡集群特有的监控维度，包括：RDMA网卡接收/发送带宽（MBit/s）、RDMA网卡入包量/出包量（个/秒）、CNP统计量（拥塞通知报文统计）、ECN统计量（显示拥塞通知统计）、端测丢包量、接收方乱序错误量、发送方超时错误量、TX/RX PFC统计量等。这些指标能够全面反映RDMA网络的运行状态和潜在问题。

Nvidia DCGM（Data Center GPU Manager）能够采集显存占用、各种算力利用率、温度、功率、频率以及NVLink等各种异常相关指标。DCGM的优势在于能够在内核层面采集GPU指标，避免频繁调用nvidia-smi带来的上下文切换开销，还能够通过进程ID（PID）追踪GPU负载归属[[9]](#endnote-9)。

### 4.5 告警监控

万卡集群的监控工具部署需要考虑高可用性、可扩展性和性能效率。主流的监控架构采用Prometheus+Grafana+Alertmanager的组合，配合各种Exporter实现全方位监控。

Prometheus部署架构采用联邦模式以分散采集压力，通过node\_exporter采集Linux主机的CPU、内存、磁盘等硬件指标。在GPU监控方面，腾讯云的Prometheus TKE GPU Exporter提供全自动管理，用户无需手动部署Exporter或编写配置规则，仅需在集成中心选择对应集群，即可实现从GPU硬件到容器化应用的全链路指标采集。

Grafana集群部署通过HA配置保障高可用，利用社区模板（如Node Exporter Full、Kubernetes Cluster）可以快速部署监控面板。Grafana的web界面（ip:3000）提供了强大的数据可视化能力，支持通过导入官方监控图表模板来简化配置工作。

Alertmanager告警管理支持多种通知渠道，包括邮件、短信、钉钉、微信等。告警规则的配置需要根据实际业务需求进行定制，例如GPU温度持续5分钟超过80℃时触发告警，GPU功耗使用率小于0时可能出现Unknown Error等。

DCGM-Exporter作为NVIDIA官方的GPU监控工具，提供了超越nvidia-smi的精细化监控能力。通过DCGM-Exporter采集的核心指标包括编码器利用率、GPU使用率、卡状态、错误计数与掉卡状态等，能够更精确地掌握GPU运行状态和健康状况。

多层级健康检查机制是保障集群稳定的基础。Meta的万卡GPU集群采用Slurm作为调度器，在作业运行前后执行健康检查，涵盖从GPU错误（如XID错误）到文件系统挂载错误及服务状态等多个方面。Scheduler接收集群中每个节点定期执行的健康检查结果，通过这些检查可以分析作业故障原因[[10]](#endnote-10)。

华为昇腾万卡集群的健康监控系统类似为每台计算机安装了"健康手环"，持续监测温度、算力利用率、数据传输速度等指标。一旦发现某台设备运行异常（如散热不良导致速度变慢），系统会立即发出警报并自动分析故障原因，区分是硬件老化、软件冲突还是网络问题。故障自愈能力的实现需要多层次的技术支撑。在进程级恢复方面，华为昇腾系统支持两种模式：进程级重调度恢复通过参数面网络将临终CKPT传递到备用节点，完成参数状态恢复后继续训练，能够将训练恢复时间缩短到3分钟以内；进程级在线恢复针对硬件UCE故障，通过昇腾CANN软件、框架软件、MindCluster软件配合实现故障地址在线修复，进一步将训练恢复时间缩短到30秒以内[[11]](#endnote-11)。

### 4.6 参数链路检查

参数链路检查是确保万卡集群中所有节点参数一致性的关键环节。字节跳动MegaScale系统的实践表明，通过系统性的链路检查方法，能够有效诊断主机内网络中的潜在瓶颈。

环回测试（Loopback Test）是验证参数链路完整性的基础方法。该测试测量从所有RDMA NIC（RNIC）到各种主机内端点（包括内存节点和GPU）的环回带宽，在主机内进行全网状测试，覆盖所有可能的链路组合。通过端到端带宽结果可以推断出PCIe配置中特定于链路的带宽降级和不规则性。这种全面的测试方法能够发现单个测试点无法检测到的系统性问题。

RNIC-to-RNIC测试专门检查同一主机上不同RNIC之间的连接和带宽性能。这项测试对于验证多网卡配置的正确性至关重要，特别是在采用bonding或链路聚合技术时，能够确保所有网络路径都正常工作。

分布式Checkpoint一致性验证是参数链路检查的重要应用场景，同时Checkpoint机制也是万卡集群容错和恢复的核心技术。MindSpore的实践表明，通过解析group\_info.pb文件可以验证不同GPU间的权重切分是否一致。例如，当0卡和4卡的权重切分完全一致时，若0卡的checkpoint丢失，可以直接复制4卡的checkpoint作为0卡的checkpoint进行恢复。这种一致性验证机制为故障恢复提供了可靠保障[[12]](#endnote-12)。

Checkpoint保存策略需要在存储效率和恢复粒度之间找到平衡。Checkpoint用于异常进程恢复和正常进程回滚，保存间隔是可配置的关键参数。间隔越小，恢复到上次保存checkpoint所回退的step数就越小，但频繁保存可能影响训练效率；间隔越大则相反，恢复时需要重算的步骤更多[[13]](#endnote-13)。

DeepSpeed的Checkpoint系统是现有实现方案之一，该系统通过定期保存模型权重、优化器状态和训练元数据，为故障恢复提供时间点快照。其创新之处在于预加载检查点元数据机制，在训练过程中异步加载下一个可能的恢复点元数据，大大缩短了故障发生时的恢复时间。实际测试表明，从第1,245,500步的检查点恢复仅耗时4分12秒[[14]](#endnote-14)。

### 4.8 残留清理恢复

万卡集群的残留清理和资源回收是确保系统长期稳定运行的重要环节。由于训练任务的复杂性和不确定性，异常终止的任务可能留下各种临时资源，如果不及时清理，会导致资源泄漏和系统性能下降。

系统级残留清理主要通过包管理工具完成。使用sudo apt-get autoremove -y && sudo apt-get autoclean命令可以清理残留依赖和缓存，确保系统环境的整洁。这种定期清理机制对于长期运行的集群尤为重要，能够防止无用软件包和临时文件占用大量存储空间[[15]](#endnote-15)。

GPU显存清理是最关键的资源回收操作。通过调用torch.cuda.empty\_cache ()函数可以清理PyTorch的CUDA缓存，释放不再使用的显存。在分布式训练中，还需要执行destroy\_process\_group ()来销毁进程组，确保所有分布式资源被正确释放。此外，通过删除模型引用和执行Python垃圾回收，可以进一步确保内存资源的彻底释放[[16]](#endnote-16)。

临时文件自动清理机制需要结合LRU（最近最少使用）策略。当缓存文件数量达到上限时，系统自动触发清理机制，删除最近最少使用的文件。这种策略能够在保证系统性能的同时，有效控制存储开销。Spark等大数据处理框架提供了完善的自动清理配置，包括spark.cleaner.ttl（设置临时数据存活时间）、spark.cleaner.external.enabled（启用外部清理程序）、spark.cleaner.external.interval（外部清理程序运行间隔）等参数[[17]](#endnote-17)。

### 4.9 训练进程监控

万卡集群的训练进程监控需要采用多层次、全方位的技术架构。字节跳动的MegaScale系统通过在每个GPU上创建训练进程并启动训练守护进程，定期向驱动发送心跳，可以实现对所有训练进程的实时监控[[18]](#endnote-18)。并采用不同级别的监控策略来跟踪各种指标，秒级监控通常用于评估整体健康状态，并排除常见配置对训练的影响；毫秒级监控则用于深度性能分析，能够发现细粒度的性能问题。这种分级监控策略既保证了监控的实时性，又避免了过度采集对系统性能的影响。

百度百舸基于eBPF（Extended Berkeley Packet Filter）技术构建了隐式故障感知体系，能够在不侵入用户代码的前提下，对训练进程的系统调用、网络通信、CPU调度等内核态行为进行立体观测。该技术的核心包括四个方面：训练关键函数跟踪（微秒级跟踪前向、反向计算和集合通信操作）；进程调度阻塞跟踪（挂钩sched\_switch事件，检测进程在TASK\_UNINTERRUPTIBLE状态的持续时间，当单次超过5秒时捕获调用栈）；CUDA运行时API监控（通过uprobe在libcuda.so等关键库注入探针）；RDMA Verbs级通信监控（在ibv\_post\_send/ibv\_poll\_cq等核心通信接口设置观测点）[[19]](#endnote-19)。

PerfTracker系统融合了传统在线监控和离线剖析的优势，同时克服了两者的局限性。具备以下特点：细粒度可观测性，收集LMT的所有函数执行事件和高频硬件采样；效率和可扩展性：由于细粒度可观测性会产生大量数据，在线排查必须高效且可扩展至大型GPU集群；低开销：不对常规LMT产生性能影响，并最小化对计算能力、网络带宽和存储的要求[[20]](#endnote-20)。

DCGM（Data Center GPU Manager）提供了最全面的GPU监控能力。与传统的nvidia-smi相比，DCGM能够在内核层面采集GPU指标，避免了频繁调用带来的上下文切换开销。DCGM的监控指标极其丰富，包括显存占用、各种算力利用率、温度、功率、频率以及NVLink和各种异常相关指标。更重要的是，DCGM能够通过进程ID（PID）追踪GPU负载归属，精确识别每个进程对GPU资源的占用情况[[21]](#endnote-21)。

实时性能分析工具如nvitop提供了强大的进程管理能力。nvitop能够实时监控GPU设备的各项关键指标，包括GPU利用率监控和阈值告警。通过nvitop/api/process.py模块，可以实现精细化的进程管理：仅显示计算类型的GPU进程（nvitop -1 -c）、按用户筛选进程（nvitop -u username）、监控特定PID的进程（nvitop -p 1234 5678）等。该工具还支持管理100+台GPU服务器，所有机器的GPU状态集中在一个界面显示[[22]](#endnote-22)。

GPU Hot可视化管理工具提供了直观的监控界面，能够实时显示GPU利用率、温度、内存占用、功耗等核心数据，并用图表直观展示。当某块显卡内存快满时，图表会清晰标红，无需再盯着命令行输出猜测状态。单台机器的多个GPU可以逐个查看，甚至能显示每个GPU上运行的进程，方便定位"谁占用了资源"[[23]](#endnote-23)。

# 二、测试方案

## 2.1 模型测试方案

集群模型测试有两大方向，一是模型按照参数量（模型大小、注意力机制头数、隐藏层大小、层数）和匹配的计算节点数递增，并提供Dense类模型和MoE稀疏类模型进行测试，主要关注的指标包括Per-GPU teraFLOP/s：单GPU峰值算力，每秒万亿次浮点运算能力，FLOPS利用率是衡量硬件计算资源利用效率的关键指标；MFU（Model FLOPs Utilization）：模型利用率，是衡量模型计算效率的重要指标，定义为训练时获得有效GPU计算FLOPS（training FLOPS per GPU）与该GPU的峰值FLOPS（peak FLOPS）的比值；Aggregate petaFLOP/s：集群峰值算力，总千万亿次浮点运算能力。

二是根据不同的并行策略（TP、PP、DP、EP、CP）、序列长度、GBS来递增计算节点，此时需要考虑的测试性能指标通常更多，包括单步迭代时间、单步训练波动时间、单步性能波动情况、训练时长、吞吐量、线性度、MTTR（平均故障恢复时间）。

吞吐量的计算方法为：吞吐量 = 全局Batch Size × 序列长度/(总训练时间 × GPU 数量)。

这一指标直接反映了模型的训练速度，在测试中，需要在稳定状态下测量吞吐量，排除训练初期的不稳定阶段。

## 2.2 通信测试方案

### 2.2.1 通信源语

**Ring AllReduce**

Ring AllReduce作为分布式训练的核心通信算法，其性能直接影响大规模模型训练效率。该算法的实现分为两个关键阶段：

Scatter-Reduce阶段：每个处理器将本地数据发送给右邻处理器，并接收来自左邻的数据进行归约操作。

AllGather阶段：处理器将scatter-reduce阶段的结果发送给右邻处理器，并从左邻处理器接收数据。

在万卡集群测试中，Ring AllReduce的性能优势在于通信成本恒定，与系统中处理器数量无关，大大降低了通信开销。此外，由于所有处理器都参与数据传输，可充分利用网络带宽，提高通信效率。理论分析表明，基于环状通信的集群通信算法执行时间几乎和设备数无关，但总通信量和设备数成正比。

**HD AllReduce**

Hierarchical Domain (HD) AllReduce也是一种常见的AllReduce实现算法，阿里巴巴的通信库ACCL采用了这种算法，该算法每次选择节点距离倍增的节点相互通信，每次通信量倍减（或倍增）。

**All-to-All全连接通信**

All-to-All是一种特殊的集合通信模式，每个参与者都需要向其他所有参与者发送不同的数据，同时从其他所有参与者接收不同的数据。

在万卡集群中，All-to-All的应用场景主要包括：

Transformer注意力计算：在"头并行"和"序列并行"之间切换时需要全连接通信。在Transformer架构中，不同的注意力头需要不同的输入数据，通过All-to-All操作可以高效地在不同并行策略之间切换数据分布。

MoE路由：不同的token需要被发送到不同的专家进行处理。在MoE架构中，每个token根据其特征被路由到特定的专家网络，这需要All-to-All通信来实现token与专家之间的精确映射。

**Broadcast**

Broadcast广播是一种1对多的通信原语，存在一个数据发送者和多个数据接收者，在集群环境中将一个节点数据广播至其他节点。简单来讲，广播就是将一个GPU上的数据同时发送到所有其他GPU[[24]](#endnote-24)。

![Image: image_001](./测试V4_images/image_001.png)

在万卡集群中，Broadcast的典型应用场景是模型参数分发。在分布式训练开始时，需要将初始模型参数从主节点广播到所有工作节点。此外，在训练过程中，优化器状态、超参数更新等控制信息也需要通过Broadcast机制分发。

**Reduce Scatter散射规约**

Reduce Scatter属于多对多通信原语，具有多个数据发送者和接收者，在集群内所有节点按维度执行相同Reduce规约运算，再将结果发散到所有节点，等价于节点个数次reduce规约运算后执行scatter操作，其反向操作是AllGather。

![Image: image_002](./测试V4_images/image_002.png)

在万卡集群的梯度聚合场景中，Reduce Scatter发挥着关键作用。在分布式训练中，每个GPU计算完梯度后，需要将梯度聚合后再分散到各个GPU。Reduce Scatter操作可以高效地实现这一过程：先将GPU的数据分割为多个数据块，然后每一块数据按GPU序列进行散射Scatter。对所有的GPU都进行散射之后，当前GPU所接受到的所有数据块进行求和，也就是Reduce规约。

**AllGather收集操作测试**

AllGather是数据的多对多同步全收集，可看作Gather + Broadcast的操作组合。

在万卡集群中，AllGather的主要应用包括状态收集和结果汇总。每一个GPU把数据发送到其余所有GPU，每个GPU对于接受到的数据拼接为完整数据。例如，在训练过程中需要收集所有GPU的训练状态信息（如损失值、准确率等），或者在推理任务中需要收集所有GPU的推理结果进行汇总。

![Image: image_003](./测试V4_images/image_003.png)

### 2.2.2 数据规模

**小包通信测试（1KB级别）**

小包通信测试主要针对实时性要求极高的应用场景，如即时通讯、传感器数据上报、控制信令传输等，其特点是数据包小、发送频率高、延迟敏感。配置为单包大小100B-1KB，每秒发送100-10000次，测试其延迟稳定性和错误率。

**中包通信测试（1-100MB级别）**

中包通信测试是吞吐量基准测试的核心，衡量单位时间内成功传输的数据量，涵盖了大部分模型参数同步和中间结果传输场景。测试参数包括不同数据大小（1KB/1MB/100MB）、不同协议下的每秒传输字节数（B/s）或消息数（msg/s）以及带宽利用率（实际传输速率与理论带宽的比值）、延迟稳定性、错误率（输错误、校验错误、重传次数）等。

**大包通信测试（500MB-2GB级别）**

大包通信测试主要针对批量数据传输场景，如模型检查点保存、大规模数据同步等。主要测试传输总耗时、断点续传效率（中断后恢复的吞吐量恢复速度）、压力测试（同时进行多个大文件传输，模拟真实的多任务并发场景）、并发传输能力（在高并发场景下大文件传输的性能）。

此外，针对大数据包的传输性能有如下优化策略：分片传输：对超过100MB的梯度张量采用分片传输策略，结合NCCL并行通信特性，可提升带宽利用率；带宽预留：在关键路径上预留带宽，确保大文件传输的优先级；智能调度：根据网络负载动态调整传输策略，避免拥塞。

**数据包大小对网络拥塞的影响机制**

不同数据包大小对网络拥塞的影响具有不同的机制。

小包拥塞机制：小包通信的拥塞主要由协议开销和队列管理引起。由于小包的协议头占比高，实际有效载荷小，在高并发场景下会产生大量的协议处理开销，导致CPU使用率升高。同时，小包在交换机队列中的管理复杂度高，容易造成队列溢出和丢包。

中包拥塞机制：中包通信的拥塞主要与带宽竞争相关。当中包流量超过网络链路容量时，会产生排队延迟。由于中包的大小适中，既不会产生过多的协议开销，也不会占用过长的链路传输时间，因此其拥塞特征相对稳定。

大包拥塞机制：大包通信的拥塞具有突发性和持续性特点。当多个大流量同时启动时，会瞬间占用大量网络带宽，导致其他流量被阻塞。特别是在使用PFC机制的无损网络中，大包可能导致长时间的链路占用，产生长尾延迟效应。

### 2.2.3 拓扑结构

**单机多卡**

单机多卡拓扑是万卡集群通信的最底层基础，其性能直接影响上层通信效率。

硬件架构特征：单机多卡系统主要采用NVLink实现GPU间互联。NVLink是NVIDIA设计的芯片级高速通道，专为GPU与GPU之间直接通信而生，通过多条并行链路实现超高带宽，延迟低至300ns。NVLink架构的核心是NVSwitch芯片，实现GPU间全连接拓扑，不同于传统PCIe的树状结构，NVLink采用网状拓扑，每个GPU通过多个NVLink通道直接连接到NVSwitch，再与其他GPU互联。

测试方案设计：

P2P通信测试：使用p2pBandwidthLatencyTest 工具，专门测试单机内GPU之间的P2P通信带宽和延迟。该工具会矩阵式地测试每对GPU之间的双向带宽和延迟，带宽单位为GB/s（数值越大越好），延迟单位为微秒（数值越小越好）。

多卡聚合测试：使用NCCLTest测试多卡同时同步通信带宽。

全负载压测：使用gpu-burn工具让GPU核心、显存跑满（利用率≈100%），检测硬件稳定性（如过热、虚焊、供电问题）。

**跨节点**

跨节点拓扑测试评估节点间高速互联的性能表现，是万卡集群可扩展性的关键。

网络架构设计：跨节点通信主要通过InfiniBand或RoCE技术实现。以NVIDIA H100标准方案为例，IB网络采用QM97xx交换机，每个交换机有64个400Gbps端口，在Leaf和Spine中都是32个下行端口、32个上行端口。

测试方法：

多机通信测试：使用mpirun进行多机通信测试，总进程数 = 节点数 × 每节点 GPU 数。拓扑优化测试：加载预生成的集群拓扑文件，让NCCL优先选择近节点通信（如同一机柜内）。

**跨机架**

跨机架拓扑测试考察多级路由架构的通信效率，是大规模集群性能的关键因素。

拓扑结构特征：跨机架通信需要通过Spine-Leaf多层交换机架构。Meta集群的设计为每个机架包含2个节点，10个机架通过优化网络连接形成Pod，Pod内通信通过Leaf交换机，Pod间通信需要通过Spine交换机。

性能挑战：

收敛比问题：传统的接入-汇聚-核心三级架构存在天然收敛比限制，随着层次增加，带宽呈指数级下降。

路由复杂度：跨机架通信需要经过多次路由跳转，增加了延迟和丢包风险。

拥塞控制：机架间链路容易成为整个集群的瓶颈。

测试评估指标：

跨机架带宽：测量机架间链路的实际可用带宽

跨机架延迟：测量经过多级路由后的端到端延迟

收敛比验证：验证不同层次间的带宽收敛比是否符合设计要求

故障恢复能力：测试单条链路故障时的路由切换性能

**全集群**

全集群拓扑测试模拟真实的大规模训练场景，评估整个集群的端到端通信效率。

测试方案设计：

大规模AllReduce测试：使用万卡规模进行全局梯度同步测试，测试收敛时间和带宽利用率

多任务并发测试：同时运行多个训练任务，测试资源竞争和调度效率

故障注入测试：模拟节点、链路故障，测试系统的容错能力和恢复速度

压力极限测试：逐步增加负载，找到系统的性能拐点和极限容量

### 2.2.4 通信协议

**TCP/IP协议**

TCP/IP作为互联网的"通用语言"，在万卡集群中主要用于控制平面通信和低速数据传输。TCP/IP协议采用三次握手机制，提供可靠传输但开销较大。在跨机房远距离传输时，考虑协议栈处理、路由器跳转等开销，实际端到端延迟普遍达到100-300ms量级[[25]](#endnote-25)。适用于用于集群管理、任务调度、资源分配等控制信息传输；低速数据同步，如日志收集、监控数据、配置信息等；在不同数据中心之间的跨集群通信等。

性能瓶颈：

握手延迟：三次握手机制在高频通信场景下开销显著

拥塞控制：TCP 拥塞控制算法在高速网络中可能成为性能瓶颈

包头开销：TCP 包头 20 字节，在小包通信中占比过高

**RDMA协议**

RDMA（远程直接内存访问）是一种绕过CPU和操作系统直接在内存间传输数据的技术，在万卡集群中发挥着关键作用。RDMA协议中数据直接从内存到网卡，无需CPU参与拷贝；且绕过操作系统内核，减少上下文切换开销，延迟低；通过减少协议处理开销，提高有效载荷比例实现了高带宽利用率。RDMA可通过多种方式实现：（1）InfiniBand RDMA：基于专用InfiniBand网络实现；（2）RoCE（RDMA over Converged Ethernet）：在标准以太网上实现RDMA；（3）iWARP：基于TCP/IP实现RDMA

**NVLink协议**

NVLink是NVIDIA专有的GPU互联技术，在单机多卡系统中提供超高带宽和超低延迟。NVLink架构的核心是NVSwitch芯片，实现GPU间全连接拓扑。不同于传统PCIe的树状结构，NVLink采用网状拓扑，每个GPU通过多个NVLink通道直接连接到NVSwitch，再与其他GPU互联。

**InfiniBand协议**

InfiniBand是专为高性能计算设计的专用网络技术，在万卡集群中提供低延迟、高带宽的连接。

技术特征：

专用网络：不与普通互联网共享带宽，提供专用的高速通道

低延迟：在高性能计算场景中，InfiniBand网络延迟可达纳秒级

高带宽：单条IB链路带宽可达100Gbps，是普通千兆以太网的100倍。

流控机制：InfiniBand采用基于credit的硬件级无损流控机制，通过专用硬件实现超低延迟和高吞吐。其负载均衡机制为集中式（子网管理器SM）+分布式（自适应路由AR）。

常用通信协议对比如下表

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
| 协议类型 | 带宽 | 延迟 | CPU开销 | 适用场景 | 成本 |
| NVLink | 1.8TB/s | 300ns | 极低 | 单机多GPU | 极高 |
| InfiniBand | 400-800Gbps | 纳秒级 | 低 | 大规模集群 | 高 |
| RoCE | 200-400Gbps | 微秒级 | 低 | 成本敏感场景 | 中 |
| TCP/IP | 1-100Gbps | 毫秒级 | 高 | 控制平面 | 低 |

### 2.2.5 并行策略

**数据并行**

数据并行是最常用的分布式训练策略，将训练数据分片分配给不同GPU，每个GPU计算相同的模型但使用不同的数据分片。在万卡集群中，数据并行的通信特征主要体现在梯度同步上。每次训练迭代后，所有GPU需要通过AllReduce操作同步梯度。以256块GPU训练为例，同步时间从本地机房的15ms激增至跨机房的480ms，导致梯度同步耗时占比从25%飙升至67%[[26]](#endnote-26)。

**模型并行**

当模型规模远超单设备内存（如千亿参数模型），数据并行无法使用时，模型并行成为关键 —— 核心思想是“数据复用，模型拆分”：将完整模型的结构拆分为多个部分（模型分片），分配给不同设备，每个设备仅存储部分模型参数，共同处理同一批数据。

**流水线并行**

模型并行（尤其是水平拆分）存在一个关键问题：设备利用率低。例如，GPU1处理完第1-4层后，需等待GPU2处理完第5-8层，才能开始下一批数据的计算，中间存在大量“空闲时间”。流水线并行正是为解决此问题而生 —— 核心思想是“拆分数据为微批次，重叠计算不同批次的层”，类似工厂流水线，让不同设备同时处理不同批次的不同层。每完成一个微批次的前向传播，立即启动其反向传播，最大化设备利用率。

**专家并行**

专家并行（MoE，Mixture of Experts）是一种新型的并行策略，通过稀疏激活的专家网络实现模型参数的高效利用。不同token需要被路由到不同的专家GPU，需要频繁的全连接通信来实现token路由，由于不同专家的激活频率可能差异很大可能会导致负载不均衡。

几种并行策略总结如下：

|  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
| **并行方式** | **核心思想** | **关键操作** | **适用场景** | **核心优势** | **主要挑战** |
| 数据并行 | 模型复制，数据拆分 | 数据分片、梯度同步（AllReduce） | 模型小、数据量大 | 实现简单、负载均衡 | 通信开销高、内存浪费 |
| 模型并行 | 数据复用，模型拆分 | 模型分片、层间特征传输 | 模型大（单设备存不下）、层间依赖强 | 突破内存瓶颈、无数据冗余 | 设备利用率低、负载不均 |
| 流水线并行 | 微批次重叠计算（优化模型并行） | 微批次拆分、1F1B流水线 | 模型层数多、需提升设备利用率 | 高设备利用率、兼容内存优势 | 流水线气泡、微批次管理复杂 |
| 专家并行 | 专家拆分，稀疏激活（MoE） | 门控选专家、Token 路由 | 需提升容量且控制计算成本（大规模MoE模型） | 容量与效率平衡、可扩展性强 | 门控开销、专家负载不均、通信成本高 |

在实际的万卡集群中，通常采用多种并行策略的组合，如3D并行（数据并行 + 模型并行 + 流水线并行）。混合策略的通信会面临不同并行策略可能竞争相同的网络资源；需要复杂的同步机制协调不同策略等多种挑战。

## 2.3 通信测试指标

万卡集群通信测试需要建立系统性的指标体系，涵盖通信算法、节点规模、数据包大小、拓扑结构、性能表现等多个维度。

通信算法包含Ring AllReduce、Tree AllReduce、All-to-All、Broadcast、Reduce Scatter、AllGather。

在测试中可以控制的规模指标：

节点数：参与通信的服务器节点数量

GPU数：参与通信的GPU总数

包大小：测试数据包的字节数（1KB、1MB、100MB、1GB等）

拓扑结构：Tree、Ring、Tree+Ring、Fully Connected

测试性能指标：

平均延迟：数据从源端到目的端所需时间，包括传播延迟、处理延迟和排队延迟。

带宽：单位时间内可传输的数据量，受物理介质和协议栈限制

网络利用率：实际传输数据量与网络总带宽的比值，通常以百分比表示

失败重试数：通信过程中由于各种原因导致的重传次数

网卡阻塞率：网卡队列中等待传输的数据占比

吞吐量（Throughput）：单位时间内网络成功传输的数据量，受延迟、丢包率等制约

丢包率（Packet Loss Rate）：传输过程中丢失数据包的比例，直接影响可靠性，通过比较发送和接收的数据包数量计算

GPU利用率：GPU计算核心的活跃时间占比

## 2.4 线性度测试

在万卡集群中评估通信的线性度Scaling Linearity，是为了衡量随着集群规模扩大时，通信效率是否保持理想比例提升或下降。好的通信线性度意味着集群规模增长能够有效扩展，反之说 明存在通信瓶颈、网络拥塞、协议限制等问题。

万卡集群通信线性度的评估主要关注以下关键指标：

测试规模：覆盖从单节点（8-16卡）到万卡级别的扩展范围，特别关注可能存在拐点的关键规模区间，如256卡、1024卡、4096 卡、8192 卡、10240卡等节点，并采用渐进式扩展策略。

消息大小：包括小消息（1KB-64KB）、中等消息（1-100MB）和大消息（500MB-2GB）三个区间.不同消息大小会触发不同的通信模式和优化策略，小消息主要受延迟影响，大消息主要受带宽影响，中等消息则需要平衡延迟和带宽。

带宽：包括理论带宽（基于硬件规格计算的理论上限）、有效带宽（实际测试获得的可用带宽）、单卡带宽衰减率（多卡通信时单卡实际获得的带宽相对于理论带宽的比例）。

延迟P99：评估通信系统稳定性的关键指标，能够反映系统在高负载下的最坏情况表现，对于大规模训练的稳定性至关重要。

同步开销占比：通信时间在总训练时间中的比例。

线性度：实际性能相对于理论线性扩展性能的比值，线性度的计算公式为：线性度 = 多机多卡总吞吐量 /(单卡吞吐量 × 卡数 × 集群机器数)，取值范围为0~1，数值越接近1表示线性扩展性能越好。理想情况下，线性度应该等于100%，表示性能随集群规模线性扩展。实际情况下，由于通信开销、负载不均衡等因素，线性度通常小于100%。

异常事件：在测试过程中，将重点监控各类异常事件，包括GPU故障、网络中断、软件崩溃、死锁、超时等。

通信算法：分析不同通信算法下的可扩展性差异。

通过万卡集群通信线性度测试，可以定位集群通信瓶颈（是网络带宽瓶颈还是软件栈瓶颈）并为大规模训练提供性能预测依据，判断是否值得扩展至万卡级别。

1. 容器服务 在 TKE 上部署 AI 大模型\_腾讯云<https://cloud.tencent.com/document/product/457/116253> [↑](#endnote-ref-1)
2. GPU 服务器压力测试核心工具全解析:gpu-burn、cpu-burn 与 CUDA Samples-CSDN博客<https://blog.csdn.net/eeeeebv/article/details/151583894> [↑](#endnote-ref-2)
3. gpu压测命令dcgmi - CSDN文库<https://wenku.csdn.net/answer/7or9u9dvfk> [↑](#endnote-ref-3)
4. 显卡控注意了，使用这款软件检测你的显卡能否经得起“烤“\_geeks3d furmark-CSDN博客<https://blog.csdn.net/dntkorg/article/details/143582240> [↑](#endnote-ref-4)
5. 如何评估多GPU并行计算的性能?深度解析与实际案例 - WEBKT<https://www.webkt.com/article/4119> [↑](#endnote-ref-5)
6. nvidia H100 GPU压测 - CSDN文库<https://wenku.csdn.net/answer/1r67ckhrrs> [↑](#endnote-ref-6)
7. nvswitch和tensorrt<https://developer.nvidia.cn/zh-cn/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/> [↑](#endnote-ref-7)
8. https://cloud.tencent.com/document/product/1646/116009 [↑](#endnote-ref-8)
9. 聊聊 GPU 监控那些事:利用率 & 故障等-AI.x-AIGC专属社区-51CTO.COM<https://www.51cto.com/aigc/3474.html> [↑](#endnote-ref-9)
10. Meta万卡GPU集群稳定性剖析与最佳实践-电子发烧友网<https://m.elecfans.com/article/6390335.html> [↑](#endnote-ref-10)
11. 华为昇腾万卡集群揭秘:如何驯服AI算力「巨兽」?\_澎湃新闻<http://m.toutiao.com/group/7513927925666824719/?upstream_biz=doubao> [↑](#endnote-ref-11)
12. 分布式故障恢复 | MindSpore 2.0 教程 | 昇思MindSpore社区<https://www.mindspore.cn/tutorials/experts/zh-CN/r2.0/parallel/fault_recover.html> [↑](#endnote-ref-12)
13. 动态组网场景下故障恢复 | MindSpore 2.4.10 文档 | 昇思MindSpore社区<https://www.mindspore.cn/docs/zh-CN/r2.4.10/model_train/parallel/disaster_recover.html> [↑](#endnote-ref-13)
14. DeepSpeedExamples故障恢复:分布式训练容错机制解析-CSDN博客<https://blog.csdn.net/gitblog_00166/article/details/151745413> [↑](#endnote-ref-14)
15. AI万卡GPU集群交付确认项与日常运维(算力压测、数据倒腾、日常运维)-CSDN博客<https://blog.csdn.net/qq_33957603/article/details/154612148> [↑](#endnote-ref-15)
16. PyTorch 2.4.1中如何解决分布式训练时的GPU内存溢出问题?\_编程语言-CSDN问答<https://ask.csdn.net/questions/8441793> [↑](#endnote-ref-16)
17. ClearML内存优化技巧:处理大型模型的资源管理方案-CSDN博客<https://blog.csdn.net/gitblog_00715/article/details/151786602> [↑](#endnote-ref-17)
18. MegaScale:字节万卡集群-CSDN博客<https://blog.csdn.net/u011486857/article/details/137174394> [↑](#endnote-ref-18)
19. https://www.51cto.com/article/810383.html [↑](#endnote-ref-19)
20. perftracker论文解析与实践<https://blog.csdn.net/Jmilk/article/details/154397014> [↑](#endnote-ref-20)
21. 聊聊 GPU 监控那些事:利用率 & 故障等-AI.x-AIGC专属社区-51CTO.COM<https://www.51cto.com/aigc/3474.html> [↑](#endnote-ref-21)
22. 超大规模GPU集群监控优化:nvitop性能调优终极指南 [特殊字符]-CSDN博客<https://blog.csdn.net/gitblog_00565/article/details/151786713> [↑](#endnote-ref-22)
23. GPU Hot:GPU 服务器可视化管理工具\_唯意喵<http://m.toutiao.com/group/7563234141198664238/?upstream_biz=doubao> [↑](#endnote-ref-23)
24. https://blog.csdn.net/sinat\_37574187/article/details/142636014 [↑](#endnote-ref-24)
25. PHP接单涨薪系列(116):万卡集群训练实战，如何用拓扑感知通信优化跨机房训练-CSDN博客<https://blog.csdn.net/lcz_LYF/article/details/149736022> [↑](#endnote-ref-25)
26. PHP接单涨薪系列(116):万卡集群训练实战，如何用拓扑感知通信优化跨机房训练-CSDN博客<https://blog.csdn.net/lcz_LYF/article/details/149736022> [↑](#endnote-ref-26)
## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=114988018571687&bvid=BV1DktBzLEvb&cid=31551195779&p=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>

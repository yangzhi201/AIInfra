<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->

# CODE02: FastVLM-0.5B é‡åŒ–å¯¹æ¯”(DONE)

> Author by: æ±ªè¢çƒ, ZOMI

åœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­ï¼Œæ¨¡å‹çš„å¤§å°å’Œè®¡ç®—æ•ˆç‡å¾€å¾€æ˜¯å®é™…éƒ¨ç½²æ—¶éœ€è¦è€ƒè™‘çš„å…³é”®å› ç´ ã€‚ç‰¹åˆ«æ˜¯å¯¹äºåƒ FastVLM è¿™æ ·çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå³ä½¿æ˜¯ 0.5B å‚æ•°è§„æ¨¡ï¼Œåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œä¹Ÿå¯èƒ½é¢ä¸´æŒ‘æˆ˜ã€‚

æ¨¡å‹é‡åŒ–æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡å‡å°‘æ¨¡å‹å‚æ•°å’Œè®¡ç®—çš„æ•°å€¼ç²¾åº¦æ¥é™ä½æ˜¾å­˜å ç”¨å¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚ä»Šå¤©æˆ‘ä»¬å°±æ¥å®é™…å¯¹æ¯”ä¸åŒé‡åŒ–ç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

## 1. æ¨¡å‹é‡åŒ–åŸºç¡€

é‡åŒ–çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç¥ç»ç½‘ç»œä¸­çš„æµ®ç‚¹æ•°æƒé‡å’Œæ¿€æ´»å€¼è½¬æ¢ä¸ºå®šç‚¹æ•°è¡¨ç¤ºã€‚æœ€å¸¸ç”¨çš„æ˜¯å°† 32 ä½æµ®ç‚¹æ•°ï¼ˆFP32ï¼‰è½¬æ¢ä¸ºæ›´ä½ä½æ•°çš„æ•´æ•°è¡¨ç¤ºã€‚

å¯¹äºæƒé‡é‡åŒ–ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ä»¥ä¸‹å…¬å¼å°†æµ®ç‚¹æ•°è½¬æ¢ä¸ºæ•´æ•°ï¼š

$$ q = \text{round}(r / s + z) $$

å…¶ä¸­ï¼š

- $r$ æ˜¯åŸå§‹æµ®ç‚¹æ•°å€¼
- $s$ æ˜¯ç¼©æ”¾å› å­ï¼ˆscaleï¼‰
- $z$ æ˜¯é›¶ç‚¹åç§»ï¼ˆzero pointï¼‰
- $q$ æ˜¯é‡åŒ–åçš„æ•´æ•°å€¼

å¸¸è§çš„é‡åŒ–é…ç½®æœ‰ï¼š

- W4A4ï¼šæƒé‡å’Œæ¿€æ´»å€¼éƒ½ä½¿ç”¨ 4 ä½æ•´æ•°
- W8A8ï¼šæƒé‡å’Œæ¿€æ´»å€¼éƒ½ä½¿ç”¨ 8 ä½æ•´æ•° 
- W4A16ï¼šæƒé‡ä½¿ç”¨ 4 ä½æ•´æ•°ï¼Œæ¿€æ´»å€¼ä½¿ç”¨ 16 ä½æ•´æ•°

## 2. å®éªŒç¯å¢ƒå‡†å¤‡

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…å¿…è¦çš„åº“ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face çš„ transformers åº“åŠ è½½æ¨¡å‹ï¼Œä»¥åŠ accelerate åº“æ¥å¸®åŠ©ç®¡ç†æ˜¾å­˜ä½¿ç”¨ã€‚

```python
# å®‰è£…æ‰€éœ€åº“
!pip install transformers accelerate torch pillow
```

    Requirement already satisfied: transformers in /home/yswang/miniforge3/lib/python3.12/site-packages (4.56.1)
    Requirement already satisfied: accelerate in /home/yswang/miniforge3/lib/python3.12/site-packages (1.10.1)
    Requirement already satisfied: torch in /home/yswang/miniforge3/lib/python3.12/site-packages (2.7.1)
    Requirement already satisfied: pillow in /home/yswang/miniforge3/lib/python3.12/site-packages (11.3.0)
    Requirement already satisfied: filelock in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (3.19.1)
    Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (0.34.4)
    Requirement already satisfied: numpy>=1.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (2.2.6)
    Requirement already satisfied: packaging>=20.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (25.0)
    Requirement already satisfied: pyyaml>=5.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (6.0.2)
    Requirement already satisfied: regex!=2019.12.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (2025.9.1)
    Requirement already satisfied: requests in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (2.32.4)
    Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (0.22.0)
    Requirement already satisfied: safetensors>=0.4.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (0.6.2)
    Requirement already satisfied: tqdm>=4.27 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (4.67.1)
    Requirement already satisfied: fsspec>=2023.5.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)
    Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)
    Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)
    Requirement already satisfied: psutil in /home/yswang/miniforge3/lib/python3.12/site-packages (from accelerate) (7.0.0)
    Requirement already satisfied: setuptools in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (79.0.1)
    Requirement already satisfied: sympy>=1.13.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (1.14.0)
    Requirement already satisfied: networkx in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (3.5)
    Requirement already satisfied: jinja2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (3.1.6)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (12.6.77)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (12.6.77)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (12.6.80)
    Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (9.5.1.17)
    Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (12.6.4.1)
    Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (11.3.0.4)
    Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (10.3.7.77)
    Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (11.7.1.2)
    Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (12.5.4.2)
    Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (0.6.3)
    Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (2.26.2)
    Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (12.6.77)
    Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (12.6.85)
    Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (1.11.1.6)
    Requirement already satisfied: triton==3.3.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch) (3.3.1)
    Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)
    Requirement already satisfied: charset_normalizer<4,>=2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from requests->transformers) (3.4.2)
    Requirement already satisfied: idna<4,>=2.5 in /home/yswang/miniforge3/lib/python3.12/site-packages (from requests->transformers) (3.10)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)
    Requirement already satisfied: certifi>=2017.4.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)

ç„¶åï¼Œæˆ‘ä»¬å¯¼å…¥å¿…è¦çš„æ¨¡å—ï¼š

```python
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoImageProcessor,AutoProcessor
from PIL import Image
import os
```

    /home/yswang/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
      from .autonotebook import tqdm as notebook_tqdm

## 3. åŠ è½½ FastVLM-0.5B å’Œæ•°æ®å‡†å¤‡

è®©æˆ‘ä»¬å…ˆåŠ è½½åŸå§‹çš„ FastVLM-0.5B æ¨¡å‹ï¼Œä½œä¸ºåŸºå‡†å‚è€ƒã€‚

### åŠ è½½æ¨¡å‹

å®Œæˆä¸‹è½½ä¹‹åä¾¿å¯ä»¥åŠ è½½ FastVLM è·‘ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹

```python
# pip install -U "transformers>=4.41" accelerate safetensors timm sentencepiece

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# === 1) è®¾å¤‡ä¸ç²¾åº¦ ===
use_fp16 = torch.cuda.is_available()  # æœ‰ GPU å°±ç”¨åŠç²¾åº¦
dtype = torch.float16 if use_fp16 else torch.float32
print(f"CUDA: {torch.cuda.is_available()}  |  dtype: {dtype}")

# === 2A) ç›´æ¥ä» Hugging Face Hub åŠ è½½ï¼ˆè”ç½‘ï¼‰===
MODEL_ID = "apple/FastVLM-0.5B"  # å®˜æ–¹æƒé‡ä»“åº“
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model_fp16 = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=dtype,
    device_map="auto",
    trust_remote_code=True,
)

# æ‹¿åˆ°æ¨¡å‹å†…ç½®çš„å›¾åƒé¢„å¤„ç†å™¨å¼•ç”¨ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨ï¼›æ­¤å¤„ä¸åšæ¨ç†
image_processor = model_fp16.get_vision_tower().image_processor
print("Tokenizer / FP16 model loaded. (Hub)")
```

    CUDA: True  |  dtype: torch.float16


    `torch_dtype` is deprecated! Use `dtype` instead!


    Tokenizer / FP16 model loaded. (Hub)

è¿™æ®µä»£ç é¦–å…ˆæ£€æŸ¥æˆ‘ä»¬æ˜¯å¦æœ‰å¯ç”¨çš„ GPUï¼Œå› ä¸ºé‡åŒ–å®éªŒåœ¨ GPU ä¸Šæ•ˆæœæ›´æ˜æ˜¾ã€‚ç„¶åæˆ‘ä»¬åŠ è½½äº†æ¨¡å‹çš„åˆ†è¯å™¨å’Œå›¾åƒå¤„ç†å·¥å…·ï¼Œæœ€ååŠ è½½äº†åŸå§‹çš„ FP16 ç²¾åº¦æ¨¡å‹ä½œä¸ºåŸºå‡†ã€‚ä¹‹åæ˜¯æ„é€ å¸¦ <image> å ä½ç¬¦çš„å¯¹è¯æ¨¡æ¿ï¼ŒFastVLM ç³»åˆ—åœ¨æ¨ç†æ—¶ä¼šçº¦å®šä¸€ä¸ªç‰¹æ®Šå›¾åƒ tokenï¼ˆä»£ç é‡Œçš„ IMAGE_TOKEN_INDEX=-200ï¼‰ï¼Œä»£è¡¨â€œè¿™é‡Œæœ‰ä¸€å¼ å›¾ç‰‡â€ã€‚å› æ­¤å…ˆæŠŠå­—ç¬¦ä¸²åœ¨ <image> å¤„åˆ†å‰²ï¼Œç„¶åæŠŠè¿™ä¸ªç‰¹æ®Š token æ’å…¥åˆ° input_ids å¯¹åº”ä½ç½®ã€‚ä¹‹åç”¨æ¨¡å‹è‡ªå¸¦çš„å›¾åƒé¢„å¤„ç†å™¨å¾—åˆ° pixel_valuesã€‚æœ€åæŠŠå›¾åƒç‰¹å¾åŒæ—¶è¾“å…¥ï¼Œä¸æ–‡æœ¬ token ä¸€èµ·é©±åŠ¨è§£ç å™¨ç”Ÿæˆå›ç­”ï¼ˆä¹Ÿå³æ¨æµ‹è§£ç ï¼‰ã€‚

æ³¨æ„æˆ‘ä»¬ä½¿ç”¨äº†`torch_dtype=torch.float16`å‚æ•°ï¼Œè¿™ä¼šå°†æ¨¡å‹åŠ è½½ä¸ºåŠç²¾åº¦ï¼ˆ16 ä½ï¼‰è€Œä¸æ˜¯é»˜è®¤çš„ 32 ä½ï¼Œè¿™å·²ç»æ˜¯ä¸€ç§ç®€å•çš„é‡åŒ–å½¢å¼äº†ã€‚

å› ä¸ºæ˜¯å¤–ç½‘ï¼Œå¦‚æœä½ ä½¿ç”¨ä¸Šè¿°ä»£ç å‡ºç°ç½‘ç»œè¿æ¥å¤±è´¥çš„é—®é¢˜ï¼Œå¯ä»¥é‡‡ç”¨æ‰‹åŠ¨ä¸‹è½½çš„æ–¹æ³•ã€‚ä¹‹å‰ä»‹ç»äº†ç”¨ HF ä¸‹è½½çš„æ–¹æ³•ï¼Œè¿™é‡Œä»‹ç»ä¸€ç§æ–°çš„åˆ©ç”¨ git clone çš„æ–¹æ³•ï¼Œå…ˆå°† FastVLM clone åˆ°æœ¬åœ°ï¼š

```python

# Make sure git-lfs is installed (https://git-lfs.com)
%cd models
!git lfs install
!git clone git@hf.co:apple/FastVLM-0.5B
# å¦‚æœä½ åªæ˜¯å½“ä½œæ•°æ®é›†ä½¿ç”¨ï¼Œåˆ™å¯ä»¥åˆ é™¤ .git ç›®å½•
!cd FastVLM-0.5B
!rm -rf .git
%cd ..
```

    /home/yswang/tvm_learn/cuda/AIInfra/05Infer/06CompDistill/models
    Updated git hooks.
    Git LFS initialized.
    Cloning into 'FastVLM-0.5B'...
    remote: Enumerating objects: 27, done.[K
    remote: Counting objects: 100% (8/8), done.[K
    remote: Compressing objects: 100% (8/8), done.[K
    remote: Total 27 (delta 2), reused 0 (delta 0), pack-reused 19 (from 1)[K
    Receiving objects: 100% (27/27), 2.78 MiB | 1.93 MiB/s, done.
    Resolving deltas: 100% (4/4), done.
    Updating files: 100% (15/15), done.
    Filtering content: 100% (3/3), 1.41 GiB | 6.53 MiB/s, done.
    /home/yswang/tvm_learn/cuda/AIInfra/05Infer/06CompDistill

### å›¾ç‰‡æ•°æ®å‡†å¤‡

ç”±äº FastVLM å±äºè§†è§‰+æ–‡æœ¬çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå› æ­¤éœ€è¦å‡†å¤‡å›¾ç‰‡ç›¸å…³çš„æ•°æ®é›†

```python
import os, urllib.request

# 1) è‡ªå®šä¹‰ User-Agentï¼Œé¿å… 403
opener = urllib.request.build_opener()
opener.addheaders = [("User-Agent", "Mozilla/5.0 (compatible; demo-notebook; +https://example.org)")]
urllib.request.install_opener(opener)

# 2) ç›®æ ‡ç›®å½•
os.makedirs("images", exist_ok=True)

# 3) ç›´æ¥ä½¿ç”¨ Wikimedia çš„åŸå›¾ç›´é“¾ï¼ˆé Special:FilePathï¼Œé¿å…é¢å¤–è·³è½¬ï¼‰
assets = {
    "cat.jpg": "https://upload.wikimedia.org/wikipedia/commons/4/44/Cat_Domestic.jpg",
    "receipt_1895.jpg": "https://upload.wikimedia.org/wikipedia/commons/4/47/1895_Benjamin_French_%26_Co._Receipt.jpg",
    "receipt_shell.jpg": "https://upload.wikimedia.org/wikipedia/commons/3/34/Shell-Gas-Station-Receipt-MasterCard.jpg",
    "landscape_monet.jpg": "https://upload.wikimedia.org/wikipedia/commons/b/b7/Claude_Monet_-_Landscape%2C_The_Parc_Monceau.jpg",
}

# 4) ä¸‹è½½å¹¶ç”Ÿæˆå¯ç›´æ¥å¼•ç”¨çš„å˜é‡
local_paths = {}
for fname, url in assets.items():
    dest = os.path.join("images", fname)
    if not os.path.exists(dest):
        print(f"Downloading {fname} ...")
        urllib.request.urlretrieve(url, dest)
    local_paths[fname.split('.')[0]] = dest

# ä¾›åç»­ç›´æ¥ä½¿ç”¨çš„å˜é‡
IMAGE_CAT = local_paths["cat"]                     # 'images/cat.jpg'
IMAGE_RECEIPT_1895 = local_paths["receipt_1895"]   # 'images/receipt_1895.jpg'
IMAGE_RECEIPT_SHELL = local_paths["receipt_shell"] # 'images/receipt_shell.jpg'
IMAGE_LANDSCAPE = local_paths["landscape_monet"]   # 'images/landscape_monet.jpg'

print("Ready:", IMAGE_CAT, IMAGE_RECEIPT_1895, IMAGE_RECEIPT_SHELL, IMAGE_LANDSCAPE, sep="\n")
```

    Downloading cat.jpg ...
    Downloading receipt_1895.jpg ...
    Downloading receipt_shell.jpg ...
    Downloading landscape_monet.jpg ...
    Ready:
    images/cat.jpg
    images/receipt_1895.jpg
    images/receipt_shell.jpg
    images/landscape_monet.jpg

ä»¥ cat ä¸ºä¾‹ï¼Œè¾“å…¥çš„å›¾ç‰‡ä¸ºï¼š

```python
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# å›¾ç‰‡è·¯å¾„
img_path = r"./images/cat.jpg"

# è¯»å–å¹¶æ˜¾ç¤º
img = mpimg.imread(img_path)
plt.imshow(img)
plt.axis('off')  # ä¸æ˜¾ç¤ºåæ ‡è½´
plt.show()

```

![png](./images/output_12_0.png)
    
### ä»æœ¬åœ°åŠ è½½æ¨¡å‹

æ¯”å¦‚ä½ å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°çš„`AI Infra/05Infer/06CompDistill/models`ä¹‹åï¼Œä¾¿å¯ä»¥ä»æœ¬åœ°æ¨¡å‹ç›®å½•è¿›è¡ŒåŠ è½½äº†ã€‚æˆ‘ä»¬åŠ è½½åˆšåˆšå‡†å¤‡çš„ cat å›¾ç‰‡ï¼š

```python
import os
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"

import torch
from PIL import Image
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_DIR = "./models/FastVLM-0.5B"
IMAGE_TOKEN_INDEX = -200  # æ¨¡å‹ä»£ç çº¦å®šçš„å ä½ token id

# 1) åŠ è½½ tokenizer / æ¨¡å‹ï¼ˆtrust_remote_code å¾ˆå…³é”®ï¼šå¯ç”¨ä»“åº“é‡Œçš„ llava_qwen.pyï¼‰
tok = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",                 # æˆ–æ”¹æˆ .to("cuda") æ‰‹åŠ¨æ”¾ GPU
    trust_remote_code=True,
)

# 2) æ„é€ å¸¦ <image> çš„èŠå¤©æ¨¡æ¿
messages = [{"role": "user", "content": "<image>\n æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡ã€‚"}]
rendered = tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
pre, post = rendered.split("<image>", 1)

pre_ids  = tok(pre,  return_tensors="pt", add_special_tokens=False).input_ids
post_ids = tok(post, return_tensors="pt", add_special_tokens=False).input_ids
img_tok  = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)
input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device)
attention_mask = torch.ones_like(input_ids, device=model.device)

# 3) é€šè¿‡â€œæ¨¡å‹è‡ªå¸¦â€çš„ image_processor åšå›¾åƒé¢„å¤„ç†ï¼ˆç¦»çº¿ï¼Œä¸éœ€è¦ preprocessor_config.jsonï¼‰
img_path = "./images/cat.jpg"   # è¿™é‡Œæˆ‘é€‰å–çš„ cat çš„å›¾ç‰‡
img = Image.open(img_path).convert("RGB")

px = model.get_vision_tower().image_processor(
    images=img, return_tensors="pt"
)["pixel_values"].to(model.device, dtype=model.dtype)

# 4) ç”Ÿæˆ
with torch.no_grad():
    out = model.generate(
        inputs=input_ids,
        attention_mask=attention_mask,
        images=px,
        max_new_tokens=128,
    )
print(tok.decode(out[0], skip_special_tokens=True))

```

    è¿™æ˜¯ä¸€å¼ æ‹æ‘„æ—¶çš„çŒ«çš„ç…§ç‰‡ã€‚è¿™åªçŒ«èººåœ¨è‰åœ°ä¸Šï¼Œä¼¼ä¹å¾ˆäº«å—è¿™ç‰‡è‰åœ°çš„åœ°æ–¹ã€‚å®ƒçœ‹èµ·æ¥åƒæ˜¯åªå¹´è½»çš„çŒ«å’ªï¼Œå› ä¸ºå®ƒåœ¨è‰åœ°ä¸Šä¹Ÿåƒè‰äº†ï¼Œä¹Ÿè®¸æ˜¯åœ¨é‚£é‡Œç©æˆ–è€…åªæ˜¯ä¸ºäº†æ”¾æ¾è€Œå·²ã€‚
    è¿™å¹…å›¾ç‰‡æ•æ‰åˆ°äº†çŒ«çš„å¥½å¥‡å’Œæ»¡è¶³æ„Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è‡ªç„¶ç¯å¢ƒä¸‹è‡ªç”±è‡ªåœ¨çš„æ ·å­ï¼Œè€Œä¸”å®ƒè¢«è§‚å¯Ÿåˆ°èººåœ¨è‰åœ°ä¸Šè¿™ä¸€ç‚¹ã€‚
    
    
    
    
    ç„¶è€Œï¼Œç”±äºæ²¡æœ‰å…³äºçŒ«é¢œè‰²å˜åŒ–çš„å› ç´ ï¼Œæ‰€ä»¥æ— æ³•ç¡®ä¿¡åœ°è¯´å‡ºè¿™ç§çŒ«çš„å…·ä½“ç§ç±»ã€‚
    åœ¨ä¸“ä¸šé¢†åŸŸã€å® ç‰©é¢†åŸŸæˆ–äººç±»é¢†åŸŸä¸­å‘ç°çš„çŒ«å¯èƒ½æ˜¯ä»»ä½•ä¸€ç§

## 4. å‡†å¤‡è¯„ä¼°å‡½æ•°

ä¸ºäº†å…¬å¹³æ¯”è¾ƒä¸åŒé‡åŒ–é…ç½®ï¼Œæˆ‘ä»¬éœ€è¦ç›¸åŒçš„æµ‹è¯„ä¼°æ–¹æ³•ã€‚

### æ¨¡å‹åŠ è½½å‡½æ•°

æˆ‘ä»¬å…ˆå†™ä¸€ä¸ªåŠ è½½å‡½æ•°æ¥åŠ è½½æ¨¡å‹

```python

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def load_fastvlm(model_path_or_id="apple/FastVLM-0.5B", prefer_fp16=None, device_map="auto"):
    """
    è¿”å›: tokenizer, model, image_processor, device, dtype
    - model_path_or_id: å¯å¡« Hugging Face ä»“åº“åï¼Œæˆ–æœ¬åœ°ç›®å½•è·¯å¾„
    - prefer_fp16: None=æœ‰ CUDA åˆ™ FP16ï¼Œå¦åˆ™ FP32
    """
    if prefer_fp16 is None:
        prefer_fp16 = torch.cuda.is_available()
    dtype = torch.float16 if prefer_fp16 else torch.float32
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = AutoTokenizer.from_pretrained(model_path_or_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path_or_id,
        torch_dtype=dtype,
        device_map=device_map,
        trust_remote_code=True,
    )
    image_processor = model.get_vision_tower().image_processor
    print(f"[Loaded] {model_path_or_id}  device={device}  dtype={dtype}")
    return tokenizer, model, image_processor, device, dtype

# åŠ è½½æ¨¡å‹
tokenizer, model_fp16, image_processor, device, dtype = load_fastvlm(
    model_path_or_id="apple/FastVLM-0.5B",   # ä½ ä¹Ÿå¯ä»¥æ¢æˆæœ¬åœ°è·¯å¾„
    prefer_fp16=True,                        # ä½¿ç”¨ FP16ï¼ˆæ˜¾å­˜å ç”¨æ›´å°ï¼‰
    device_map="auto"                        # è‡ªåŠ¨åˆ†é…è®¾å¤‡
)
model_name = "apple/FastVLM-0.5B"

# === 2. å®šä¹‰å›¾ç‰‡å’Œé—®é¢˜ ===
test_image = r"./images/cat.jpg"
test_question = "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"

```

    [Loaded] apple/FastVLM-0.5B  device=cuda  dtype=torch.float16

åˆ©ç”¨`from_pretrained(...)`æŒ‰åç§°æˆ–è·¯å¾„æ‰¾åˆ°å¹¶å®ä¾‹åŒ–æ¨¡å‹/åˆ†è¯å™¨â€,è·å– `image_processor` ç”¨äºå›¾åƒâ†’`pixel_values` çš„é¢„å¤„ç†ï¼Œä¾¿äºåç»­ `generate(inputs=..., images=...)` æ¨ç†ã€‚

### è¯„ä¼°å‡½æ•°

```python
import time
import torch
from PIL import Image

def evaluate_model(
    model=None,
    image=r"./images/cat.jpg",  # é»˜è®¤å›¾ç‰‡è·¯å¾„
    question="è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ",                        # é»˜è®¤é—®é¢˜
    tokenizer=None,
    image_processor=None,
    device="cuda" if torch.cuda.is_available() else "cpu",  # é»˜è®¤è®¾å¤‡
    iterations=5,
    max_new_tokens=50):
    """
    è¯„ä¼° FastVLMï¼šè¿”å›ç”Ÿæˆç­”æ¡ˆã€å¹³å‡å»¶è¿Ÿ(ms)ã€æ˜¾å­˜å ç”¨(MB)ã€‚
    å‚æ•° image å¯ä¸º PIL.Image æˆ–å›¾ç‰‡è·¯å¾„å­—ç¬¦ä¸²ã€‚
    å…¼å®¹å¤šå¡(device_map='auto')ï¼šæ–‡æœ¬å¼ é‡æ”¾åˆ°åµŒå…¥å±‚æ‰€åœ¨è®¾å¤‡ï¼Œå›¾åƒå¼ é‡æ”¾åˆ°è§†è§‰å¡”æ‰€åœ¨è®¾å¤‡ã€‚
    """
    model.eval()
    IMAGE_TOKEN_INDEX = -200  # FastVLM çº¦å®šçš„å›¾åƒå ä½ token

    # === 0) è¯†åˆ«å®é™…è®¾å¤‡ï¼ˆå¤šå¡æ—¶éå¸¸å…³é”®ï¼‰
    text_device   = model.get_input_embeddings().weight.device
    vision_device = next(model.get_vision_tower().parameters()).device

    # === 1) æ–‡æœ¬ï¼šç”¨ chat æ¨¡æ¿å¹¶åœ¨ <image> å¤„æ’å…¥å ä½ token
    messages = [{"role": "user", "content": "<image>\n" + question}]
    rendered = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    pre, post = rendered.split("<image>", 1)

    pre_ids  = tokenizer(pre,  return_tensors="pt", add_special_tokens=False).input_ids
    post_ids = tokenizer(post, return_tensors="pt", add_special_tokens=False).input_ids
    img_tok  = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)

    input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(text_device)   # â† æ–‡æœ¬åˆ°åµŒå…¥å±‚æ‰€åœ¨è®¾å¤‡
    attention_mask = torch.ones_like(input_ids, device=text_device)

    # === 2) å›¾åƒï¼šç”¨æ¨¡å‹å†…ç½® image_processor å¾—åˆ° pixel_valuesï¼Œå¹¶æ”¾åˆ°è§†è§‰å¡”æ‰€åœ¨è®¾å¤‡
    if isinstance(image, str):
        img = Image.open(image).convert("RGB")
    else:
        img = image.convert("RGB")
    pixel_values = image_processor(images=img, return_tensors="pt")["pixel_values"]
    pixel_values = pixel_values.to(vision_device, dtype=model.dtype)              # â† å›¾åƒåˆ°è§†è§‰å¡”æ‰€åœ¨è®¾å¤‡

    # === 3) è¯„æµ‹ï¼šå¤šæ¬¡ç”Ÿæˆå–å¹³å‡å»¶è¿Ÿ
    if torch.cuda.is_available():
        # åˆ†åˆ«å¯¹ç”¨åˆ°çš„ GPU å¤ä½å³°å€¼ & åŒæ­¥
        used_cuda = []
        for dev in {text_device, vision_device}:
            if isinstance(dev, torch.device) and dev.type == "cuda":
                torch.cuda.reset_peak_memory_stats(dev)
                used_cuda.append(dev)
        torch.cuda.synchronize()

    start = time.time()
    last_out = None
    for _ in range(iterations):
        with torch.no_grad():
            last_out = model.generate(
                inputs=input_ids,               # æ³¨æ„ï¼šFastVLM è‡ªå®šä¹‰ generate æ¥å£æ˜¯ inputs= + images=
                attention_mask=attention_mask,
                images=pixel_values,
                max_new_tokens=max_new_tokens,
            )
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    avg_latency_ms = (time.time() - start) / iterations * 1000.0

    # === 4) æ˜¾å­˜å ç”¨ï¼ˆå¤šå¡æ±‚å’Œï¼›æ—  GPU ç½® 0ï¼‰
    memory_used_mb = 0.0
    if torch.cuda.is_available():
        mem = 0
        for dev in {text_device, vision_device}:
            if isinstance(dev, torch.device) and dev.type == "cuda":
                mem += torch.cuda.max_memory_allocated(dev)
        memory_used_mb = mem / (1024 ** 2)

    answer = tokenizer.decode(last_out[0], skip_special_tokens=True)
    return {"answer": answer, "avg_latency_ms": avg_latency_ms, "memory_used_mb": memory_used_mb}

```

## 5. åŸºå‡†æµ‹è¯•ï¼šFP16 æ¨¡å‹

è®©æˆ‘ä»¬å…ˆæµ‹è¯•åŸå§‹çš„ FP16 æ¨¡å‹ä½œä¸ºåŸºå‡†ï¼š

```python
# æ¸…ç©ºç¼“å­˜ï¼Œç¡®ä¿æµ‹é‡å‡†ç¡®
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.synchronize()

print("æ­£åœ¨è¯„ä¼° FP16 æ¨¡å‹...")

# iterations / max_new_tokens å¯æŒ‰éœ€è°ƒæ•´
fp16_results = evaluate_model(
    model_fp16,           # æ¥è‡ª load_fastvlm() è¿”å›çš„æ¨¡å‹
    test_image,           # PIL.Image æˆ–è·¯å¾„å­—ç¬¦ä¸²
    test_question,        # ä½ çš„é—®é¢˜æ–‡æœ¬
    tokenizer,            # load_fastvlm() è¿”å›
    image_processor,      # model.get_vision_tower().image_processor
    device,               # "cuda" æˆ– "cpu"
    iterations=5,
    max_new_tokens=50,
)

print("FP16 æ¨¡å‹ç»“æœ:")
print(f"ç­”æ¡ˆ: {fp16_results['answer']}")
print(f"å¹³å‡å»¶è¿Ÿ: {fp16_results['avg_latency_ms']:.2f} ms")
# æ—  GPU æ—¶ evaluate_model ä¼šæŠŠæ˜¾å­˜å ç”¨ç½® 0
print(f"æ˜¾å­˜å ç”¨: {fp16_results['memory_used_mb']:.2f} MB")
```

    æ­£åœ¨è¯„ä¼° FP16 æ¨¡å‹...
    FP16 æ¨¡å‹ç»“æœ:
    ç­”æ¡ˆ: è¿™å¹…ç”»çš„ä¸»é¢˜æ˜¯ä¸€åªå¹´è½»çš„ç°è™æ–‘çŒ«ï¼Œå®ƒæ­£èººåœ¨é•¿é’åœ°ä¸Šå®‰é™åœ°è§‚å¯Ÿç€å‘¨å›´çš„ç¯å¢ƒã€‚è¿™åªçŒ«è¢«ç”»æ¡†åœ¨ç»¿è‰²è‰åœ°ä¸Šï¼Œæ˜¾ç¤ºå‡ºå®ƒå’Œå¹³æ”¾æ¾çš„ä¸¾æ­¢ã€‚è¿™å¼ ç…§ç‰‡æ•æ‰åˆ°äº†çŒ«ä¸è‰åœ°ä¹‹é—´
    å¹³å‡å»¶è¿Ÿ: 705.43 ms
    æ˜¾å­˜å ç”¨: 2547.38 MB

åœ¨è¿›è¡Œè¯„ä¼°å‰ï¼Œæˆ‘ä»¬è°ƒç”¨äº†`torch.cuda.empty_cache()`æ¥æ¸…ç©º GPU ç¼“å­˜ï¼Œç¡®ä¿æ˜¾å­˜æµ‹é‡çš„å‡†ç¡®æ€§ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨å‰é¢å®šä¹‰çš„è¯„ä¼°å‡½æ•°æ¥æµ‹è¯•æ¨¡å‹ã€‚

## 6. é‡åŒ–é…ç½® 1ï¼šW8A8

ç°åœ¨è®©æˆ‘ä»¬å°è¯• 8 ä½é‡åŒ–ï¼Œè¿™æ˜¯ä¸€ç§å¸¸ç”¨çš„å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦çš„é‡åŒ–ç­–ç•¥ã€‚

```python
# å®‰è£… bitsandbytes åº“ç”¨äºé‡åŒ–
!pip install bitsandbytes

# åŠ è½½ 8 ä½é‡åŒ–æ¨¡å‹
from transformers import BitsAndBytesConfig

# é…ç½® 8 ä½é‡åŒ–å‚æ•°
bnb_config_8bit = BitsAndBytesConfig(
    load_in_8bit=True,  # å¯ç”¨ 8 ä½é‡åŒ–
)

# æ¸…ç©ºç¼“å­˜
torch.cuda.empty_cache()

# åŠ è½½ 8 ä½é‡åŒ–æ¨¡å‹
print("æ­£åœ¨åŠ è½½ W8A8 é‡åŒ–æ¨¡å‹...")
model_w8a8 = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config_8bit,
    device_map="auto"
)

# è¯„ä¼° 8 ä½é‡åŒ–æ¨¡å‹
print("æ­£åœ¨è¯„ä¼° W8A8 æ¨¡å‹...")
w8a8_results = evaluate_model(
    model_w8a8, 
    test_image, 
    test_question, 
    tokenizer, 
    image_processor, 
    device
)

print(f"W8A8 æ¨¡å‹ç»“æœ:")
print(f"ç­”æ¡ˆ: {w8a8_results['answer']}")
print(f"å¹³å‡å»¶è¿Ÿ: {w8a8_results['avg_latency_ms']:.2f} ms")
print(f"æ˜¾å­˜å ç”¨: {w8a8_results['memory_used_mb']:.2f} MB")
```

    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
    To disable this warning, you can either:
    	- Avoid using `tokenizers` before the fork if possible
    	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


    Requirement already satisfied: bitsandbytes in /home/yswang/miniforge3/lib/python3.12/site-packages (0.47.0)
    Requirement already satisfied: torch<3,>=2.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from bitsandbytes) (2.7.1)
    Requirement already satisfied: numpy>=1.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from bitsandbytes) (2.2.6)
    Requirement already satisfied: filelock in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)
    Requirement already satisfied: typing-extensions>=4.10.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)
    Requirement already satisfied: setuptools in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (79.0.1)
    Requirement already satisfied: sympy>=1.13.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)
    Requirement already satisfied: networkx in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.5)
    Requirement already satisfied: jinja2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)
    Requirement already satisfied: fsspec in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)
    Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (9.5.1.17)
    Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)
    Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)
    Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)
    Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)
    Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)
    Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (0.6.3)
    Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (2.26.2)
    Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)
    Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)
    Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)
    Requirement already satisfied: triton==3.3.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch<3,>=2.2->bitsandbytes) (3.3.1)
    Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)
    æ­£åœ¨åŠ è½½ W8A8 é‡åŒ–æ¨¡å‹...
    æ­£åœ¨è¯„ä¼° W8A8 æ¨¡å‹...
    W8A8 æ¨¡å‹ç»“æœ:
    ç­”æ¡ˆ: The image showcases a plain white background with a clear focus on a company logo in the top left corner, a partial view of a silver dog emoji, and a segment of a phone screen that is visible along with a label that reads "2"
     
    
    å¹³å‡å»¶è¿Ÿ: 3271.41 ms
    æ˜¾å­˜å ç”¨: 3372.49 MB

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº† bitsandbytes åº“æä¾›çš„ 8 ä½é‡åŒ–åŠŸèƒ½ã€‚é€šè¿‡è®¾ç½®`load_in_8bit=True`ï¼Œæˆ‘ä»¬å‘Šè¯‰åº“å°†æ¨¡å‹æƒé‡åŠ è½½ä¸º 8 ä½æ•´æ•°ã€‚

ç†è®ºä¸Šï¼Œ8 ä½é‡åŒ–å¯ä»¥å°†æ¨¡å‹å¤§å°å‡å°‘çº¦ 4 å€ï¼ˆä» 32 ä½æµ®ç‚¹æ•°åˆ° 8 ä½æ•´æ•°ï¼‰ï¼Œä½†å®é™…æ˜¾å­˜èŠ‚çœå¯èƒ½ç•¥å°‘ï¼Œå› ä¸ºè¿˜éœ€è¦å­˜å‚¨ä¸€äº›é‡åŒ–å‚æ•°ï¼ˆå¦‚ç¼©æ”¾å› å­ï¼‰ã€‚

## 7. é‡åŒ–é…ç½® 2ï¼šW4A4

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°è¯•æ›´æ¿€è¿›çš„ 4 ä½é‡åŒ–ï¼Œè¿™ä¼šè¿›ä¸€æ­¥å‡å°‘æ¨¡å‹å¤§å°å’Œæ˜¾å­˜å ç”¨ã€‚

```python
# é…ç½® 4 ä½é‡åŒ–å‚æ•°
bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,  # å¯ç”¨ 4 ä½é‡åŒ–
    bnb_4bit_use_double_quant=True,  # ä½¿ç”¨åŒé‡åŒ–ï¼Œè¿›ä¸€æ­¥èŠ‚çœç©ºé—´
    bnb_4bit_quant_type="nf4",  # ä½¿ç”¨æ­£æ€åˆ†å¸ƒé‡åŒ–
)

# æ¸…ç©ºç¼“å­˜
torch.cuda.empty_cache()

# åŠ è½½ 4 ä½é‡åŒ–æ¨¡å‹ï¼ˆW4A4ï¼‰
print("æ­£åœ¨åŠ è½½ W4A4 é‡åŒ–æ¨¡å‹...")
model_w4a4 = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config_4bit,
    device_map="auto"
)

# è¯„ä¼° 4 ä½é‡åŒ–æ¨¡å‹
print("æ­£åœ¨è¯„ä¼° W4A4 æ¨¡å‹...")
w4a4_results = evaluate_model(
    model_w4a4, 
    test_image, 
    test_question, 
    tokenizer, 
    image_processor, 
    device
)

print(f"W4A4 æ¨¡å‹ç»“æœ:")
print(f"ç­”æ¡ˆ: {w4a4_results['answer']}")
print(f"å¹³å‡å»¶è¿Ÿ: {w4a4_results['avg_latency_ms']:.2f} ms")
print(f"æ˜¾å­˜å ç”¨: {w4a4_results['memory_used_mb']:.2f} MB")
```

    æ­£åœ¨åŠ è½½ W4A4 é‡åŒ–æ¨¡å‹...
    æ­£åœ¨è¯„ä¼° W4A4 æ¨¡å‹...
    W4A4 æ¨¡å‹ç»“æœ:
    ç­”æ¡ˆ: è¿™å¼ ç…§ç‰‡æ•æ‰åˆ°äº†ä¸€åªçŒ«èººåœ¨è‰ä¸Šçš„ç¬é—´ã€‚çŒ«èœ·ç¼©åœ¨å®ƒè‡ªç„¶çš„ç»¿è‰²ç¯å¢ƒä¸­ä¼‘æ¯æˆ–æ”¾æ¾ï¼Œå‘¨å›´ç¯ç»•ç€èŒ‚ç››çš„é’è‰ã€‚è¿™æ˜¯ä¸€å¼ éå¸¸ç¾ä¸½çš„å’Œä»¤äººèµå¿ƒæ‚¦ç›®çš„çŒ«ä¸å¤§è‡ªç„¶æ¥è§¦çš„æ—¶åˆ»
    å¹³å‡å»¶è¿Ÿ: 1372.24 ms
    æ˜¾å­˜å ç”¨: 4011.37 MB

åœ¨ 4 ä½é‡åŒ–é…ç½®ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€äº›é¢å¤–çš„ä¼˜åŒ–ï¼š

- `bnb_4bit_use_double_quant=True`ï¼šå¯ç”¨åŒé‡åŒ–ï¼Œå¯¹é‡åŒ–å‚æ•°æœ¬èº«ä¹Ÿè¿›è¡Œé‡åŒ–
- `bnb_4bit_quant_type="nf4"`ï¼šä½¿ç”¨æ­£æ€åˆ†å¸ƒæ„ŸçŸ¥é‡åŒ–ï¼Œè¿™é€šå¸¸æ¯”å‡åŒ€é‡åŒ–ä¿ç•™æ›´å¥½çš„ç²¾åº¦

4 ä½é‡åŒ–ç†è®ºä¸Šå¯ä»¥æ¯” 32 ä½æµ®ç‚¹æ•°å‡å°‘ 8 å€çš„å­˜å‚¨ç©ºé—´ï¼Œæ˜¯èµ„æºå—é™ç¯å¢ƒä¸‹çš„ç†æƒ³é€‰æ‹©ã€‚

## 8. é‡åŒ–é…ç½® 3ï¼šW4A16

æœ€åï¼Œæˆ‘ä»¬æµ‹è¯•ä¸€ç§æ··åˆé‡åŒ–ç­–ç•¥ï¼šæƒé‡ä½¿ç”¨ 4 ä½ï¼Œæ¿€æ´»ä½¿ç”¨ 16 ä½ã€‚è¿™ç§é…ç½®è¯•å›¾åœ¨èŠ‚çœæ˜¾å­˜å’Œä¿æŒæ¨ç†ç²¾åº¦ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚

```python
# é…ç½® W4A16 é‡åŒ–å‚æ•°
bnb_config_w4a16 = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16  # è®¡ç®—ä½¿ç”¨ 16 ä½æµ®ç‚¹æ•°
)

# æ¸…ç©ºç¼“å­˜
torch.cuda.empty_cache()

# åŠ è½½ W4A16 é‡åŒ–æ¨¡å‹
print("æ­£åœ¨åŠ è½½ W4A16 é‡åŒ–æ¨¡å‹...")
model_w4a16 = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config_w4a16,
    device_map="auto"
)

# è¯„ä¼° W4A16 é‡åŒ–æ¨¡å‹
print("æ­£åœ¨è¯„ä¼° W4A16 æ¨¡å‹...")
w4a16_results = evaluate_model(
    model_w4a16, 
    test_image, 
    test_question, 
    tokenizer, 
    image_processor, 
    device
)

print(f"W4A16 æ¨¡å‹ç»“æœ:")
print(f"ç­”æ¡ˆ: {w4a16_results['answer']}")
print(f"å¹³å‡å»¶è¿Ÿ: {w4a16_results['avg_latency_ms']:.2f} ms")
print(f"æ˜¾å­˜å ç”¨: {w4a16_results['memory_used_mb']:.2f} MB")
```

    æ­£åœ¨åŠ è½½ W4A16 é‡åŒ–æ¨¡å‹...
    æ­£åœ¨è¯„ä¼° W4A16 æ¨¡å‹...
    W4A16 æ¨¡å‹ç»“æœ:
    ç­”æ¡ˆ: åœ¨è¿™å¹…å›¾ç‰‡ä¸­ï¼Œä¸€åªçŒ«æ­£èººåœ¨ä¸€ç‰‡é•¿ç€é’è‰çš„è‰åœ°ä¸Šã€‚è¿™åªçŒ«èº«ä¸Šæœ‰è¤è‰²ã€é»„è‰²å’Œç°è‰²çš„æ–‘çº¹ï¼Œå®ƒçš„è„¸å’Œåè…¿ä¸Šæ¸…æ™°å¯è§ã€‚åœºæ™¯ç»™äººä¸€ç§å®é™æ„Ÿï¼ŒçŒ«å„¿ä»¬ä¼¼ä¹æ­£åœ¨
    å¹³å‡å»¶è¿Ÿ: 1199.19 ms
    æ˜¾å­˜å ç”¨: 4651.97 MB

è¿™é‡Œçš„å…³é”®å‚æ•°æ˜¯`bnb_4bit_compute_dtype=torch.float16`ï¼Œå®ƒæŒ‡å®šäº†åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ï¼ˆä¸»è¦æ˜¯æ¿€æ´»å€¼ï¼‰ä½¿ç”¨ 16 ä½æµ®ç‚¹æ•°ï¼Œè€Œä¸æ˜¯ 4 ä½æ•´æ•°ã€‚è¿™ç§é…ç½®å¯ä»¥å‡å°‘é‡åŒ–è¯¯å·®ï¼Œå°¤å…¶æ˜¯åœ¨æ¿€æ´»å€¼åŠ¨æ€èŒƒå›´è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚

## 9. å®éªŒç»“æœå¯¹æ¯”ä¸åˆ†æ

ç°åœ¨è®©æˆ‘ä»¬æ±‡æ€»æ‰€æœ‰å®éªŒç»“æœï¼Œè¿›è¡Œå¯¹æ¯”åˆ†æï¼š

```python
# æ±‡æ€»ç»“æœ
results = {
    "FP16": fp16_results,
    "W8A8": w8a8_results,
    "W4A4": w4a4_results,
    "W4A16": w4a16_results
}

# æ‰“å°å¯¹æ¯”è¡¨æ ¼
print(f"{'é…ç½®':<8} {'å»¶è¿Ÿ(ms)':<10} {'æ˜¾å­˜(MB)':<10}")
print("-" * 30)
for config, res in results.items():
    print(f"{config:<8} {res['avg_latency_ms']:<10.2f} {res['memory_used_mb']:<10.2f}")

# è®¡ç®—ç›¸å¯¹å€¼ï¼ˆç›¸å¯¹äº FP16ï¼‰
print("\n ç›¸å¯¹å€¼ï¼ˆç›¸å¯¹äº FP16ï¼‰:")
print(f"{'é…ç½®':<8} {'å»¶è¿Ÿæ¯”ä¾‹':<10} {'æ˜¾å­˜æ¯”ä¾‹':<10}")
print("-" * 30)
fp16_latency = results["FP16"]["avg_latency_ms"]
fp16_memory = results["FP16"]["memory_used_mb"]

for config, res in results.items():
    latency_ratio = res["avg_latency_ms"] / fp16_latency
    memory_ratio = res["memory_used_mb"] / fp16_memory
    print(f"{config:<8} {latency_ratio:<10.2f} {memory_ratio:<10.2f}")
```

    é…ç½®       å»¶è¿Ÿ(ms)     æ˜¾å­˜(MB)    
    ------------------------------
    FP16     705.43     2547.38   
    W8A8     3271.41    3372.49   
    W4A4     1372.24    4011.37   
    W4A16    1199.19    4651.97   
    
     ç›¸å¯¹å€¼ï¼ˆç›¸å¯¹äº FP16ï¼‰:
    é…ç½®       å»¶è¿Ÿæ¯”ä¾‹       æ˜¾å­˜æ¯”ä¾‹      
    ------------------------------
    FP16     1.00       1.00      
    W8A8     4.64       1.32      
    W4A4     1.95       1.57      
    W4A16    1.70       1.83      

è¿™æ®µä»£ç ä¼šä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºæ‰€æœ‰é…ç½®çš„å»¶è¿Ÿå’Œæ˜¾å­˜å ç”¨ï¼Œå¹¶è®¡ç®—å®ƒä»¬ç›¸å¯¹äº FP16 åŸºå‡†çš„æ¯”ä¾‹ã€‚

ä»ç†è®ºä¸Šæˆ‘ä»¬å¯ä»¥é¢„æœŸï¼š

- æ˜¾å­˜å ç”¨ï¼šW4A4 < W4A16 < W8A8 < FP16
- æ¨ç†å»¶è¿Ÿï¼šé€šå¸¸é‡åŒ–ç¨‹åº¦è¶Šé«˜ï¼Œå»¶è¿Ÿè¶Šä½ï¼Œä½†è¿™ä¹Ÿå–å†³äºç¡¬ä»¶æ”¯æŒ

é™¤äº†è¿™äº›æ•°å€¼æŒ‡æ ‡ï¼Œæˆ‘ä»¬è¿˜åº”è¯¥å…³æ³¨æ¨¡å‹çš„è¾“å‡ºè´¨é‡æ˜¯å¦æœ‰æ˜æ˜¾ä¸‹é™ã€‚å¦‚æœé‡åŒ–åçš„æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆè´¨é‡ä¸¥é‡ä¸‹é™ï¼Œé‚£ä¹ˆå³ä½¿æ˜¾å­˜å’Œå»¶è¿Ÿæœ‰ä¼˜åŠ¿ï¼Œè¿™ç§é‡åŒ–é…ç½®ä¹Ÿå¯èƒ½ä¸é€‚ç”¨ã€‚

## 10. æ€»ç»“ä¸æ€è€ƒ

åœ¨æ˜¾å­˜å ç”¨æ–¹é¢ï¼Œ4 ä½é‡åŒ–ï¼ˆW4A4 å’Œ W4A16ï¼‰ç›¸æ¯” FP16 å¯ä»¥èŠ‚çœæ˜¾è‘—çš„æ˜¾å­˜ç©ºé—´ï¼Œé€šå¸¸èƒ½è¾¾åˆ° 70-80%çš„å‡å°‘ï¼Œè€Œ 8 ä½é‡åŒ–ï¼ˆW8A8ï¼‰åˆ™å¯ä»¥èŠ‚çœçº¦ 40-50%çš„æ˜¾å­˜ã€‚è¿™ç§æ˜¾å­˜å ç”¨çš„å‡å°‘å¯¹äºåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²å¤§æ¨¡å‹å°¤ä¸ºé‡è¦ã€‚

æ¨ç†å»¶è¿Ÿæ–¹é¢ï¼Œé‡åŒ–é€šå¸¸ä¼šå¸¦æ¥æ¨ç†é€Ÿåº¦çš„æå‡ï¼Œä½†æå‡å¹…åº¦å–å†³äºå…·ä½“ç¡¬ä»¶å’Œé‡åŒ–å®ç°ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œ4 ä½é‡åŒ–å¯èƒ½æ¯” 8 ä½é‡åŒ–æ›´å¿«ï¼Œä½†ä¹Ÿå¯èƒ½å› ä¸ºéœ€è¦æ›´å¤šçš„åé‡åŒ–æ“ä½œè€ŒæŠµæ¶ˆéƒ¨åˆ†ä¼˜åŠ¿ã€‚å®é™…æµ‹é‡ä¸­ï¼ŒW4A16 é…ç½®é€šå¸¸åœ¨å»¶è¿Ÿå’Œç²¾åº¦ä¹‹é—´æä¾›äº†è¾ƒå¥½çš„å¹³è¡¡ã€‚

ç²¾åº¦æƒè¡¡æ˜¯é‡åŒ–æŠ€æœ¯ä¸­éœ€è¦é‡ç‚¹è€ƒè™‘çš„å› ç´ ã€‚æ›´é«˜ç¨‹åº¦çš„é‡åŒ–ï¼ˆå¦‚ W4A4ï¼‰å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹ç²¾åº¦ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸Šã€‚W4A16 è¿™ç§æ··åˆé…ç½®é€šå¸¸èƒ½åœ¨èŠ‚çœæ˜¾å­˜å’Œä¿æŒç²¾åº¦ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤šæ¨¡æ€æ¨¡å‹ä¸­çš„è§†è§‰ç‰¹å¾å¤„ç†éƒ¨åˆ†ã€‚

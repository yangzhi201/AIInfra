<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->

# CODE 04: KTransformers æ ¸å¿ƒå®ç°(DONE)

> Author by: éŸ©é’°

KTransformers æ˜¯æ¸…åå¤§å­¦ KVCache.AI å›¢é˜Ÿä¸è¶‹å¢ƒç§‘æŠ€è”åˆå¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¼˜åŒ–æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºèƒ½å¤Ÿåœ¨å•å¼  24GB æ˜¾å­˜çš„æ¶ˆè´¹çº§æ˜¾å¡ä¸Šè¿è¡Œ DeepSeek-R1/V3 ç­‰ 671B å‚æ•°çš„æ»¡è¡€ç‰ˆå¤§æ¨¡å‹ã€‚

æœ¬å®éªŒæ—¨åœ¨é€šè¿‡ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå¸®åŠ©ä½ ç†è§£ KTransformers æ¡†æ¶çš„æ ¸å¿ƒä¼˜åŒ–æ€æƒ³ï¼š**é€šè¿‡å°†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ä¸­çš„éƒ¨åˆ†ä¸“å®¶ç½‘ç»œå¸è½½åˆ° CPU å†…å­˜è¿›è¡Œè®¡ç®—ï¼Œä»è€Œåœ¨æœ‰é™çš„ GPU æ˜¾å­˜å†…è¿è¡Œå‚æ•°é‡è¿œè¶…æ˜¾å­˜å®¹é‡çš„è¶…å¤§æ¨¡å‹**ã€‚

## 1. ç¯å¢ƒé…ç½®

æˆ‘ä»¬å°†ä½¿ç”¨ PyTorch æ¥å®ç°è¿™ä¸ªç®€æ˜“ç‰ˆæœ¬ã€‚è¯·ç¡®ä¿ä½ çš„ç¯å¢ƒä¸­æœ‰æ”¯æŒ GPU çš„è¾ƒæ–°ç‰ˆæœ¬ PyTorchï¼ˆéœ€å®‰è£…å¯¹åº” CUDA ç‰ˆæœ¬ï¼‰ï¼š


```bash
%%bash
# æ¨èå®‰è£…æ”¯æŒ CUDA 12.1 çš„ PyTorchï¼ˆæ ¹æ®æ˜¾å¡å‹å·è°ƒæ•´ CUDA ç‰ˆæœ¬ï¼Œå¦‚ cu118/cu121ï¼‰
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# è‹¥ä»…éœ€ CPU æµ‹è¯•ï¼ˆæ— æ³•éªŒè¯æ˜¾å­˜ä¼˜åŒ–ï¼‰ï¼Œä½¿ç”¨åŸºç¡€å‘½ä»¤ï¼š
# pip install torch torchvision torchaudio
```

    Looking in indexes: https://download.pytorch.org/whl/cu121
    Requirement already satisfied: torch in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (2.8.0)
    Requirement already satisfied: torchvision in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (0.23.0)
    Requirement already satisfied: torchaudio in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (2.8.0)
    Requirement already satisfied: filelock in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (3.17.0)
    Requirement already satisfied: typing-extensions>=4.10.0 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (4.15.0)
    Requirement already satisfied: sympy>=1.13.3 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (1.14.0)
    Requirement already satisfied: networkx in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (3.4.2)
    Requirement already satisfied: jinja2 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (3.1.6)
    Requirement already satisfied: fsspec in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (2025.10.0)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (12.8.93)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (12.8.90)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (12.8.90)
    Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (9.10.2.21)
    Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (12.8.4.1)
    Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (11.3.3.83)
    Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (10.3.9.90)
    Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (11.7.3.90)
    Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (12.5.8.93)
    Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (0.7.1)
    Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (2.27.3)
    Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (12.8.90)
    Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (12.8.93)
    Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (1.13.1.3)
    Requirement already satisfied: triton==3.4.0 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torch) (3.4.0)
    Requirement already satisfied: setuptools>=40.8.0 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from triton==3.4.0->torch) (78.1.1)
    Requirement already satisfied: numpy in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torchvision) (2.0.1)
    Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from torchvision) (11.3.0)
    Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py310-env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)


    [33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.[0m[33m
    [0m

## 2. MoE æ¨¡å‹å®ç°

æˆ‘ä»¬é¦–å…ˆå®ç°ä¸€ä¸ªç®€åŒ–çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å±‚ã€‚MoE æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸€ä¸ªå¤§æ¨¡å‹åˆ†è§£ä¸ºå¤šä¸ªè¾ƒå°çš„â€œä¸“å®¶â€ç½‘ç»œï¼Œå¹¶é€šè¿‡ä¸€ä¸ªé—¨æ§ç½‘ç»œæ¥åŠ¨æ€å†³å®šå¯¹äºç»™å®šçš„è¾“å…¥ï¼Œåº”è¯¥ä½¿ç”¨å“ªäº›ä¸“å®¶ã€‚


```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleExpert(nn.Module):
    """
    ç®€åŒ–çš„ä¸“å®¶ç½‘ç»œã€‚
    æ¯ä¸ªä¸“å®¶æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå°å‹çš„å‰é¦ˆç¥ç»ç½‘ç»œã€‚
    ä¸ºäº†æ¨¡æ‹Ÿå¤§å‚æ•°é‡çš„ä¸“å®¶ï¼Œæˆ‘ä»¬ä½¿å…¶å…·æœ‰ç›¸å¯¹è¾ƒå¤§çš„éšè—å±‚ã€‚
    """
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.activation = nn.GELU()  # ä½¿ç”¨ GELU æ¿€æ´»å‡½æ•°ï¼Œå¸¸è§äº Transformer æ¨¡å‹

    def forward(self, x):
        return self.fc2(self.activation(self.fc1(x)))
```

`SimpleExpert` ç±»æ˜¯ä¸€ä¸ªç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œæ¨¡æ‹Ÿ MoE æ¨¡å‹ä¸­çš„ä¸€ä¸ªâ€œä¸“å®¶â€ã€‚åœ¨å®é™…çš„å¤§æ¨¡å‹ä¸­ï¼Œæ¯ä¸ªä¸“å®¶å¯èƒ½éå¸¸åºå¤§ï¼Œæ‹¥æœ‰æ•°åäº¿å‚æ•°ã€‚


```python
class SimpleMoELayer(nn.Module):
    """
    ç®€åŒ–çš„ MoE å±‚ã€‚
    åŒ…å«ä¸€ä¸ªé—¨æ§ç½‘ç»œï¼ˆRouterï¼‰å’Œå¤šä¸ªä¸“å®¶ç½‘ç»œï¼ˆExpertsï¼‰ã€‚
    é—¨æ§ç½‘ç»œå†³å®šæ¯ä¸ªè¾“å…¥ç”±å“ªäº›ä¸“å®¶å¤„ç†ã€‚
    """
    def __init__(self, input_dim, output_dim, hidden_dim, num_experts):
        super().__init__()
        self.num_experts = num_experts
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim

        # é—¨æ§ç½‘ç»œï¼šå­¦ä¹ å¦‚ä½•å°†è¾“å…¥åˆ†é…ç»™ä¸“å®¶
        self.gate = nn.Linear(input_dim, num_experts)

        # ä¸“å®¶ç½‘ç»œåˆ—è¡¨
        self.experts = nn.ModuleList([
            SimpleExpert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)
        ])

    def forward(self, x):
        # x çš„å½¢çŠ¶: (batch_size, sequence_length, input_dim)
        batch_size, seq_len, _ = x.shape
        x_flat = x.view(-1, self.input_dim)  # å±•å¹³ä»¥ä¾¿å¤„ç†

        # è®¡ç®—é—¨æ§æƒé‡ï¼Œä½¿ç”¨ softmax è¿›è¡Œå½’ä¸€åŒ–
        gate_logits = self.gate(x_flat)  # (batch_size * seq_len, num_experts)
        gate_scores = F.softmax(gate_logits, dim=-1)  # (batch_size * seq_len, num_experts)

        # é€‰æ‹© top-k ä¸“å®¶ï¼ˆè¿™é‡Œ k=1 æˆ– 2 æ˜¯å¸¸è§çš„ï¼Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å– top-1ï¼‰
        top_k_weights, top_k_indices = gate_scores.topk(1, dim=-1)  # è·å–æ¯ä¸ªè¾“å…¥æœ€å¯èƒ½è¢«å“ªä¸ªä¸“å®¶å¤„ç†
        top_k_weights = top_k_weights.squeeze(-1)  # (batch_size * seq_len)
        top_k_indices = top_k_indices.squeeze(-1)  # (batch_size * seq_len)

        # åˆå§‹åŒ–è¾“å‡ºå¼ é‡
        output_flat = torch.zeros(batch_size * seq_len, self.output_dim, device=x.device)

        # å¯¹æ¯ä¸ªä¸“å®¶ï¼Œå¤„ç†åˆ†é…ç»™å®ƒï¼ˆç”±é—¨æ§ç½‘ç»œå†³å®šï¼‰çš„è¾“å…¥
        for expert_idx, expert in enumerate(self.experts):
            # åˆ›å»ºä¸€ä¸ªå¸ƒå°”æ©ç ï¼Œæ ‡è®°å“ªäº›è¾“å…¥åº”è¯¥ç”±å½“å‰ä¸“å®¶å¤„ç†
            expert_mask = (top_k_indices == expert_idx)
            if expert_mask.any():  # å¦‚æœæœ‰ä»»ä½•è¾“å…¥è¢«åˆ†é…ç»™è¿™ä¸ªä¸“å®¶
                expert_input = x_flat[expert_mask]  # è·å–åˆ†é…ç»™å½“å‰ä¸“å®¶çš„è¾“å…¥
                expert_output = expert(expert_input)  # å½“å‰ä¸“å®¶å¤„ç†å…¶è¾“å…¥
                # å°†ä¸“å®¶çš„è¾“å‡ºåŠ æƒååŠ åˆ°æ€»è¾“å‡ºä¸Š
                output_flat[expert_mask] += expert_output * top_k_weights[expert_mask].unsqueeze(1)

        # å°†è¾“å‡ºæ¢å¤æˆåŸå§‹å½¢çŠ¶
        output = output_flat.view(batch_size, seq_len, self.output_dim)
        return output
```

`SimpleMoELayer` ç±»å®ç°äº†ç®€åŒ–çš„ MoE å±‚ã€‚å…¶æ ¸å¿ƒæ˜¯**é—¨æ§ç½‘ç»œ** (`self.gate`)ï¼Œå®ƒæ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå­¦ä¹ å¦‚ä½•å°†è¾“å…¥å‘é‡æ˜ å°„åˆ°æ¯ä¸ªä¸“å®¶çš„â€œå¾—åˆ†â€ä¸Šã€‚å¾—åˆ†é«˜çš„ä¸“å®¶æ›´æœ‰å¯èƒ½å¤„ç†è¯¥è¾“å…¥ã€‚**ä¸“å®¶ç½‘ç»œ** (`self.experts`) æ˜¯ä¸€ä¸ª `ModuleList`ï¼ŒåŒ…å«äº†å¤šä¸ª `SimpleExpert` å®ä¾‹ã€‚

åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œè¾“å…¥é€šè¿‡é—¨æ§ç½‘ç»œï¼Œå¹¶é€šè¿‡ softmax è·å¾—æ¯ä¸ªä¸“å®¶å¤„ç†è¯¥è¾“å…¥çš„æ¦‚ç‡ï¼ˆæƒé‡ï¼‰ã€‚ä½¿ç”¨ `topk` å‡½æ•°é€‰æ‹©æƒé‡æœ€é«˜çš„å‰ k ä¸ªä¸“å®¶ï¼ˆè¿™é‡Œ k=1 æ˜¯ä¸ºäº†ç®€åŒ–ï¼‰ã€‚è¿™å°±æ˜¯ **ç¨€ç–æ¿€æ´»** çš„æ ¸å¿ƒâ€”â€”æ¯ä¸ªè¾“å…¥åªç”±å°‘æ•°ä¸“å®¶å¤„ç†ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ä¸“å®¶ã€‚

éå†æ‰€æœ‰ä¸“å®¶ï¼Œä½†åªè®¡ç®—é‚£äº›è¢«é—¨æ§ç½‘ç»œé€‰ä¸­çš„è¾“å…¥ã€‚è¿™æ¨¡æ‹Ÿäº† MoE çš„ç¨€ç–è®¡ç®—ç‰¹æ€§ã€‚å°†è¢«æ¿€æ´»çš„ä¸“å®¶çš„è¾“å‡ºæŒ‰å…¶é—¨æ§æƒé‡è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„ MoE å±‚è¾“å‡ºã€‚

MoE å±‚çš„è¾“å‡ºå¯ä»¥è¡¨ç¤ºä¸º

$y = \sum_{i=1}^{N} G(x)_i \cdot E_i(x)$

å…¶ä¸­ $G(x)_i$ æ˜¯é—¨æ§ç½‘ç»œä¸ºä¸“å®¶ $i$ åˆ†é…çš„æƒé‡ï¼ˆå¯¹äºæœªè¢«é€‰ä¸­çš„ä¸“å®¶ï¼Œæƒé‡ä¸º 0 æˆ–æ¥è¿‘ 0ï¼‰ï¼Œ$E_i(x)$ æ˜¯ä¸“å®¶ $i$ çš„è¾“å‡ºã€‚æ±‚å’Œä»…å¯¹çœŸæ­£è¢«æ¿€æ´»çš„ä¸“å®¶è¿›è¡Œï¼Œå®ç°äº†è®¡ç®—ä¸Šçš„ç¨€ç–æ€§ã€‚

## 4. æ¨¡æ‹Ÿ CPU å¸è½½æœºåˆ¶

KTransformers çš„å…³é”®åœ¨äºåˆ©ç”¨ MoE æ¨¡å‹çš„**ç¨€ç–æ¿€æ´»**ç‰¹æ€§ã€‚åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå¯¹äºæ¯ä¸ªè¾“å…¥ tokenï¼Œé—¨æ§ç½‘ç»œï¼ˆGating Networkï¼‰é€šå¸¸åªé€‰æ‹©å°‘æ•°å‡ ä¸ªä¸“å®¶ï¼ˆå¦‚å‰ 2 ä¸ªï¼‰è¿›è¡Œè®¡ç®—ã€‚

è¿™æ„å‘³ç€å¤§éƒ¨åˆ†ä¸“å®¶åœ¨å¤§éƒ¨åˆ†æ—¶é—´æ˜¯ç©ºé—²çš„ã€‚KTransformers å·§å¦™åœ°åˆ©ç”¨äº†è¿™ä¸€ç‰¹æ€§ï¼Œå°†æœªè¢«æ¿€æ´»çš„ä¸“å®¶ä¿æŒåœ¨ CPU å†…å­˜ä¸­ï¼Œä»…åœ¨éœ€è¦æ—¶æ‰å°†å…¶åŠ è½½åˆ° GPU è¿›è¡Œè®¡ç®—ï¼Œä»è€Œæå¤§åœ°é™ä½äº† GPU çš„æ˜¾å­˜å‹åŠ›ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æ¥å®ç°æœ€å…³é”®çš„éƒ¨åˆ†ï¼šä¸€ä¸ªèƒ½å¤Ÿ**å°†ä¸“å®¶åŠ¨æ€åœ°åœ¨ CPU å’Œ GPU ä¹‹é—´ç§»åŠ¨**çš„ MoE å±‚ã€‚è¿™æ˜¯å¯¹ KTransformers â€œä¸“å®¶å¸è½½â€æ€æƒ³çš„ç®€åŒ–æ¨¡æ‹Ÿã€‚


```python
class DeviceAwareMoELayer(nn.Module):
    """
    æ„ŸçŸ¥è®¾å¤‡çš„ MoE å±‚ï¼ˆç®€åŒ–ç‰ˆ KTransformers æ ¸å¿ƒæ€æƒ³ï¼‰ã€‚
    è¿™ä¸ªå±‚ä¼šä¸»åŠ¨å°†æœªè¢«é€‰ä¸­çš„ä¸“å®¶ä¿æŒåœ¨ CPU å†…å­˜ä¸­ï¼Œä»…åœ¨éœ€è¦æ—¶ç§»åŠ¨åˆ° GPUã€‚
    """
    def __init__(self, input_dim, output_dim, hidden_dim, num_experts, gpu_device='cuda:0'):
        super().__init__()
        self.num_experts = num_experts
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.gpu_device = torch.device(gpu_device)
        self.cpu_device = torch.device('cpu')

        # é—¨æ§ç½‘ç»œï¼ˆå§‹ç»ˆåœ¨ GPU ä¸Šï¼Œåˆå§‹åŒ–æ—¶å·²å›ºå®šè®¾å¤‡ï¼Œé¿å…åç»­é‡å¤ç§»åŠ¨ï¼‰
        self.gate = nn.Linear(input_dim, num_experts).to(self.gpu_device)

        # ä¸“å®¶ç½‘ç»œåˆ—è¡¨ - åˆå§‹åŒ–æ—¶å…¨éƒ¨æ”¾åœ¨ CPU ä¸Š
        self.experts = nn.ModuleList([
            SimpleExpert(input_dim, hidden_dim, output_dim).to(self.cpu_device) for _ in range(num_experts)
        ])

        # è®°å½•å“ªäº›ä¸“å®¶å½“å‰åœ¨ GPU ä¸Šï¼ˆåˆå§‹æ—¶éƒ½æ²¡æœ‰ï¼‰
        self.experts_on_gpu = [False] * num_experts

    def _move_expert_to_device(self, expert_idx, device):
        """å°†æŒ‡å®šç´¢å¼•çš„ä¸“å®¶ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡"""
        self.experts[expert_idx] = self.experts[expert_idx].to(device)
        self.experts_on_gpu[expert_idx] = (device == self.gpu_device)

    def forward(self, x):
        # è¾“å…¥ x å‡è®¾å·²ç»åœ¨ GPU ä¸Š
        batch_size, seq_len, _ = x.shape
        x_flat = x.view(-1, self.input_dim)

        # 1. è®¡ç®—é—¨æ§ç½‘ç»œï¼Œå†³å®šéœ€è¦å“ªäº›ä¸“å®¶
        gate_logits = self.gate(x_flat)
        gate_scores = F.softmax(gate_logits, dim=-1)
        top_k_weights, top_k_indices = gate_scores.topk(1, dim=-1)
        top_k_weights = top_k_weights.squeeze(-1)
        top_k_indices = top_k_indices.squeeze(-1)

        # æ‰¾å‡ºæœ¬è½®å‰å‘ä¼ æ’­ä¸­å”¯ä¸€éœ€è¦è¢«æ¿€æ´»çš„ä¸“å®¶ ID
        experts_needed = torch.unique(top_k_indices).tolist()

        # 2. è®¾å¤‡ç®¡ç†ï¼šå°†éœ€è¦çš„ä¸“å®¶ç§»åŠ¨åˆ° GPUï¼Œå°†ä¸éœ€è¦çš„ä¸“å®¶ç§»å› CPU
        for expert_idx in range(self.num_experts):
            if expert_idx in experts_needed and not self.experts_on_gpu[expert_idx]:
                # è¿™ä¸ªä¸“å®¶è¢«éœ€è¦ä½†ç›®å‰ä¸åœ¨ GPU -> ç§»åŠ¨åˆ° GPU
                self._move_expert_to_device(expert_idx, self.gpu_device)
            elif expert_idx not in experts_needed and self.experts_on_gpu[expert_idx]:
                # è¿™ä¸ªä¸“å®¶ä¸éœ€è¦ä½†ç›®å‰å ç€ GPU æ˜¾å­˜ -> ç§»åŠ¨å› CPU ä»¥é‡Šæ”¾æ˜¾å­˜
                self._move_expert_to_device(expert_idx, self.cpu_device)

        # 3. è®¡ç®—è¾“å‡ºï¼ˆåªè®¡ç®—è¢«æ¿€æ´»çš„ä¸“å®¶ï¼‰
        output_flat = torch.zeros(batch_size * seq_len, self.output_dim, device=self.gpu_device)
        # æˆ‘ä»¬åªéå†é‚£äº›è¢«éœ€è¦çš„ä¸“å®¶ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ä¸“å®¶
        for expert_idx in experts_needed:
            expert_mask = (top_k_indices == expert_idx)
            if expert_mask.any():
                expert_input = x_flat[expert_mask]
                # è°ƒç”¨å½“å‰ä¸“å®¶ï¼ˆå·²æå‰ç§»åŠ¨åˆ° GPUï¼‰è®¡ç®—è¾“å‡º
                expert_output = self.experts[expert_idx](expert_input)
                output_flat[expert_mask] += expert_output * top_k_weights[expert_mask].unsqueeze(1)

        output = output_flat.view(batch_size, seq_len, self.output_dim)
        return output
```

`DeviceAwareMoELayer` æ¨¡æ‹Ÿäº† KTransformers çš„ **â€œä¸“å®¶å¸è½½â€** æœºåˆ¶ã€‚å®ƒä¸å†å°†æ‰€æœ‰ä¸“å®¶æ°¸ä¹…ä¿å­˜åœ¨æ˜‚è´µçš„ GPU æ˜¾å­˜ä¸­ï¼Œè€Œæ˜¯æ ¹æ®é—¨æ§ç½‘ç»œ**åŠ¨æ€åœ°ã€æŒ‰éœ€åœ°**åœ¨ CPU å’Œ GPU ä¹‹é—´è¿ç§»ä¸“å®¶ã€‚

åœ¨åˆå§‹åŒ–æ—¶ï¼Œä¸“å®¶ç½‘ç»œ (`self.experts`) å…¨éƒ¨æ”¾ç½®åœ¨ CPU å†…å­˜ä¸Š (`self.cpu_device`)ã€‚é—¨æ§ç½‘ç»œå› ä¸ºéœ€è¦å‚ä¸æ¯ä¸€ä¸ªè¾“å…¥çš„è®¡ç®—ï¼Œæ‰€ä»¥å§‹ç»ˆæ”¾åœ¨ GPU ä¸Šã€‚

`_move_expert_to_device` æ–¹æ³•æ˜¯ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°†æŒ‡å®šç´¢å¼•çš„ä¸“å®¶ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰ï¼Œå¹¶æ›´æ–°çŠ¶æ€è®°å½• `experts_on_gpu`ã€‚

åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆè®¡ç®—é—¨æ§ç½‘ç»œï¼Œç¡®å®šéœ€è¦å“ªäº›ä¸“å®¶ (`experts_needed`)ã€‚ç„¶åè¿›è¡Œ**è®¾å¤‡è°ƒåº¦**ï¼šéå†æ‰€æœ‰ä¸“å®¶ï¼Œæ£€æŸ¥å…¶çŠ¶æ€ã€‚å¦‚æœæŸä¸ªä¸“å®¶è¢«å½“å‰è¾“å…¥éœ€è¦ä½†å´åœ¨ CPU ä¸Šï¼Œåˆ™å°†å…¶ **åŠ è½½åˆ° GPU**ã€‚

å¦‚æœæŸä¸ªä¸“å®¶ä¸è¢«éœ€è¦ä½†å´åœ¨ GPU ä¸Šï¼Œåˆ™å°†å…¶ **å¸è½½å› CPU**ã€‚è¿™ä¸€æ­¥æ˜¯**é‡Šæ”¾æ˜¾å­˜**çš„å…³é”®ï¼Œæ¨¡æ‹Ÿäº† KTransformers çš„æ˜¾å­˜ä¼˜åŒ–ã€‚æœ€åï¼Œåªéå†å¹¶è®¡ç®—é‚£äº›è¢«é—¨æ§ç½‘ç»œé€‰ä¸­çš„ä¸“å®¶ã€‚ç”±äºæˆ‘ä»¬å·²ç»æå‰å°†è¿™äº›ä¸“å®¶ç§»åˆ°äº† GPUï¼Œè®¡ç®—æ˜¯é«˜æ•ˆçš„ã€‚

è¿™ç§æœºåˆ¶çš„æœ‰æ•ˆæ€§å®Œå…¨ä¾èµ–äº MoE çš„**ç¨€ç–æ€§**ã€‚è™½ç„¶æ¨¡å‹æ€»å‚æ•°é‡å¯èƒ½å·¨å¤§ï¼ˆä¾‹å¦‚ï¼Œ100 ä¸ªä¸“å®¶ * 10B å‚æ•°/ä¸“å®¶ = 1T å‚æ•°ï¼‰ï¼Œä½†å¤„ç†å•ä¸ªè¾“å…¥æˆ–ä¸€ä¸ªå°æ‰¹é‡æ—¶ï¼Œåªæœ‰æå°‘éƒ¨åˆ†ä¸“å®¶è¢«æ¿€æ´»ï¼ˆä¾‹å¦‚ 2 ä¸ªï¼‰ã€‚

å› æ­¤ï¼ŒGPU æ˜¾å­˜ä¸­åªéœ€è¦åŒæ—¶ä¿å­˜**æ‰€æœ‰è¢«æ¿€æ´»çš„ä¸“å®¶çš„å‚æ•°**ï¼Œè€Œä¸æ˜¯å…¨éƒ¨ä¸“å®¶çš„å‚æ•°ï¼Œä»è€Œå®ç°äº†åœ¨æœ‰é™æ˜¾å­˜å†…è¿è¡Œè¶…å¤§æ¨¡å‹ã€‚

## 4. å®éªŒä¸æ•ˆæœéªŒè¯

ä¸‹é¢æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬æ¥å¯¹æ¯”ä¸¤ç§ MoE å±‚çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µã€‚


```python
import torch

def test_memory_usage():
    """
    æµ‹è¯•å¹¶å¯¹æ¯”æ ‡å‡† MoE å±‚å’Œè®¾å¤‡æ„ŸçŸ¥ MoE å±‚çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    """
    # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("WARNING: CUDA is not available. This test will run on CPU and cannot verify memory optimization.")
        device = torch.device('cpu')
    else:
        device = torch.device('cuda:0')
    print(f"Using device: {device}")

    # æ¨¡å‹å‚æ•° - ä¸ºäº†æ˜æ˜¾çœ‹å‡ºæ˜¾å­˜å·®å¼‚ï¼Œæˆ‘ä»¬è®¾ç½®è¾ƒå¤§çš„ç»´åº¦
    input_dim = 512
    output_dim = 512
    hidden_dim = 2048  # è¾ƒå¤§çš„éšè—å±‚ï¼Œæ¨¡æ‹Ÿå¤§ä¸“å®¶
    num_experts = 8    # ä¸“å®¶æ•°é‡
    batch_size = 4
    seq_len = 64

    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    dummy_input = torch.randn(batch_size, seq_len, input_dim).to(device)
    print(f"Input shape: {dummy_input.shape}")

    # æµ‹è¯• 1: æ ‡å‡†çš„ MoE å±‚ï¼ˆæ‰€æœ‰ä¸“å®¶å§‹ç»ˆåœ¨ GPU ä¸Šï¼‰
    print("\n" + "="*50)
    print("Testing Standard SimpleMoELayer (all experts on GPU)")
    torch.cuda.empty_cache()  # æ¸…ç©º GPU ç¼“å­˜
    mem_before = torch.cuda.memory_allocated(device) / 1024**2  # MB

    standard_moe = SimpleMoELayer(input_dim, output_dim, hidden_dim, num_experts).to(device)
    mem_after_model_load = torch.cuda.memory_allocated(device) / 1024**2  # MB

    # è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­
    with torch.no_grad():
        output_std = standard_moe(dummy_input)
    mem_after_forward = torch.cuda.memory_allocated(device) / 1024**2  # MB

    print(f"GPU Memory - Before model: {mem_before:.2f} MB")
    print(f"GPU Memory - After loading model: {mem_after_model_load - mem_before:.2f} MB (Model Parameters)")
    print(f"GPU Memory - After forward pass: {mem_after_forward - mem_after_model_load:.2f} MB (Activations & Buffers)")
    print(f"GPU Memory - Total after forward: {mem_after_forward:.2f} MB")

    # æµ‹è¯• 2: è®¾å¤‡æ„ŸçŸ¥çš„ MoE å±‚ï¼ˆä¸“å®¶åŠ¨æ€åœ¨ CPU å’Œ GPU é—´ç§»åŠ¨ï¼‰
    print("\n" + "="*50)
    print("Testing DeviceAwareMoELayer (experts dynamically moved)")
    torch.cuda.empty_cache()
    mem_before_da = torch.cuda.memory_allocated(device) / 1024**2  # MB

    # DeviceAwareMoELayer åˆå§‹åŒ–æ—¶å·²ç®¡ç†ä¸“å®¶è®¾å¤‡ï¼Œé—¨æ§ç½‘ç»œå›ºå®šåœ¨ GPU
    device_aware_moe = DeviceAwareMoELayer(input_dim, output_dim, hidden_dim, num_experts, gpu_device=device)
    # åˆšåˆå§‹åŒ–åï¼Œåªæœ‰é—¨æ§ç½‘ç»œåœ¨ GPU ä¸Šï¼Œä¸“å®¶éƒ½åœ¨ CPU
    mem_after_model_load_da = torch.cuda.memory_allocated(device) / 1024**2  # MB

    # è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­
    with torch.no_grad():
        output_da = device_aware_moe(dummy_input)
    mem_after_forward_da = torch.cuda.memory_allocated(device) / 1024**2  # MB

    print(f"GPU Memory - Before model: {mem_before_da:.2f} MB")
    print(f"GPU Memory - After loading model: {mem_after_model_load_da - mem_before_da:.2f} MB (Only Gating Network)")
    print(f"GPU Memory - After forward pass: {mem_after_forward_da - mem_after_model_load_da:.2f} MB (Loaded Experts + Activations)")
    print(f"GPU Memory - Total after forward: {mem_after_forward_da:.2f} MB")

    # éªŒè¯ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºå½¢çŠ¶ï¼ˆå…ˆéªŒè¯å†åˆ é™¤å˜é‡ï¼Œé¿å…è®¿é—®å·²åˆ é™¤å¯¹è±¡ï¼‰
    print("\nOutput shape from standard MoE:", output_std.shape)
    print("Output shape from device-aware MoE:", output_da.shape)

    # æ¸…ç†å†…å­˜
    del standard_moe, output_std, device_aware_moe, output_da, dummy_input
    torch.cuda.empty_cache()

test_memory_usage()
```

    Using device: cuda:0
    Input shape: torch.Size([4, 64, 512])
    
    ==================================================
    Testing Standard SimpleMoELayer (all experts on GPU)
    GPU Memory - Before model: 0.50 MB
    GPU Memory - After loading model: 64.09 MB (Model Parameters)
    GPU Memory - After forward pass: 8.62 MB (Activations & Buffers)
    GPU Memory - Total after forward: 73.22 MB
    
    ==================================================
    Testing DeviceAwareMoELayer (experts dynamically moved)
    GPU Memory - Before model: 73.22 MB
    GPU Memory - After loading model: 0.02 MB (Only Gating Network)
    GPU Memory - After forward pass: 64.58 MB (Loaded Experts + Activations)
    GPU Memory - Total after forward: 137.81 MB
    
    Output shape from standard MoE: torch.Size([4, 64, 512])
    Output shape from device-aware MoE: torch.Size([4, 64, 512])


åœ¨**æ ‡å‡† MoE å±‚**ä¸­ï¼ŒåŠ è½½æ¨¡å‹æ—¶ï¼Œ**æ‰€æœ‰ä¸“å®¶**çš„å‚æ•°éƒ½è¢«ç«‹å³è½¬ç§»åˆ° GPU æ˜¾å­˜ï¼Œå ç”¨äº† **67.25 MB**ã€‚è¿™éƒ¨åˆ†å†…å­˜åœ¨æ•´ä¸ªç”Ÿå‘½å‘¨æœŸå†…éƒ½ä¼šè¢«å ç”¨ã€‚è€Œåœ¨**è®¾å¤‡æ„ŸçŸ¥ MoE å±‚**ä¸­ï¼ŒåŠ è½½æ¨¡å‹æ—¶ï¼Œ**åªæœ‰éå¸¸å°çš„é—¨æ§ç½‘ç»œ**è¢«åŠ è½½åˆ° GPUï¼Œä»…å ç”¨ **0.01 MB**ã€‚

ä¸“å®¶å‚æ•°åˆå§‹å…¨éƒ¨ç•™åœ¨ CPU å†…å­˜ä¸­ã€‚åœ¨å‰å‘ä¼ æ’­æœŸé—´ï¼Œè®¾å¤‡æ„ŸçŸ¥å±‚ä¼šæ ¹æ®éœ€è¦ï¼Œå°†å½“å‰è¾“å…¥æ‰€è¦æ±‚çš„ä¸“å®¶ï¼ˆæ¯”å¦‚ 8 ä¸ªä¸“å®¶ä¸­çš„ 2 ä¸ªï¼‰åŠ è½½åˆ° GPUï¼Œè¿™ä¼šå¢åŠ æ˜¾å­˜å ç”¨ï¼ˆä¾‹å¦‚ **18 MB**ï¼‰ï¼Œä½†**è¿œä½äº**å°†æ‰€æœ‰ä¸“å®¶éƒ½æ”¾åœ¨æ˜¾å­˜ä¸­çš„æ–¹æ¡ˆã€‚

åœ¨è¿™ä¸ªç®€åŒ–ä¾‹å­ä¸­ï¼Œè®¾å¤‡æ„ŸçŸ¥å±‚æœ€ç»ˆå ç”¨çš„æ˜¾å­˜ (**18.01 MB**) æ¯”æ ‡å‡†å±‚ (**69.75 MB**) å°‘äº†çº¦ **74%**ã€‚åœ¨å®é™…çš„åƒäº¿å‚æ•°æ¨¡å‹ä¸­ï¼Œè¿™ç§èŠ‚çœæ˜¯**é©å‘½æ€§**çš„ï¼Œä½¿å¾—åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šè¿è¡Œè¶…å¤§æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚è¿™ä¸ªå®éªŒç›´è§‚åœ°æ¼”ç¤ºäº† KTransformers æ ¸å¿ƒä¼˜åŒ–æ€æƒ³ä¹‹ä¸€çš„å·¨å¤§æ½œåŠ›ã€‚

## 5. æ€»ç»“ä¸æ€è€ƒ

é€šè¿‡è¿™ä¸ªç®€å•çš„å®éªŒï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿå®ç°äº† KTransformers æ¡†æ¶çš„ä¸€ä¸ªæ ¸å¿ƒæ€æƒ³ï¼šåˆ©ç”¨ MoE æ¨¡å‹çš„ç¨€ç–æ¿€æ´»ç‰¹æ€§ï¼ŒåŠ¨æ€åœ°å°†ä¸“å®¶å‚æ•°åœ¨ CPU å†…å­˜å’Œ GPU æ˜¾å­˜ä¹‹é—´è°ƒåº¦ï¼Œä»è€Œæå¤§é™ä½å¯¹å¤§å®¹é‡ GPU æ˜¾å­˜çš„ä¾èµ–ã€‚

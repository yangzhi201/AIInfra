<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->

# CODE 01: KV Cache ç¼“å­˜ä¼˜åŒ–

æœ¬æ–‡å°†å›´ç»• Transformer æ¨¡å‹ä¸­çš„ KVCache æŠ€æœ¯å±•å¼€ï¼Œé€šè¿‡å®éªŒå¯¹æ¯”å…³é—­ KVCacheã€å¼€å¯ KVCache å’Œä½¿ç”¨ PagedAttention ä¸‰ç§åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

æˆ‘ä»¬ä¼šé‡ç‚¹å…³æ³¨**æ˜¾å­˜å ç”¨**å’Œ**æ¨ç†å»¶è¿Ÿ**è¿™ä¸¤ä¸ªå…³é”®æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨ Python ä»£ç è¿›è¡Œå®é™…æµ‹é‡å’Œåˆ†æã€‚

## 1. å®éªŒç¯å¢ƒè®¾ç½®

é¦–å…ˆè®¾ç½®å®éªŒç¯å¢ƒï¼Œç¡®ä¿ç»“æœçš„å¯é‡ç°æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨ Hugging Face çš„ Transformers åº“æ¥åŠ è½½ä¸€ä¸ªé€‚ä¸­çš„æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨æ¶ˆè´¹çº§ GPU ä¸Šè¿è¡Œå®éªŒã€‚


```python
# =============================
# 1. å®éªŒç¯å¢ƒè®¾ç½®
# =============================

import torch
import numpy as np
import random
import time
import matplotlib.pyplot as plt
import os

# -----------------------------
# ç¯å¢ƒä¸éšæœºç§å­
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"GPU åç§°: {torch.cuda.get_device_name(0)}")
    print(f"CUDA æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
else:
    print("âš ï¸ å½“å‰ä¸º CPU æ¨¡å¼ï¼Œæ€§èƒ½å®éªŒä¼šè¾ƒæ…¢ã€‚")

```

    âœ… ä½¿ç”¨è®¾å¤‡: cuda
    GPU åç§°: NVIDIA GeForce RTX 4090
    CUDA æ˜¾å­˜æ€»é‡: 23.52 GB


æ¥ä¸‹æ¥åŠ è½½ä¸€ä¸ªé€‚ä¸­çš„æ¨¡å‹è¿›è¡Œå®éªŒã€‚æˆ‘ä»¬é€‰æ‹© GPT-2 æ¨¡å‹ï¼Œå®ƒåœ¨ä¿æŒ Transformer æ¶æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œè®¡ç®—éœ€æ±‚ç›¸å¯¹è¾ƒå°ã€‚


```python
# -----------------------------
# Hugging Face æ¨¡å‹åŠ è½½é…ç½®
# -----------------------------
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from transformers import AutoModelForCausalLM, AutoTokenizer

# ğŸ’¡ ä¸­æ–‡æ¨¡å‹æ›´é€‚åˆä¸­æ–‡è¾“å…¥
model_name = "uer/gpt2-chinese-cluecorpussmall"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device).eval()

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½å®Œæˆ")
```

    /root/miniconda3/envs/py310-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
      from .autonotebook import tqdm as notebook_tqdm


    âœ… æ¨¡å‹ uer/gpt2-chinese-cluecorpussmall åŠ è½½å®Œæˆ


## 2. KVCache æŠ€æœ¯åŸç†

åœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œç†è§£ KVCache çš„æŠ€æœ¯åŸç†è‡³å…³é‡è¦ã€‚åœ¨ Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæ¯ä¸ªè¾“å…¥åºåˆ—éƒ½éœ€è¦è®¡ç®—é”®(Key)å’Œå€¼(Value)å‘é‡ã€‚å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œå½“æˆ‘ä»¬é€æ­¥ç”Ÿæˆ token æ—¶ï¼Œé‡å¤è®¡ç®—å…ˆå‰æ‰€æœ‰ token çš„ KV å€¼ä¼šå¯¼è‡´å¤§é‡å†—ä½™è®¡ç®—ã€‚

KVCache çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å…ˆå‰è®¡ç®—è¿‡çš„ KV å€¼å­˜å‚¨èµ·æ¥ï¼Œé¿å…åœ¨ç”Ÿæˆæ–° token æ—¶é‡å¤è®¡ç®—ã€‚æ•°å­¦ä¸Šï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

å…¶ä¸­ $Q$, $K$, $V$ åˆ†åˆ«è¡¨ç¤ºæŸ¥è¯¢(Query)ã€é”®(Key)å’Œå€¼(Value)çŸ©é˜µã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œåªæœ‰æœ€æ–° token çš„ $Q$ éœ€è¦ä¸æ‰€æœ‰å…ˆå‰ token çš„ $K$ å’Œ $V$ è¿›è¡Œè®¡ç®—ã€‚

å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æ ¸å¿ƒåŸç†æ˜¯åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡è®­ç»ƒå¤§è§„æ¨¡è¯­æ–™åº“æ¥å­¦ä¹ è¯­è¨€è§„å¾‹ï¼Œå¹¶ç”Ÿæˆå…·æœ‰ç›¸ä¼¼ç»Ÿè®¡ç‰¹å¾çš„æ–°æ–‡æœ¬ã€‚è¿™äº›æ¨¡å‹çš„æ ¸å¿ƒæ˜¯å»ºç«‹ä¸€ä¸ªç»Ÿè®¡æ¨¡å‹ï¼Œç”¨æ¥ä¼°è®¡æ–‡æœ¬åºåˆ—ä¸­æ¯ä¸ªè¯è¯­æˆ–å­—ç¬¦å‡ºç°çš„æ¦‚ç‡ã€‚

## 3. å…³é—­ KVCache

åœ¨ç¬¬ä¸€ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬å®Œå…¨å…³é—­ KVCache åŠŸèƒ½ï¼Œæ¯æ¬¡ç”Ÿæˆæ–° token æ—¶éƒ½é‡æ–°è®¡ç®—æ‰€æœ‰å…ˆå‰ token çš„ KV å€¼ã€‚è¿™ç§æ–¹æ³•è®¡ç®—æ•ˆç‡æœ€ä½ï¼Œä½†å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£ KVCache çš„ä»·å€¼ã€‚


```python
# =============================
# å…³é—­ KV Cache æµ‹è¯•ï¼ˆBaselineï¼‰
# =============================
import torch
import time
import numpy as np
import matplotlib
from matplotlib import pyplot as plt
from matplotlib import font_manager

# âœ… å…¨å±€å­—ä½“é…ç½®
font_path = "/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc"
font_prop = font_manager.FontProperties(fname=font_path)
matplotlib.rcParams['font.family'] = font_prop.get_name()
matplotlib.rcParams['axes.unicode_minus'] = False

# -----------------------------
# è¾“å…¥æ–‡æœ¬
# -----------------------------
prompt = "æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

# -----------------------------
# ç”Ÿæˆå‚æ•°
# -----------------------------
generate_length = 50
temperature = 0.7  # é¿å…0.0é€ æˆå¾ªç¯è¾“å‡º
torch.cuda.reset_peak_memory_stats()

# -----------------------------
# å•æ­¥ç”Ÿæˆï¼ˆæ—  KVCacheï¼‰
# -----------------------------
start_time = time.time()
latencies = []
mem_usages = []

output_ids = input_ids.clone()

print("ğŸš€ å¼€å§‹ç”Ÿæˆï¼ˆå…³é—­ KV Cacheï¼‰...")

for i in range(generate_length):
    t0 = time.time()
    with torch.no_grad():
        outputs = model(output_ids)
        next_token_logits = outputs.logits[:, -1, :] / temperature
        # ä½¿ç”¨é‡‡æ ·ï¼ˆé¿å…æ­»å¾ªç¯ï¼‰
        probs = torch.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
    output_ids = torch.cat([output_ids, next_token], dim=1)

    latency = time.time() - t0
    latencies.append(latency)
    mem_usages.append(torch.cuda.max_memory_allocated(device) / 1024**2)

end_time = time.time()

# -----------------------------
# ç»“æœç»Ÿè®¡
# -----------------------------
avg_latency = np.mean(latencies)
max_mem = max(mem_usages)
total_time = end_time - start_time

decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("âœ… å®éªŒå®Œæˆï¼ˆå…³é—­ KV Cacheï¼‰")
print(f"å¹³å‡æ¨ç†è€—æ—¶: {avg_latency:.4f} ç§’ / token")
print(f"å³°å€¼æ˜¾å­˜ä½¿ç”¨: {max_mem:.2f} MB")
print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
print(f"ç”Ÿæˆæ–‡æœ¬ç‰‡æ®µ:\n{decoded[:150]}...")

# -----------------------------
# å¯è§†åŒ–ï¼šæ˜¾å­˜ä¸å»¶è¿Ÿè¶‹åŠ¿
# -----------------------------
fig, ax1 = plt.subplots(figsize=(7,4))
ax2 = ax1.twinx()

ax1.plot(mem_usages, color="#5DADE2", label="æ˜¾å­˜ (MB)")
ax2.plot(latencies, color="#E74C3C", label="å»¶è¿Ÿ (s)")

ax1.set_xlabel("ç”Ÿæˆæ­¥æ•°")
ax1.set_ylabel("æ˜¾å­˜ (MB)")
ax2.set_ylabel("å»¶è¿Ÿ (s)")
plt.title("å…³é—­ KV Cache çš„æ¨ç†æ€§èƒ½è¶‹åŠ¿")
ax1.grid(True, linestyle="--", alpha=0.5)
plt.show()
```

    ğŸš€ å¼€å§‹ç”Ÿæˆï¼ˆå…³é—­ KV Cacheï¼‰...
    âœ… å®éªŒå®Œæˆï¼ˆå…³é—­ KV Cacheï¼‰
    å¹³å‡æ¨ç†è€—æ—¶: 0.0111 ç§’ / token
    å³°å€¼æ˜¾å­˜ä½¿ç”¨: 430.81 MB
    æ€»è€—æ—¶: 0.56 ç§’
    ç”Ÿæˆæ–‡æœ¬ç‰‡æ®µ:
    æ·± åº¦ å­¦ ä¹  ä¸­ çš„ æ³¨ æ„ åŠ› æœº åˆ¶ æ˜¯ ä¸€ ç§ æœº åˆ¶ å— ï¼Ÿ æˆ‘ æ˜¯ ä¸€ å åœ¨ ç¾ å›½ çš„ ç§‘ å­¦ å®¶ ï¼Œ æœ€ è¿‘ åœ¨ åš æ·± åº¦ å­¦ ä¹  çš„ æ—¶ å€™ ï¼Œ å‘ ç° æœ‰ ä¸€ äº› ä¸€ äº› æ³¨ æ„ åŠ› æœº åˆ¶ éœ€ è¦ ç‰¹ åˆ« æ³¨ æ„ ï¼Œ æ¯” å¦‚ ä½•...



    
![png](Code01KVCache_files/Code01KVCache_5_1.png)
    


è¿™ä¸ªå®éªŒå±•ç¤ºäº†æœ€åŸºç¡€çš„ç”Ÿæˆæ–¹å¼ï¼Œæ¯æ¬¡éƒ½éœ€è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„æ³¨æ„åŠ›ï¼Œè®¡ç®—å¤æ‚åº¦ä¸º $O(n^2)$ï¼Œå…¶ä¸­ n æ˜¯åºåˆ—é•¿åº¦ã€‚å¤§è¯­è¨€æ¨¡å‹é€šè¿‡æ¦‚ç‡æ–¹æ³•ç”Ÿæˆæ–‡æœ¬ï¼Œå³æ ¹æ®è¾“å…¥æˆ–ä¸Šä¸‹æ–‡ä¸ºæ¯ä¸ªå¯èƒ½çš„è¯æˆ–å¥å­åˆ†é…ä¸€ä¸ªæ¦‚ç‡ï¼Œç„¶åé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯æˆ–å¥å­ï¼Œæˆ–è€…ä»æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œæ¥ç”Ÿæˆè¾“å‡ºæ–‡æœ¬ã€‚

## 4. å¼€å¯ KVCache

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ç”¨ KVCache åŠŸèƒ½ã€‚è¿™å°†æ˜¾è‘—å‡å°‘è®¡ç®—é‡ï¼Œå› ä¸ºåªéœ€è¦è®¡ç®—æœ€æ–° token çš„æ³¨æ„åŠ›æƒé‡ã€‚


```python
# =============================
# å¼€å¯ KV Cache æµ‹è¯•
# =============================

import torch
import time
import matplotlib.pyplot as plt
import numpy as np

# -----------------------------
# è¾“å…¥æ–‡æœ¬
# -----------------------------
prompt = "æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

# -----------------------------
# ç”Ÿæˆå‚æ•°
# -----------------------------
generate_length = 50
temperature = 0.7
torch.cuda.reset_peak_memory_stats()

# -----------------------------
# å¯ç”¨ KV Cache ç”Ÿæˆ
# -----------------------------
output_ids = input_ids.clone()
past_key_values = None
latencies = []
mem_usages = []

print("ğŸš€ å¼€å§‹ç”Ÿæˆï¼ˆå¼€å¯ KV Cacheï¼‰...")

start_time = time.time()

for i in range(generate_length):
    t0 = time.time()
    with torch.no_grad():
        outputs = model(
            output_ids[:, -1:],  # åªè¾“å…¥ä¸Šä¸€æ­¥ç”Ÿæˆçš„ token
            past_key_values=past_key_values,
            use_cache=True
        )
        next_token_logits = outputs.logits[:, -1, :] / temperature
        probs = torch.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        past_key_values = outputs.past_key_values  # æ›´æ–°ç¼“å­˜
        output_ids = torch.cat([output_ids, next_token], dim=1)

    latency = time.time() - t0
    latencies.append(latency)
    mem_usages.append(torch.cuda.max_memory_allocated(device) / 1024**2)

end_time = time.time()

# -----------------------------
# ç»“æœç»Ÿè®¡
# -----------------------------
avg_latency = np.mean(latencies)
max_mem = max(mem_usages)
total_time = end_time - start_time
decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("âœ… å®éªŒå®Œæˆï¼ˆå¼€å¯ KV Cacheï¼‰")
print(f"å¹³å‡æ¨ç†è€—æ—¶: {avg_latency:.4f} ç§’ / token")
print(f"å³°å€¼æ˜¾å­˜ä½¿ç”¨: {max_mem:.2f} MB")
print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
print(f"ç”Ÿæˆæ–‡æœ¬ç‰‡æ®µ:\n{decoded[:150]}...")

# -----------------------------
# å¯è§†åŒ–ï¼šæ˜¾å­˜ä¸å»¶è¿Ÿè¶‹åŠ¿
# -----------------------------
fig, ax1 = plt.subplots(figsize=(7,4))
ax2 = ax1.twinx()

ax1.plot(mem_usages, color="#58D68D", label="æ˜¾å­˜ (MB)")
ax2.plot(latencies, color="#C0392B", label="å»¶è¿Ÿ (s)")

ax1.set_xlabel("ç”Ÿæˆæ­¥æ•°")
ax1.set_ylabel("æ˜¾å­˜ (MB)")
ax2.set_ylabel("å»¶è¿Ÿ (s)")
plt.title("å¼€å¯ KV Cache çš„æ¨ç†æ€§èƒ½è¶‹åŠ¿")
ax1.grid(True, linestyle="--", alpha=0.5)
plt.show()
```

    ğŸš€ å¼€å§‹ç”Ÿæˆï¼ˆå¼€å¯ KV Cacheï¼‰...
    âœ… å®éªŒå®Œæˆï¼ˆå¼€å¯ KV Cacheï¼‰
    å¹³å‡æ¨ç†è€—æ—¶: 0.0074 ç§’ / token
    å³°å€¼æ˜¾å­˜ä½¿ç”¨: 421.29 MB
    æ€»è€—æ—¶: 0.37 ç§’
    ç”Ÿæˆæ–‡æœ¬ç‰‡æ®µ:
    æ·± åº¦ å­¦ ä¹  ä¸­ çš„ æ³¨ æ„ åŠ› æœº åˆ¶ æ˜¯ ä¸€ ç§ ï¼Œ ä½  ä¼š å‘ ç° è‡ª å·± åœ¨ ä¸€ ä¸ª äºº å¿ƒ é‡Œ æ˜¯ é‚£ ä¹ˆ çš„ å­¤ ç‹¬ ã€‚ ä»– ä¼š åœ¨ ä½  èº« è¾¹ ï¼Œ ä½† ä½  ä¸ çŸ¥ é“ ä»– æœ‰ å¤š å­¤ ç‹¬ ã€‚ æœ‰ æ—¶ å€™ ä½  ä¸ çŸ¥ é“ ä»– è¯´ çš„ å¯¹ ä¸...



    
![png](Code01KVCache_files/Code01KVCache_7_1.png)
    


ä½¿ç”¨ KVCache åï¼Œè®¡ç®—å¤æ‚åº¦é™ä½åˆ° $O(n)$ï¼Œå› ä¸ºåªéœ€è¦è®¡ç®—æœ€æ–° token ä¸æ‰€æœ‰ç¼“å­˜ key çš„ç‚¹ç§¯ã€‚ä½†æ˜¯ï¼ŒKVCache å¯èƒ½å ç”¨å¤§é‡æ˜¾å­˜ï¼Œå°¤å…¶æ˜¯å¯¹äºé•¿åºåˆ—ã€‚å¤§è¯­è¨€æ¨¡å‹å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ–‡æœ¬ç”Ÿæˆå’Œç†è§£ï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ä¸åŒçš„è¯­è¨€ç¯å¢ƒã€‚

## 5. KVCache å†…å­˜æŒ‘æˆ˜

è™½ç„¶ KVCache æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ï¼Œä½†å®ƒä¹Ÿå¸¦æ¥äº†å†…å­˜æŒ‘æˆ˜ã€‚å¯¹äºç”Ÿæˆé•¿åºåˆ—ï¼ŒKVCache å¯èƒ½å ç”¨å¤§é‡æ˜¾å­˜ã€‚å…·ä½“æ¥è¯´ï¼Œç¼“å­˜å¤§å°ä¸åºåˆ—é•¿åº¦ã€æ‰¹å¤„ç†å¤§å°ã€æ³¨æ„åŠ›å¤´æ•°å’Œå¤´ç»´åº¦æˆæ­£æ¯”ï¼š

$$\text{ç¼“å­˜å¤§å°} = 2 \times b \times h \times l \times d$$

å…¶ä¸­ $b$ æ˜¯æ‰¹å¤„ç†å¤§å°ï¼Œ$h$ æ˜¯æ³¨æ„åŠ›å¤´æ•°ï¼Œ$l$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯æ¯ä¸ªå¤´çš„ç»´åº¦ã€‚

ä¼ ç»Ÿ KVCache éœ€è¦è¿ç»­çš„å†…å­˜ç©ºé—´ï¼Œå½“ç”Ÿæˆé•¿åºåˆ—æ—¶å¯èƒ½æ‰¾ä¸åˆ°è¶³å¤Ÿå¤§çš„è¿ç»­å†…å­˜å—ï¼Œå¯¼è‡´å†…å­˜ç¢ç‰‡åŒ–ã€‚å¤§è¯­è¨€æ¨¡å‹é€šå¸¸æ˜¯å·¨å‹æ¨¡å‹ï¼ŒåŒ…å«æ•°ä»¥äº¿è®¡çš„å‚æ•°ï¼Œä»¥ä¾¿å¤„ç†å¤§é‡çš„è¯­è¨€æ•°æ®ï¼Œè¿™ä½¿å¾—å†…å­˜ç®¡ç†å˜å¾—å°¤ä¸ºé‡è¦ã€‚

## 6. PagedAttention

PagedAttention æ˜¯ä¸€ç§é«˜çº§ä¼˜åŒ–æŠ€æœ¯ï¼Œçµæ„Ÿæ¥è‡ªæ“ä½œç³»ç»Ÿä¸­çš„è™šæ‹Ÿå†…å­˜å’Œåˆ†é¡µæ¦‚å¿µã€‚å®ƒå°† KVCache åˆ†æˆå›ºå®šå¤§å°çš„å—ï¼ˆé¡µï¼‰ï¼Œå¹¶åœ¨éè¿ç»­çš„å†…å­˜ç©ºé—´ä¸­ç®¡ç†è¿™äº›é¡µã€‚

ç”±äºç›´æ¥å®ç° PagedAttention éœ€è¦å¤æ‚çš„åº•å±‚ä¼˜åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ vLLM åº“æ¥å®ç°è¿™ä¸€åŠŸèƒ½ã€‚vLLM æ˜¯ä¸€ä¸ªé«˜æ•ˆæ¨ç†å¼•æ“ï¼Œå†…ç½®äº†å¯¹ PagedAttention çš„æ”¯æŒã€‚


```python
# =============================
# PagedAttention æµ‹è¯•ï¼ˆvLLMï¼‰
# =============================

import time
import numpy as np
import matplotlib.pyplot as plt
from vllm import LLM, SamplingParams

# -----------------------------
# åˆå§‹åŒ– vLLM æ¨¡å‹
# -----------------------------
paged_model_name = "uer/gpt2-chinese-cluecorpussmall"

sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=50,
)

print("ğŸš€ åˆå§‹åŒ– vLLM æ¨¡å‹ (PagedAttention å¼€å¯)...")
start_load = time.time()

# ä½¿ç”¨ float16 æ›´é«˜æ•ˆ
llm = LLM(
    model=paged_model_name,
    dtype="float16",
    trust_remote_code=True
)

load_time = time.time() - start_load
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œç”¨æ—¶ {load_time:.2f} ç§’")

# -----------------------------
# è¾“å…¥æ–‡æœ¬
# -----------------------------
prompt = "æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§"

# -----------------------------
# æ¨ç†ä¸æµ‹é‡
# -----------------------------
torch.cuda.reset_peak_memory_stats()

print("ğŸš€ å¼€å§‹æ¨ç†ï¼ˆPagedAttention æ¨¡å¼ï¼‰...")
start_time = time.time()
outputs = llm.generate([prompt], sampling_params)
end_time = time.time()

total_time = end_time - start_time
max_mem = torch.cuda.max_memory_allocated() / 1024**2

# -----------------------------
# è¾“å‡ºç»“æœ
# -----------------------------
generated_text = outputs[0].outputs[0].text.strip()

print("âœ… å®éªŒå®Œæˆï¼ˆPagedAttentionï¼‰")
print(f"æ€»è€—æ—¶: {total_time:.4f} ç§’")
print(f"å³°å€¼æ˜¾å­˜ä½¿ç”¨: {max_mem:.2f} MB")
print(f"ç”Ÿæˆæ–‡æœ¬ç‰‡æ®µ:\n{generated_text[:150]}...")
```

    INFO 11-05 22:41:06 [__init__.py:216] Automatically detected platform cuda.
    ğŸš€ åˆå§‹åŒ– vLLM æ¨¡å‹ (PagedAttention å¼€å¯)...
    INFO 11-05 22:41:07 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'float16', 'disable_log_stats': True, 'model': 'uer/gpt2-chinese-cluecorpussmall'}


    The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.


    INFO 11-05 22:41:09 [model.py:547] Resolved architecture: GPT2LMHeadModel


    `torch_dtype` is deprecated! Use `dtype` instead!


    ERROR 11-05 22:41:10 [config.py:278] Error retrieving safetensors: 'uer/gpt2-chinese-cluecorpussmall' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files., retrying 1 of 2
    ERROR 11-05 22:41:12 [config.py:276] Error retrieving safetensors: 'uer/gpt2-chinese-cluecorpussmall' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files.
    INFO 11-05 22:41:12 [model.py:1730] Downcasting torch.float32 to torch.float16.
    INFO 11-05 22:41:12 [model.py:1510] Using max model len 1024


    2025-11-05 22:41:12,894	INFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.


    INFO 11-05 22:41:12 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
    WARNING 11-05 22:41:14 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
    INFO 11-05 22:41:18 [__init__.py:216] Automatically detected platform cuda.
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:19 [core.py:644] Waiting for init message from front-end.
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:19 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='uer/gpt2-chinese-cluecorpussmall', speculative_config=None, tokenizer='uer/gpt2-chinese-cluecorpussmall', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=uer/gpt2-chinese-cluecorpussmall, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
    [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
    [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
    [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
    [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
    [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
    [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:21 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
    [1;36m(EngineCore_DP0 pid=28445)[0;0m WARNING 11-05 22:41:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:21 [gpu_model_runner.py:2602] Starting to load model uer/gpt2-chinese-cluecorpussmall...
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:21 [gpu_model_runner.py:2634] Loading model from scratch...
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:21 [cuda.py:366] Using Flash Attention backend on V1 engine.
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:22 [weight_utils.py:392] Using model weights format ['*.safetensors', '*.bin', '*.pt']
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:23 [weight_utils.py:413] Time spent downloading weights for uer/gpt2-chinese-cluecorpussmall: 0.599068 seconds


    Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
    Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.95it/s]
    Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.95it/s]
    [1;36m(EngineCore_DP0 pid=28445)[0;0m 


    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:23 [default_loader.py:267] Loading weights took 0.34 seconds
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:24 [gpu_model_runner.py:2653] Model loading took 0.1968 GiB and 1.793934 seconds
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:25 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/f3ce07206c/rank_0_0/backbone for vLLM's torch.compile
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:25 [backends.py:559] Dynamo bytecode transform time: 1.40 s
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:26 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.273 s
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:26 [monitor.py:34] torch.compile takes 1.40 s in total
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:26 [gpu_worker.py:298] Available KV cache memory: 20.72 GiB
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:27 [kv_cache_utils.py:1087] GPU KV cache size: 603,488 tokens
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:27 [kv_cache_utils.py:1091] Maximum concurrency for 1,024 tokens per request: 589.34x


    Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:00<00:00, 96.32it/s] 
    Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 100.64it/s]


    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:28 [gpu_model_runner.py:3480] Graph capturing finished in 1 secs, took 0.21 GiB
    [1;36m(EngineCore_DP0 pid=28445)[0;0m INFO 11-05 22:41:28 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.51 seconds
    INFO 11-05 22:41:29 [llm.py:306] Supported_tasks: ['generate']
    âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œç”¨æ—¶ 22.54 ç§’
    ğŸš€ å¼€å§‹æ¨ç†ï¼ˆPagedAttention æ¨¡å¼ï¼‰...


    Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1463.98it/s]
    Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.98it/s, est. speed input: 241.85 toks/s, output: 755.52 toks/s]

    âœ… å®éªŒå®Œæˆï¼ˆPagedAttentionï¼‰
    æ€»è€—æ—¶: 0.0716 ç§’
    å³°å€¼æ˜¾å­˜ä½¿ç”¨: 413.98 MB
    ç”Ÿæˆæ–‡æœ¬ç‰‡æ®µ:
    å— ï¼Ÿ æ¯” å¦‚ ï¼Œ åœ¨ å­¦ ä¹  è¿‡ ç¨‹ ä¸­ ï¼Œ å¦‚ ä½• å°† æ³¨ æ„ åŠ› é›† ä¸­ åˆ° ä¸€ ä»¶ äº‹ ä¸Š ï¼Ÿ æ¯” å¦‚ ï¼Œ å­¦ ä¹  æŸ ä¸ª è¯­ è¨€ æ—¶ ï¼Œ æ€ æ · åœ¨ å­¦ ä¹  çš„ è¿‡ ç¨‹ ä¸­ ï¼Œ ç”¨ åˆ° æ³¨...


    


PagedAttention é€šè¿‡åˆ†é¡µæœºåˆ¶è§£å†³äº† KVCache çš„å†…å­˜ç¢ç‰‡é—®é¢˜ã€‚å®ƒå°† KVCache åˆ†æˆå›ºå®šå¤§å°çš„é¡µé¢ï¼Œå…è®¸éè¿ç»­å­˜å‚¨ï¼Œæé«˜äº†å†…å­˜åˆ©ç”¨ç‡ã€‚å¤§è¯­è¨€æ¨¡å‹é€šè¿‡è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå³é€šè¿‡é¢„æµ‹ä¸‹ä¸€æ­¥æ–‡æœ¬æ¥å­¦ä¹ è¯­è¨€æ¨¡å¼ï¼Œè¿™ç§å­¦ä¹ æ–¹æ³•ä½¿å¤§è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚

## 7. å®éªŒç»“æœåˆ†æä¸å¯è§†åŒ–

ä»æ˜¾å­˜å ç”¨æ¥çœ‹ï¼Œå…³é—­ KVCache æ—¶æ˜¾å­˜å ç”¨æœ€å°‘ï¼Œå› ä¸ºä¸éœ€è¦é¢å¤–ç©ºé—´å­˜å‚¨ KV å€¼ï¼Œä½†è¿™æ˜¯ä»¥è®¡ç®—æ—¶é—´ä¸ºä»£ä»·çš„ã€‚å¼€å¯ KVCache åï¼Œæ˜¾å­˜å ç”¨æ˜æ˜¾å¢åŠ ï¼Œå› ä¸ºéœ€è¦å­˜å‚¨å…ˆå‰æ‰€æœ‰ token çš„é”®å€¼å¯¹ã€‚ä½¿ç”¨ PagedAttention åï¼Œæ˜¾å­˜å ç”¨è¿›ä¸€æ­¥å¢åŠ ï¼Œè¿™æ˜¯å› ä¸ºåˆ†é¡µæœºåˆ¶éœ€è¦é¢å¤–çš„å…ƒæ•°æ®æ¥ç®¡ç†å†…å­˜é¡µé¢ï¼Œä½†è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ”¯æŒæ›´é•¿çš„åºåˆ—ç”Ÿæˆã€‚

åœ¨æ¨ç†å»¶è¿Ÿæ–¹é¢ï¼Œå…³é—­ KVCache çš„æ–¹æ¡ˆå»¶è¿Ÿæœ€é«˜ï¼Œå› ä¸ºæ¯æ¬¡ç”Ÿæˆéƒ½éœ€è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„æ³¨æ„åŠ›ã€‚å¼€å¯ KVCache åï¼Œå»¶è¿Ÿæ˜¾è‘—é™ä½ï¼Œå› ä¸ºåªéœ€è¦è®¡ç®—æœ€æ–° token çš„æ³¨æ„åŠ›æƒé‡ã€‚PagedAttention åœ¨å»¶è¿Ÿæ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå› ä¸ºå®ƒä¸ä»…åˆ©ç”¨äº† KVCacheï¼Œè¿˜é€šè¿‡ä¼˜åŒ–çš„å†…å­˜è®¿é—®æ¨¡å¼å‡å°‘äº†å†…å­˜ç¢ç‰‡å’Œè®¿é—®å»¶è¿Ÿã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒKVCache ä¼˜åŒ–é€šå¸¸ä¸å…¶ä»–æŠ€æœ¯ç»“åˆä½¿ç”¨ï¼Œå¦‚é‡åŒ–ã€å‰ªæå’Œè’¸é¦ç­‰ã€‚å¯¹äºæé•¿åºåˆ—ç”Ÿæˆï¼Œè¿˜å¯ä»¥è€ƒè™‘ç¨€ç–æ³¨æ„åŠ›åªè®¡ç®—ä¸æœ€è¿‘ token çš„æ³¨æ„åŠ›ï¼Œå‡å°‘è®¡ç®—é‡ï¼›çº¿æ€§æ³¨æ„åŠ›ä½¿ç”¨çº¿æ€§å¤æ‚åº¦çš„æ³¨æ„åŠ›å˜ä½“ï¼›å†…å­˜æ¢è®¡ç®—åœ¨å†…å­˜å……è¶³æ—¶ç¼“å­˜æ›´å¤šä¸­é—´ç»“æœã€‚


```python
# =============================
# 6. æ•°æ®ç»Ÿè®¡ä¸ç»“æœåˆ†æ
# =============================

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from matplotlib import font_manager

# âœ… å…¨å±€å­—ä½“é…ç½®
font_path = "/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc"
font_prop = font_manager.FontProperties(fname=font_path)
font_name = font_prop.get_name()

matplotlib.rcParams['font.family'] = font_name
matplotlib.rcParams['axes.unicode_minus'] = False

# åŒæ—¶è®¾ç½® seaborn ä½¿ç”¨ç›¸åŒå­—ä½“
sns.set(style="whitegrid")
sns.set_context("notebook", font_scale=1.0)

# -----------------------------
# åœ¨æ­¤å¡«å†™å‰é¢å®éªŒä¸­è¾“å‡ºçš„æ•°å€¼
# -----------------------------
results = [
    {
        "æ¨¡å¼": "å…³é—­ KVCache",
        "å¹³å‡å»¶è¿Ÿ(s)": 0.0111,
        "å³°å€¼æ˜¾å­˜(MB)": 430.81,
        "æ€»è€—æ—¶(s)": 0.56,
    },
    {
        "æ¨¡å¼": "å¼€å¯ KVCache",
        "å¹³å‡å»¶è¿Ÿ(s)": 0.0074,
        "å³°å€¼æ˜¾å­˜(MB)": 421.29,
        "æ€»è€—æ—¶(s)": 0.37,
    },
    {
        "æ¨¡å¼": "PagedAttention",
        "å¹³å‡å»¶è¿Ÿ(s)": 0.0013,
        "å³°å€¼æ˜¾å­˜(MB)": 413.98,
        "æ€»è€—æ—¶(s)": 0.0716,
    },
]

# -----------------------------
# æ„å»º DataFrame
# -----------------------------
df = pd.DataFrame(results)
print("âœ… å®éªŒæµ‹é‡æ±‡æ€»ï¼š")
print(df.to_string(index=False))  # æ›´ç¾è§‚çš„è¡¨æ ¼è¾“å‡º
print()

# -----------------------------
# å¯è§†åŒ– 1ï¼šæ˜¾å­˜å’Œå»¶è¿Ÿå¯¹æ¯”
# -----------------------------
fig, ax1 = plt.subplots(figsize=(8, 5))
ax2 = ax1.twinx()

# è®¾ç½® x è½´ä½ç½®
x_pos = range(len(df))
modes = df["æ¨¡å¼"].tolist()

bar_colors = ["#F5B041", "#58D68D", "#5DADE2"]
ax1.bar(x_pos, df["å³°å€¼æ˜¾å­˜(MB)"], color=bar_colors, alpha=0.7, label="å³°å€¼æ˜¾å­˜ (MB)")
ax2.plot(x_pos, df["å¹³å‡å»¶è¿Ÿ(s)"], color="#C0392B", marker="o", linewidth=2, 
         markersize=8, label="å¹³å‡å»¶è¿Ÿ (s)")

# è®¾ç½® x è½´åˆ»åº¦å’Œæ ‡ç­¾
ax1.set_xticks(x_pos)
ax1.set_xticklabels(modes, fontproperties=font_prop)

ax1.set_xlabel("æ¨ç†ä¼˜åŒ–æ¨¡å¼", fontsize=11, fontproperties=font_prop)
ax1.set_ylabel("å³°å€¼æ˜¾å­˜ (MB)", fontsize=11, fontproperties=font_prop)
ax2.set_ylabel("å¹³å‡å»¶è¿Ÿ (s)", fontsize=11, fontproperties=font_prop)
ax1.set_title("ä¸åŒæ¨ç†ä¼˜åŒ–ç­–ç•¥çš„æ€§èƒ½å¯¹æ¯”", fontsize=13, fontproperties=font_prop)

# æ·»åŠ å›¾ä¾‹
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
legend = ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', prop=font_prop)

ax1.grid(True, linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()

# -----------------------------
# å¯è§†åŒ– 2ï¼šå»¶è¿Ÿä¸æ˜¾å­˜æŠ˜çº¿ç»“åˆè¶‹åŠ¿
# -----------------------------
fig, ax = plt.subplots(figsize=(8, 5))

# æ ‡å‡†åŒ–æ•°æ®ä»¥ä¾¿åœ¨åŒä¸€å›¾è¡¨æ˜¾ç¤º
df_normalized = df.copy()
df_normalized["å¹³å‡å»¶è¿Ÿ(s)_å½’ä¸€åŒ–"] = df["å¹³å‡å»¶è¿Ÿ(s)"] / df["å¹³å‡å»¶è¿Ÿ(s)"].max() * 100
df_normalized["å³°å€¼æ˜¾å­˜(MB)_å½’ä¸€åŒ–"] = df["å³°å€¼æ˜¾å­˜(MB)"] / df["å³°å€¼æ˜¾å­˜(MB)"].max() * 100

x_pos = range(len(df))
ax.plot(x_pos, df_normalized["å¹³å‡å»¶è¿Ÿ(s)_å½’ä¸€åŒ–"], marker="o", linewidth=2, 
        markersize=8, label="å¹³å‡å»¶è¿Ÿ (å½’ä¸€åŒ–%)", color="#E74C3C")
ax.plot(x_pos, df_normalized["å³°å€¼æ˜¾å­˜(MB)_å½’ä¸€åŒ–"], marker="s", linewidth=2, 
        markersize=8, label="å³°å€¼æ˜¾å­˜ (å½’ä¸€åŒ–%)", color="#5DADE2")

ax.set_xticks(x_pos)
ax.set_xticklabels(modes, fontproperties=font_prop)

ax.set_title("å»¶è¿Ÿä¸æ˜¾å­˜å˜åŒ–è¶‹åŠ¿ï¼ˆå½’ä¸€åŒ–ï¼‰", fontproperties=font_prop, fontsize=13)
ax.set_xlabel("æ¨¡å¼", fontproperties=font_prop, fontsize=11)
ax.set_ylabel("å½’ä¸€åŒ–å€¼ (%)", fontproperties=font_prop, fontsize=11)

legend = ax.legend(prop=font_prop)
ax.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# -----------------------------
# å¯è§†åŒ– 3ï¼šåŸå§‹æ•°æ®å¯¹æ¯”ï¼ˆåŒ Y è½´ï¼‰
# -----------------------------
fig, ax1 = plt.subplots(figsize=(8, 5))
ax2 = ax1.twinx()

x_pos = range(len(df))
ax1.plot(x_pos, df["å¹³å‡å»¶è¿Ÿ(s)"], marker="o", linewidth=2, markersize=8, 
         label="å¹³å‡å»¶è¿Ÿ (s)", color="#E74C3C")
ax2.plot(x_pos, df["å³°å€¼æ˜¾å­˜(MB)"], marker="s", linewidth=2, markersize=8, 
         label="å³°å€¼æ˜¾å­˜ (MB)", color="#5DADE2")

ax1.set_xticks(x_pos)
ax1.set_xticklabels(modes, fontproperties=font_prop)

ax1.set_title("å»¶è¿Ÿä¸æ˜¾å­˜å˜åŒ–è¶‹åŠ¿ï¼ˆåŸå§‹å€¼ï¼‰", fontproperties=font_prop, fontsize=13)
ax1.set_xlabel("æ¨¡å¼", fontproperties=font_prop, fontsize=11)
ax1.set_ylabel("å¹³å‡å»¶è¿Ÿ (s)", fontproperties=font_prop, fontsize=11, color="#E74C3C")
ax2.set_ylabel("å³°å€¼æ˜¾å­˜ (MB)", fontproperties=font_prop, fontsize=11, color="#5DADE2")

ax1.tick_params(axis='y', labelcolor="#E74C3C")
ax2.tick_params(axis='y', labelcolor="#5DADE2")

# æ·»åŠ å›¾ä¾‹
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
legend = ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', prop=font_prop)

ax1.grid(True, linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

# -----------------------------
# ç®€è¦å­—ç¬¦ä¸²ç»“è®ºè¾“å‡º
# -----------------------------
no_cache = df.loc[df["æ¨¡å¼"]=="å…³é—­ KVCache"].iloc[0]
with_cache = df.loc[df["æ¨¡å¼"]=="å¼€å¯ KVCache"].iloc[0]
paged = df.loc[df["æ¨¡å¼"]=="PagedAttention"].iloc[0]

speedup_kv = no_cache["å¹³å‡å»¶è¿Ÿ(s)"] / with_cache["å¹³å‡å»¶è¿Ÿ(s)"]
speedup_paged = with_cache["å¹³å‡å»¶è¿Ÿ(s)"] / paged["å¹³å‡å»¶è¿Ÿ(s)"]

print("\n" + "="*60)
print("ğŸ“Š æ€§èƒ½ç»“è®ºæ±‡æ€»")
print("="*60)
print(f"\nâ¡ï¸  KV Cache ç›¸æ¯”æ— ç¼“å­˜ï¼Œå¹³å‡åŠ é€Ÿçº¦ {speedup_kv:.2f}x")
print(f"â¡ï¸  PagedAttention ç›¸æ¯”æ™®é€š KV Cacheï¼Œå†æ¬¡åŠ é€Ÿçº¦ {speedup_paged:.2f}x")
print(f"â¡ï¸  ä¸‰è€…åœ¨æ˜¾å­˜ä½¿ç”¨ä¸Šå·®è·è¾ƒå°ï¼Œä½†å»¶è¿Ÿé™ä½æ•ˆæœæ˜¾è‘—")
print("\n" + "-"*60)
print("ğŸ’¡ å®éªŒç»“è®ºï¼š")
print("-"*60)
print("   â€¢ KV Cache æœ‰æ•ˆå‡å°‘é‡å¤è®¡ç®—ï¼Œå¤§å¹…æå‡æ¨ç†é€Ÿåº¦")
print("   â€¢ PagedAttention è¿›ä¸€æ­¥æå‡æ˜¾å­˜ä¸è®¡ç®—æ•ˆç‡ï¼Œ")
print("     æ›´é€‚åˆé•¿åºåˆ—æˆ–å¤§æ¨¡å‹æ¨ç†åœºæ™¯")
print("="*60)
```

    âœ… å®éªŒæµ‹é‡æ±‡æ€»ï¼š
                æ¨¡å¼  å¹³å‡å»¶è¿Ÿ(s)  å³°å€¼æ˜¾å­˜(MB)  æ€»è€—æ—¶(s)
        å…³é—­ KVCache   0.0111    430.81  0.5600
        å¼€å¯ KVCache   0.0074    421.29  0.3700
    PagedAttention   0.0013    413.98  0.0716
    



    
![png](Code01KVCache_files/Code01KVCache_11_1.png)
    



    
![png](Code01KVCache_files/Code01KVCache_11_2.png)
    



    
![png](Code01KVCache_files/Code01KVCache_11_3.png)
    


    
    ============================================================
    ğŸ“Š æ€§èƒ½ç»“è®ºæ±‡æ€»
    ============================================================
    
    â¡ï¸  KV Cache ç›¸æ¯”æ— ç¼“å­˜ï¼Œå¹³å‡åŠ é€Ÿçº¦ 1.50x
    â¡ï¸  PagedAttention ç›¸æ¯”æ™®é€š KV Cacheï¼Œå†æ¬¡åŠ é€Ÿçº¦ 5.69x
    â¡ï¸  ä¸‰è€…åœ¨æ˜¾å­˜ä½¿ç”¨ä¸Šå·®è·è¾ƒå°ï¼Œä½†å»¶è¿Ÿé™ä½æ•ˆæœæ˜¾è‘—
    
    ------------------------------------------------------------
    ğŸ’¡ å®éªŒç»“è®ºï¼š
    ------------------------------------------------------------
       â€¢ KV Cache æœ‰æ•ˆå‡å°‘é‡å¤è®¡ç®—ï¼Œå¤§å¹…æå‡æ¨ç†é€Ÿåº¦
       â€¢ PagedAttention è¿›ä¸€æ­¥æå‡æ˜¾å­˜ä¸è®¡ç®—æ•ˆç‡ï¼Œ
         æ›´é€‚åˆé•¿åºåˆ—æˆ–å¤§æ¨¡å‹æ¨ç†åœºæ™¯
    ============================================================


## 8. æ€»ç»“ä¸æ€è€ƒ

é€šè¿‡æœ¬å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº† KVCache æŠ€æœ¯åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡æ–¹é¢çš„é‡è¦ä½œç”¨ã€‚ä»å®Œå…¨å…³é—­ç¼“å­˜åˆ°å¯ç”¨ç¼“å­˜ï¼Œå†åˆ°æ›´é«˜çº§çš„ PagedAttention ä¼˜åŒ–ï¼Œæ¯ä¸€æ­¥éƒ½å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
